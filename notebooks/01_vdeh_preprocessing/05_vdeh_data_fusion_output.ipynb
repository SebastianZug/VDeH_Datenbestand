{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1450aed3",
   "metadata": {
    "papermill": {
     "duration": 0.005248,
     "end_time": "2025-12-25T21:02:29.304182",
     "exception": false,
     "start_time": "2025-12-25T21:02:29.298934",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# VDEH Data Fusion Pipeline\n",
    "\n",
    "**Fokus:** KI-gest√ºtzte Fusion von VDEH und DNB Daten + Dual-Source Language Fusion\n",
    "\n",
    "## üéØ Ziel\n",
    "- Intelligente Fusion von VDEH-Original und DNB-Daten\n",
    "- Konfliktaufl√∂sung via Ollama LLM\n",
    "- **Dual-Source Language Fusion**: MARC21 Sprache + langdetect Erkennung\n",
    "- Vollst√§ndige Nachvollziehbarkeit aller Entscheidungen\n",
    "- Qualit√§tsverbesserung durch Datenanreicherung\n",
    "\n",
    "## üìö Input/Output\n",
    "- **Input**: `data/vdeh/processed/04_dnb_enriched_data.parquet`\n",
    "- **Output**: `data/vdeh/processed/05_fused_data.parquet`\n",
    "\n",
    "## ü§ñ KI-Modell\n",
    "- **Ollama**: Lokales LLM (llama3.3:70b)\n",
    "- **API**: http://localhost:11434"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8895ae5a",
   "metadata": {
    "papermill": {
     "duration": 0.003883,
     "end_time": "2025-12-25T21:02:29.312767",
     "exception": false,
     "start_time": "2025-12-25T21:02:29.308884",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## üîÑ Fusion-Architektur\n",
    "\n",
    "**Drei Fusion-Strategien:**\n",
    "1. **Keine DNB-Daten** ‚Üí VDEH behalten\n",
    "2. **Keine Konflikte** ‚Üí Einfacher Merge (VDEH priorisiert, DNB erg√§nzt)\n",
    "3. **Konflikte vorhanden** ‚Üí KI-Entscheidung via Ollama\n",
    "\n",
    "**Vollst√§ndige Nachvollziehbarkeit:**\n",
    "- `fusion_*_source`: Welche Quelle f√ºr jedes Feld\n",
    "- `fusion_conflicts`: JSON mit allen erkannten Konflikten\n",
    "- `fusion_ai_reasoning`: KI-Begr√ºndung der Entscheidung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf981b20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T21:02:29.318835Z",
     "iopub.status.busy": "2025-12-25T21:02:29.318599Z",
     "iopub.status.idle": "2025-12-25T21:02:29.654677Z",
     "shell.execute_reply": "2025-12-25T21:02:29.654296Z"
    },
    "papermill": {
     "duration": 0.340005,
     "end_time": "2025-12-25T21:02:29.655699",
     "exception": false,
     "start_time": "2025-12-25T21:02:29.315694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-25 22:02:29 - utils.notebook_utils - INFO - Searching for project root...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-25 22:02:29 - utils.notebook_utils - INFO - Project root found: /media/sz/Data/Bibo/analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-25 22:02:29 - utils.notebook_utils - INFO - Loading configuration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-25 22:02:29 - config_loader - INFO - Configuration loaded from /media/sz/Data/Bibo/analysis/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-25 22:02:29 - utils.notebook_utils - INFO - Configuration loaded successfully: Dual-Source Bibliothek Bestandsvergleich\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Project root: /media/sz/Data/Bibo/analysis\n",
      "‚úÖ Project: Dual-Source Bibliothek Bestandsvergleich v2.2.0\n"
     ]
    }
   ],
   "source": [
    "# üõ†Ô∏è SETUP UND DATEN LADEN\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Add src to path (temporary until utils is imported)\n",
    "project_root = Path.cwd()\n",
    "while not (project_root / 'config.yaml').exists() and project_root.parent != project_root:\n",
    "    project_root = project_root.parent\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "# Now use the utility function\n",
    "from utils.notebook_utils import setup_notebook\n",
    "\n",
    "project_root, config = setup_notebook()\n",
    "print(f\"‚úÖ Project root: {project_root}\")\n",
    "print(f\"‚úÖ Project: {config.get('project.name')} v{config.get('project.version')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0063e59b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T21:02:29.663539Z",
     "iopub.status.busy": "2025-12-25T21:02:29.663144Z",
     "iopub.status.idle": "2025-12-25T21:02:30.033456Z",
     "shell.execute_reply": "2025-12-25T21:02:30.032989Z"
    },
    "papermill": {
     "duration": 0.376673,
     "end_time": "2025-12-25T21:02:30.034480",
     "exception": false,
     "start_time": "2025-12-25T21:02:29.657807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Daten geladen aus: /media/sz/Data/Bibo/analysis/data/vdeh/processed/04_dnb_enriched_data.parquet\n",
      "üìä Records: 58,305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Memory: 80.9 MB\n",
      "\n",
      "üìä DNB-Daten vorhanden: 5,855 (10.0%)\n",
      "   ISBN: 5,855\n"
     ]
    }
   ],
   "source": [
    "# üìÇ DNB-ANGEREICHERTE DATEN LADEN\n",
    "processed_dir = config.project_root / config.get('paths.data.vdeh.processed')\n",
    "input_path = processed_dir / '04_dnb_enriched_data.parquet'\n",
    "metadata_path = processed_dir / '04_metadata.json'\n",
    "\n",
    "if not input_path.exists():\n",
    "    raise FileNotFoundError(f\"Input-Datei nicht gefunden: {input_path}\\n\"\n",
    "                          \"Bitte f√ºhren Sie zuerst 04_vdeh_data_enrichment.ipynb aus.\")\n",
    "\n",
    "# Daten laden\n",
    "df_enriched = pd.read_parquet(input_path)\n",
    "\n",
    "# Vorherige Metadaten laden\n",
    "with open(metadata_path, 'r') as f:\n",
    "    prev_metadata = json.load(f)\n",
    "\n",
    "print(f\"üìÇ Daten geladen aus: {input_path}\")\n",
    "print(f\"üìä Records: {len(df_enriched):,}\")\n",
    "print(f\"üíæ Memory: {df_enriched.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# DNB-Daten Statistiken\n",
    "if 'dnb_query_method' in df_enriched.columns:\n",
    "    dnb_records = df_enriched['dnb_query_method'].notna().sum()\n",
    "    print(f\"\\nüìä DNB-Daten vorhanden: {dnb_records:,} ({dnb_records/len(df_enriched)*100:.1f}%)\")\n",
    "    \n",
    "    method_counts = df_enriched['dnb_query_method'].value_counts()\n",
    "    for method, count in method_counts.items():\n",
    "        print(f\"   {method}: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65df10de",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-12-25T21:02:30.036849",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# üìã FUSION-SETUP\n",
    "from fusion import OllamaClient, FusionEngine\n",
    "\n",
    "print(\"üìã === FUSION-SETUP ===\\n\")\n",
    "\n",
    "# Ollama-Client initialisieren\n",
    "ollama_client = OllamaClient(\n",
    "    api_url=\"http://localhost:11434/api/generate\",\n",
    "    model=\"llama3.3:70b\",\n",
    "    timeout_sec=220,\n",
    "    max_retries=4,\n",
    "    retry_backoff_base_sec=2,\n",
    "    abort_on_timeout=True,\n",
    "    enable_fallback=True,\n",
    "    fallback_model=\"llama3.2\"\n",
    ")\n",
    "\n",
    "# Test connection\n",
    "if ollama_client.test_connection():\n",
    "    print(f\"‚úÖ Ollama verbunden: {ollama_client.model}\")\n",
    "else:\n",
    "    raise RuntimeError(\"‚ùå Ollama nicht erreichbar! Stellen Sie sicher, dass Ollama l√§uft: ollama serve\")\n",
    "\n",
    "# Fusion-Engine initialisieren\n",
    "fusion_engine = FusionEngine(\n",
    "    ollama_client=ollama_client,\n",
    "    variant_priority=[\"id\", \"title_author\"]\n",
    ")\n",
    "\n",
    "print(f\"‚öôÔ∏è  Timeout: {ollama_client.timeout_sec}s | Retries: {ollama_client.max_retries}\")\n",
    "print(f\"ü§ñ Aktives Modell: {ollama_client.model}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5366010d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# üöÄ FUSION AUSF√úHREN\n",
    "from tqdm.auto import tqdm\n",
    "from fusion import OllamaUnavailableError\n",
    "\n",
    "print(\"üöÄ === FUSION AUSF√úHREN ===\\n\")\n",
    "\n",
    "# Configuration\n",
    "RESET_FUSION = False  # Set to True to reset all fusion results\n",
    "SAVE_INTERVAL = 50    # Save progress every N records\n",
    "\n",
    "# Optional limit for testing\n",
    "FUSION_LIMIT = None\n",
    "try:\n",
    "    FUSION_LIMIT = int(config.get('debug.fusion_limit', 0))\n",
    "    if FUSION_LIMIT <= 0:\n",
    "        FUSION_LIMIT = None\n",
    "except Exception:\n",
    "    FUSION_LIMIT = None\n",
    "\n",
    "# Progress tracking files\n",
    "progress_file = processed_dir / '05_fused_data_progress.parquet'\n",
    "retry_queue_file = processed_dir / '05_fused_retry_queue.json'\n",
    "\n",
    "# Reset if requested\n",
    "if RESET_FUSION:\n",
    "    if progress_file.exists():\n",
    "        progress_file.unlink()\n",
    "    if retry_queue_file.exists():\n",
    "        retry_queue_file.unlink()\n",
    "    print(\"üóëÔ∏è Fusion-Ergebnisse zur√ºckgesetzt\\n\")\n",
    "\n",
    "# WICHTIG: Progress-Datei hat Vorrang √ºber Input-Datei\n",
    "# Wenn wir resumed, laden wir den kompletten Zwischenstand\n",
    "if not RESET_FUSION and progress_file.exists():\n",
    "    print(\"üìÇ Lade Fortschritt aus Progress-Datei...\")\n",
    "    df_enriched = pd.read_parquet(progress_file)\n",
    "    \n",
    "    if not df_enriched.index.is_unique:\n",
    "        df_enriched = df_enriched[~df_enriched.index.duplicated(keep='last')]\n",
    "    \n",
    "    # Bestimme bereits fusionierte Records\n",
    "    if 'fusion_title_source' in df_enriched.columns:\n",
    "        already_fused = set(df_enriched[df_enriched['fusion_title_source'].notna()].index)\n",
    "    else:\n",
    "        already_fused = set()\n",
    "    \n",
    "    print(f\"   ‚úÖ {len(already_fused):,} Records bereits fusioniert\")\n",
    "    print(f\"   ‚úÖ Gesamte Daten wiederhergestellt aus Progress-Datei\")\n",
    "else:\n",
    "    already_fused = set()\n",
    "\n",
    "# Statistics BEFORE fusion\n",
    "print(\"\\nüìä Vollst√§ndigkeit VOR Fusion:\")\n",
    "before_stats = {\n",
    "    'title': df_enriched['title'].notna().sum(),\n",
    "    'authors': (df_enriched['authors_str'].notna() & (df_enriched['authors_str'] != '')).sum(),\n",
    "    'year': df_enriched['year'].notna().sum(),\n",
    "    'publisher': df_enriched['publisher'].notna().sum()\n",
    "}\n",
    "for field, count in before_stats.items():\n",
    "    print(f\"   {field}: {count:,} ({count/len(df_enriched)*100:.1f}%)\")\n",
    "\n",
    "# Identify records to process (those with any DNB variant)\n",
    "has_id = df_enriched[['dnb_title','dnb_authors','dnb_year','dnb_publisher']].notna().any(axis=1) if 'dnb_title' in df_enriched.columns else False\n",
    "has_ta = df_enriched[['dnb_title_ta','dnb_authors_ta','dnb_year_ta','dnb_publisher_ta']].notna().any(axis=1) if 'dnb_title_ta' in df_enriched.columns else False\n",
    "records_to_process = df_enriched[has_id | has_ta].copy()\n",
    "\n",
    "total_with_dnb = len(records_to_process)\n",
    "print(f\"\\nüîÑ Records mit DNB-Varianten: {total_with_dnb:,}\")\n",
    "\n",
    "# Filter already processed\n",
    "records_to_process = records_to_process[~records_to_process.index.isin(already_fused)]\n",
    "\n",
    "# Apply limit if in test mode\n",
    "if FUSION_LIMIT and FUSION_LIMIT > 0:\n",
    "    print(f\"üß™ Testmodus aktiv ‚Äì verarbeite nur die ersten {FUSION_LIMIT} Records.\")\n",
    "    records_to_process = records_to_process.head(FUSION_LIMIT)\n",
    "\n",
    "# Load and prioritize retry queue\n",
    "retry_indices = []\n",
    "if retry_queue_file.exists():\n",
    "    try:\n",
    "        with open(retry_queue_file, 'r', encoding='utf-8') as f:\n",
    "            retry_indices = json.load(f)\n",
    "    except Exception:\n",
    "        retry_indices = []\n",
    "\n",
    "retry_indices = [i for i in retry_indices if i in records_to_process.index]\n",
    "if len(retry_indices) > 0:\n",
    "    print(f\"üîÅ Retry-Queue: {len(retry_indices):,} Records werden zuerst verarbeitet\")\n",
    "    retry_df = records_to_process.loc[records_to_process.index.isin(retry_indices)]\n",
    "    fresh_df = records_to_process.loc[~records_to_process.index.isin(retry_indices)]\n",
    "    records_to_process = pd.concat([retry_df, fresh_df], axis=0)\n",
    "\n",
    "print(f\"üîÑ Verbleibende Records: {len(records_to_process):,}\")\n",
    "print(f\"   (Bereits fusioniert: {len(already_fused):,})\\n\")\n",
    "\n",
    "# Initialize statistics\n",
    "fusion_stats = {\n",
    "    'total_processed': len(already_fused),\n",
    "    'conflicts_found': 0,\n",
    "    'dnb_preferred': 0,\n",
    "    'simple_merges': 0,\n",
    "    'errors': 0,\n",
    "    'dnb_matches_rejected': 0,\n",
    "    'ai_decisions': 0,\n",
    "    'variant_id': 0,\n",
    "    'variant_title_author': 0,\n",
    "    'variant_none': 0\n",
    "}\n",
    "\n",
    "fusion_count = 0\n",
    "aborted = False\n",
    "\n",
    "# Main fusion loop\n",
    "for idx, row in tqdm(records_to_process.iterrows(), total=len(records_to_process), desc=\"üîÑ Fusion\", unit=\"records\"):\n",
    "    try:\n",
    "        # Perform fusion\n",
    "        result = fusion_engine.merge_record(row)\n",
    "        result_dict = result.to_dict()\n",
    "        \n",
    "        # Update statistics\n",
    "        variant = result_dict.get('dnb_variant_selected')\n",
    "        if variant == 'id':\n",
    "            fusion_stats['variant_id'] += 1\n",
    "        elif variant == 'title_author':\n",
    "            fusion_stats['variant_title_author'] += 1\n",
    "        else:\n",
    "            fusion_stats['variant_none'] += 1\n",
    "        \n",
    "        # Store results in DataFrame\n",
    "        df_enriched.loc[idx, 'title'] = result_dict.get('title')\n",
    "        df_enriched.loc[idx, 'authors_str'] = result_dict.get('authors')\n",
    "        \n",
    "        # Convert year to numeric\n",
    "        year_val = result_dict.get('year')\n",
    "        if pd.notna(year_val):\n",
    "            try:\n",
    "                df_enriched.loc[idx, 'year'] = pd.to_numeric(year_val, errors='coerce')\n",
    "            except:\n",
    "                df_enriched.loc[idx, 'year'] = year_val\n",
    "        \n",
    "        df_enriched.loc[idx, 'publisher'] = result_dict.get('publisher')\n",
    "        df_enriched.loc[idx, 'fusion_title_source'] = result_dict.get('title_source')\n",
    "        df_enriched.loc[idx, 'fusion_authors_source'] = result_dict.get('authors_source')\n",
    "        df_enriched.loc[idx, 'fusion_year_source'] = result_dict.get('year_source')\n",
    "        df_enriched.loc[idx, 'fusion_publisher_source'] = result_dict.get('publisher_source')\n",
    "        df_enriched.loc[idx, 'fusion_conflicts'] = result_dict.get('conflicts')\n",
    "        df_enriched.loc[idx, 'fusion_confirmations'] = result_dict.get('confirmations')\n",
    "        df_enriched.loc[idx, 'fusion_ai_reasoning'] = result_dict.get('ai_reasoning')\n",
    "        df_enriched.loc[idx, 'fusion_dnb_match_rejected'] = result_dict.get('dnb_match_rejected', False)\n",
    "        df_enriched.loc[idx, 'fusion_rejection_reason'] = result_dict.get('rejection_reason')\n",
    "        df_enriched.loc[idx, 'fusion_dnb_variant_selected'] = result_dict.get('dnb_variant_selected')\n",
    "        \n",
    "        # Clear retry flag if set\n",
    "        if 'fusion_needs_retry' in df_enriched.columns:\n",
    "            df_enriched.loc[idx, 'fusion_needs_retry'] = False\n",
    "        if idx in retry_indices:\n",
    "            retry_indices = [i for i in retry_indices if i != idx]\n",
    "        \n",
    "        # Update statistics\n",
    "        fusion_stats['total_processed'] += 1\n",
    "        fusion_count += 1\n",
    "        fusion_stats['ai_decisions'] += 1\n",
    "        \n",
    "        if result_dict.get('dnb_match_rejected'):\n",
    "            fusion_stats['dnb_matches_rejected'] += 1\n",
    "        elif result_dict.get('conflicts'):\n",
    "            fusion_stats['conflicts_found'] += 1\n",
    "            fusion_stats['dnb_preferred'] += 1\n",
    "        else:\n",
    "            fusion_stats['simple_merges'] += 1\n",
    "        \n",
    "        # Incremental save\n",
    "        if fusion_count % SAVE_INTERVAL == 0:\n",
    "            df_enriched.to_parquet(progress_file, index=True)\n",
    "            with open(retry_queue_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(retry_indices, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"\\nüíæ Zwischenstand: {fusion_stats['total_processed']:,} Records fusioniert\")\n",
    "    \n",
    "    except OllamaUnavailableError as e:\n",
    "        print(f\"\\n‚ùå Ollama nicht erreichbar: {e}\")\n",
    "        print(\"üëâ Record wird in die Retry-Queue gelegt\")\n",
    "        fusion_stats['errors'] += 1\n",
    "        aborted = True\n",
    "        \n",
    "        df_enriched.loc[idx, 'fusion_needs_retry'] = True\n",
    "        if idx not in retry_indices:\n",
    "            retry_indices.append(idx)\n",
    "        \n",
    "        # Save immediately\n",
    "        df_enriched.to_parquet(progress_file, index=True)\n",
    "        with open(retry_queue_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(retry_indices, f, ensure_ascii=False, indent=2)\n",
    "        break\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è Fehler bei Record {idx}: {e}\")\n",
    "        fusion_stats['errors'] += 1\n",
    "\n",
    "# Final save\n",
    "if fusion_count % SAVE_INTERVAL != 0 or fusion_count == 0:\n",
    "    df_enriched.to_parquet(progress_file, index=True)\n",
    "    with open(retry_queue_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(retry_indices, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"\\nüíæ Finaler Stand gespeichert\")\n",
    "\n",
    "if aborted:\n",
    "    print(\"\\n‚õîÔ∏è Lauf abgebrochen (Ollama-Timeout)\")\n",
    "\n",
    "print(\"\\n‚úÖ Fusion abgeschlossen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y9ismt9zsql",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# üåç LANGUAGE FUSION (Dual-Source Strategie)\n",
    "print(\"\\nüåç === LANGUAGE FUSION ===\\n\")\n",
    "\n",
    "def merge_language(row):\n",
    "    \"\"\"\n",
    "    Merge MARC21 language and langdetect results.\n",
    "    \n",
    "    Priority:\n",
    "    1. MARC21 language (from catalog metadata - most reliable)\n",
    "    2. langdetect detected_language (from title analysis)\n",
    "    3. None if neither available\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (language_final, language_source, language_confidence)\n",
    "    \"\"\"\n",
    "    marc21_lang = row.get('language')\n",
    "    detected_lang = row.get('detected_language')\n",
    "    detected_conf = row.get('detected_language_confidence', 0.0)\n",
    "    \n",
    "    # MARC21 has priority\n",
    "    if pd.notna(marc21_lang) and str(marc21_lang).strip() not in ['', 'unknown']:\n",
    "        return str(marc21_lang).strip(), 'marc21', 1.0\n",
    "    \n",
    "    # Fallback to langdetect\n",
    "    elif pd.notna(detected_lang) and str(detected_lang).strip() not in ['', 'unknown']:\n",
    "        return str(detected_lang).strip(), 'langdetect', float(detected_conf) if pd.notna(detected_conf) else 0.0\n",
    "    \n",
    "    # No language information\n",
    "    else:\n",
    "        return None, None, 0.0\n",
    "\n",
    "# Apply language fusion\n",
    "if 'language' in df_enriched.columns or 'detected_language' in df_enriched.columns:\n",
    "    print(\"üìä Applying dual-source language fusion...\")\n",
    "    \n",
    "    # Create new columns for merged language\n",
    "    df_enriched[['language_final', 'language_source', 'language_confidence']] = df_enriched.apply(\n",
    "        merge_language, axis=1, result_type='expand'\n",
    "    )\n",
    "    \n",
    "    # Statistics\n",
    "    marc21_count = df_enriched[df_enriched['language_source'] == 'marc21'].shape[0]\n",
    "    langdetect_count = df_enriched[df_enriched['language_source'] == 'langdetect'].shape[0]\n",
    "    total_with_lang = df_enriched['language_final'].notna().sum()\n",
    "    \n",
    "    print(f\"\\nüìä Language Fusion Results:\")\n",
    "    print(f\"   Total with language: {total_with_lang:,} ({total_with_lang/len(df_enriched)*100:.1f}%)\")\n",
    "    print(f\"   From MARC21: {marc21_count:,} ({marc21_count/len(df_enriched)*100:.1f}%)\")\n",
    "    print(f\"   From langdetect: {langdetect_count:,} ({langdetect_count/len(df_enriched)*100:.1f}%)\")\n",
    "    print(f\"   No language: {len(df_enriched) - total_with_lang:,} ({(len(df_enriched) - total_with_lang)/len(df_enriched)*100:.1f}%)\")\n",
    "    \n",
    "    # Language distribution\n",
    "    print(f\"\\nüåç Top 10 Languages (final):\")\n",
    "    lang_dist = df_enriched['language_final'].value_counts().head(10)\n",
    "    for lang, count in lang_dist.items():\n",
    "        if pd.notna(lang):\n",
    "            pct = count/total_with_lang*100 if total_with_lang > 0 else 0\n",
    "            print(f\"   {str(lang):10}: {count:6,} ({pct:5.1f}%)\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Language fusion complete\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No language columns found - skipping language fusion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc12345",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# üìä FUSION-STATISTIKEN\n",
    "print(\"üìä === FUSION-ERGEBNISSE ===\\n\")\n",
    "\n",
    "# Statistics AFTER fusion\n",
    "print(\"üìä Vollst√§ndigkeit NACH Fusion:\")\n",
    "after_stats = {\n",
    "    'title': df_enriched['title'].notna().sum(),\n",
    "    'authors': (df_enriched['authors_str'].notna() & (df_enriched['authors_str'] != '')).sum(),\n",
    "    'year': df_enriched['year'].notna().sum(),\n",
    "    'publisher': df_enriched['publisher'].notna().sum()\n",
    "}\n",
    "for field, count in after_stats.items():\n",
    "    improvement = count - before_stats[field]\n",
    "    print(f\"   {field}: {count:,} ({count/len(df_enriched)*100:.1f}%) [+{improvement:,}]\")\n",
    "\n",
    "# Fusion statistics\n",
    "print(f\"\\nüìä Fusion-Statistiken:\")\n",
    "print(f\"   Verarbeitet: {fusion_stats['total_processed']:,}\")\n",
    "print(f\"   Einfache Merges: {fusion_stats['simple_merges']:,}\")\n",
    "print(f\"   DNB gew√§hlt: {fusion_stats['dnb_preferred']:,}\")\n",
    "print(f\"   Konflikte: {fusion_stats['conflicts_found']:,}\")\n",
    "print(f\"   üö´ DNB verworfen: {fusion_stats['dnb_matches_rejected']:,}\")\n",
    "print(f\"   KI-Entscheidungen: {fusion_stats['ai_decisions']:,}\")\n",
    "print(f\"   Variante ID: {fusion_stats['variant_id']:,}\")\n",
    "print(f\"   Variante Titel/Autor: {fusion_stats['variant_title_author']:,}\")\n",
    "\n",
    "# Source distribution\n",
    "print(f\"\\nüìä Datenquellen:\")\n",
    "for field in ['title', 'authors', 'year', 'publisher']:\n",
    "    source_col = f'fusion_{field}_source'\n",
    "    if source_col in df_enriched.columns:\n",
    "        sources = df_enriched[source_col].value_counts()\n",
    "        print(f\"\\n   {field.upper()}:\")\n",
    "        for source, count in sources.items():\n",
    "            if source:\n",
    "                print(f\"     {source}: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gap_filling_logic",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# üìù GAP FILLING: Fehlende Felder aus DNB-Daten erg√§nzen\n",
    "print(\"\\nüìù === GAP FILLING ===\\n\")\n",
    "\n",
    "print(\"F√ºlle fehlende Metadaten aus DNB-Daten...\\n\")\n",
    "\n",
    "# Statistics BEFORE gap filling\n",
    "before_gap_filling = {\n",
    "    'isbn': df_enriched['isbn'].notna().sum(),\n",
    "    'issn': df_enriched['issn'].notna().sum() if 'issn' in df_enriched.columns else 0,\n",
    "}\n",
    "\n",
    "filled_count = {\n",
    "    'isbn': 0,\n",
    "    'issn': 0,\n",
    "    'authors': 0,\n",
    "    'year': 0,\n",
    "    'publisher': 0\n",
    "}\n",
    "\n",
    "# 1. ISBN Gap Filling\n",
    "# Priority: dnb_isbn_ta (from title/author search - finds new ISBNs) > dnb_isbn (from ISBN search - only duplicates)\n",
    "if 'dnb_isbn_ta' in df_enriched.columns:\n",
    "    # Records with no ISBN but DNB has one\n",
    "    no_isbn = df_enriched['isbn'].isna()\n",
    "    has_dnb_isbn_ta = df_enriched['dnb_isbn_ta'].notna()\n",
    "    \n",
    "    fill_isbn_mask = no_isbn & has_dnb_isbn_ta\n",
    "    filled_count['isbn'] = fill_isbn_mask.sum()\n",
    "    \n",
    "    if filled_count['isbn'] > 0:\n",
    "        df_enriched.loc[fill_isbn_mask, 'isbn'] = df_enriched.loc[fill_isbn_mask, 'dnb_isbn_ta']\n",
    "        # Mark source\n",
    "        if 'isbn_source' not in df_enriched.columns:\n",
    "            df_enriched['isbn_source'] = None\n",
    "        df_enriched.loc[fill_isbn_mask, 'isbn_source'] = 'dnb_title_author'\n",
    "        \n",
    "        print(f\"   ISBN: {filled_count['isbn']:,} neu gef√ºllt aus dnb_isbn_ta\")\n",
    "\n",
    "# 2. ISSN Gap Filling\n",
    "if 'issn' in df_enriched.columns and 'dnb_issn_ta' in df_enriched.columns:\n",
    "    no_issn = df_enriched['issn'].isna()\n",
    "    has_dnb_issn_ta = df_enriched['dnb_issn_ta'].notna()\n",
    "    \n",
    "    fill_issn_mask = no_issn & has_dnb_issn_ta\n",
    "    filled_count['issn'] = fill_issn_mask.sum()\n",
    "    \n",
    "    if filled_count['issn'] > 0:\n",
    "        df_enriched.loc[fill_issn_mask, 'issn'] = df_enriched.loc[fill_issn_mask, 'dnb_issn_ta']\n",
    "        # Mark source\n",
    "        if 'issn_source' not in df_enriched.columns:\n",
    "            df_enriched['issn_source'] = None\n",
    "        df_enriched.loc[fill_issn_mask, 'issn_source'] = 'dnb_title_author'\n",
    "        \n",
    "        print(f\"   ISSN: {filled_count['issn']:,} neu gef√ºllt aus dnb_issn_ta\")\n",
    "\n",
    "# 3. Authors Gap Filling (from DNB where fusion didn't already fill)\n",
    "# This fills authors that were NOT handled by fusion (e.g., records without fusion)\n",
    "no_authors = (df_enriched['authors_str'].isna() | (df_enriched['authors_str'] == ''))\n",
    "not_fused = df_enriched['fusion_authors_source'].isna()\n",
    "\n",
    "# Try dnb_authors_ta first\n",
    "if 'dnb_authors_ta' in df_enriched.columns:\n",
    "    has_dnb_authors_ta = (df_enriched['dnb_authors_ta'].notna() & (df_enriched['dnb_authors_ta'] != ''))\n",
    "    fill_authors_mask = no_authors & not_fused & has_dnb_authors_ta\n",
    "    \n",
    "    if fill_authors_mask.sum() > 0:\n",
    "        df_enriched.loc[fill_authors_mask, 'authors_str'] = df_enriched.loc[fill_authors_mask, 'dnb_authors_ta']\n",
    "        df_enriched.loc[fill_authors_mask, 'fusion_authors_source'] = 'dnb_title_author_gap_fill'\n",
    "        filled_count['authors'] += fill_authors_mask.sum()\n",
    "\n",
    "# Then try dnb_authors\n",
    "if 'dnb_authors' in df_enriched.columns:\n",
    "    no_authors = (df_enriched['authors_str'].isna() | (df_enriched['authors_str'] == ''))\n",
    "    not_fused = df_enriched['fusion_authors_source'].isna()\n",
    "    has_dnb_authors = (df_enriched['dnb_authors'].notna() & (df_enriched['dnb_authors'] != ''))\n",
    "    fill_authors_mask = no_authors & not_fused & has_dnb_authors\n",
    "    \n",
    "    if fill_authors_mask.sum() > 0:\n",
    "        df_enriched.loc[fill_authors_mask, 'authors_str'] = df_enriched.loc[fill_authors_mask, 'dnb_authors']\n",
    "        df_enriched.loc[fill_authors_mask, 'fusion_authors_source'] = 'dnb_id_gap_fill'\n",
    "        filled_count['authors'] += fill_authors_mask.sum()\n",
    "\n",
    "if filled_count['authors'] > 0:\n",
    "    print(f\"   Authors: {filled_count['authors']:,} neu gef√ºllt aus DNB\")\n",
    "\n",
    "# 4. Year Gap Filling\n",
    "no_year = df_enriched['year'].isna()\n",
    "not_fused = df_enriched['fusion_year_source'].isna()\n",
    "\n",
    "# Try dnb_year_ta first\n",
    "if 'dnb_year_ta' in df_enriched.columns:\n",
    "    has_dnb_year_ta = df_enriched['dnb_year_ta'].notna()\n",
    "    fill_year_mask = no_year & not_fused & has_dnb_year_ta\n",
    "    \n",
    "    if fill_year_mask.sum() > 0:\n",
    "        df_enriched.loc[fill_year_mask, 'year'] = df_enriched.loc[fill_year_mask, 'dnb_year_ta']\n",
    "        df_enriched.loc[fill_year_mask, 'fusion_year_source'] = 'dnb_title_author_gap_fill'\n",
    "        filled_count['year'] += fill_year_mask.sum()\n",
    "\n",
    "# Then try dnb_year\n",
    "if 'dnb_year' in df_enriched.columns:\n",
    "    no_year = df_enriched['year'].isna()\n",
    "    not_fused = df_enriched['fusion_year_source'].isna()\n",
    "    has_dnb_year = df_enriched['dnb_year'].notna()\n",
    "    fill_year_mask = no_year & not_fused & has_dnb_year\n",
    "    \n",
    "    if fill_year_mask.sum() > 0:\n",
    "        df_enriched.loc[fill_year_mask, 'year'] = df_enriched.loc[fill_year_mask, 'dnb_year']\n",
    "        df_enriched.loc[fill_year_mask, 'fusion_year_source'] = 'dnb_id_gap_fill'\n",
    "        filled_count['year'] += fill_year_mask.sum()\n",
    "\n",
    "if filled_count['year'] > 0:\n",
    "    print(f\"   Year: {filled_count['year']:,} neu gef√ºllt aus DNB\")\n",
    "\n",
    "# 5. Publisher Gap Filling\n",
    "no_publisher = df_enriched['publisher'].isna()\n",
    "not_fused = df_enriched['fusion_publisher_source'].isna()\n",
    "\n",
    "# Try dnb_publisher_ta first\n",
    "if 'dnb_publisher_ta' in df_enriched.columns:\n",
    "    has_dnb_pub_ta = df_enriched['dnb_publisher_ta'].notna()\n",
    "    fill_pub_mask = no_publisher & not_fused & has_dnb_pub_ta\n",
    "    \n",
    "    if fill_pub_mask.sum() > 0:\n",
    "        df_enriched.loc[fill_pub_mask, 'publisher'] = df_enriched.loc[fill_pub_mask, 'dnb_publisher_ta']\n",
    "        df_enriched.loc[fill_pub_mask, 'fusion_publisher_source'] = 'dnb_title_author_gap_fill'\n",
    "        filled_count['publisher'] += fill_pub_mask.sum()\n",
    "\n",
    "# Then try dnb_publisher\n",
    "if 'dnb_publisher' in df_enriched.columns:\n",
    "    no_publisher = df_enriched['publisher'].isna()\n",
    "    not_fused = df_enriched['fusion_publisher_source'].isna()\n",
    "    has_dnb_pub = df_enriched['dnb_publisher'].notna()\n",
    "    fill_pub_mask = no_publisher & not_fused & has_dnb_pub\n",
    "    \n",
    "    if fill_pub_mask.sum() > 0:\n",
    "        df_enriched.loc[fill_pub_mask, 'publisher'] = df_enriched.loc[fill_pub_mask, 'dnb_publisher']\n",
    "        df_enriched.loc[fill_pub_mask, 'fusion_publisher_source'] = 'dnb_id_gap_fill'\n",
    "        filled_count['publisher'] += fill_pub_mask.sum()\n",
    "\n",
    "if filled_count['publisher'] > 0:\n",
    "    print(f\"   Publisher: {filled_count['publisher']:,} neu gef√ºllt aus DNB\")\n",
    "\n",
    "# Statistics AFTER gap filling\n",
    "after_gap_filling = {\n",
    "    'isbn': df_enriched['isbn'].notna().sum(),\n",
    "    'issn': df_enriched['issn'].notna().sum() if 'issn' in df_enriched.columns else 0,\n",
    "}\n",
    "\n",
    "print(\"\\nüìä Gap Filling Zusammenfassung:\")\n",
    "total_filled = sum(filled_count.values())\n",
    "print(f\"   Gesamt neu gef√ºllt: {total_filled:,} Felder\")\n",
    "print(f\"   ISBN: {before_gap_filling['isbn']:,} ‚Üí {after_gap_filling['isbn']:,} (+{filled_count['isbn']:,})\")\n",
    "print(f\"   ISSN: {before_gap_filling['issn']:,} ‚Üí {after_gap_filling['issn']:,} (+{filled_count['issn']:,})\")\n",
    "\n",
    "print(\"\\n‚úÖ Gap Filling abgeschlossen\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def45678",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# üíæ FINALE AUSGABE SPEICHERN\n",
    "import numpy as np\n",
    "\n",
    "output_path = processed_dir / '05_fused_data.parquet'\n",
    "output_metadata_path = processed_dir / '05_metadata.json'\n",
    "\n",
    "# Save fused data\n",
    "df_enriched.to_parquet(output_path, index=True)\n",
    "print(f\"üíæ Fusionierte Daten gespeichert: {output_path}\")\n",
    "print(f\"   Gr√∂√üe: {output_path.stat().st_size / 1024**2:.1f} MB\")\n",
    "\n",
    "# Helper function to convert numpy/pandas types to native Python types\n",
    "def convert_to_native(obj):\n",
    "    \"\"\"Recursively convert numpy/pandas types to native Python types for JSON serialization.\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_native(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_native(item) for item in obj]\n",
    "    elif isinstance(obj, (np.integer, np.int64)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif pd.isna(obj):\n",
    "        return None\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Save metadata with type conversion\n",
    "metadata = {\n",
    "    'notebook': '05_vdeh_data_fusion',\n",
    "    'timestamp': pd.Timestamp.now().isoformat(),\n",
    "    'input_file': str(input_path),\n",
    "    'output_file': str(output_path),\n",
    "    'total_records': int(len(df_enriched)),\n",
    "    'fusion_statistics': convert_to_native(fusion_stats),\n",
    "    'completeness_before': convert_to_native(before_stats),\n",
    "    'completeness_after': convert_to_native(after_stats),\n",
    "    'previous_metadata': prev_metadata\n",
    "}\n",
    "\n",
    "with open(output_metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"üìã Metadaten gespeichert: {output_metadata_path}\")\n",
    "print(f\"\\n‚úÖ Pipeline-Stufe 05 abgeschlossen!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bibo-analysis-DoEGeq_l-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "notebooks/01_vdeh_preprocessing/05_vdeh_data_fusion.ipynb",
   "output_path": "notebooks/01_vdeh_preprocessing/05_vdeh_data_fusion_output.ipynb",
   "parameters": {},
   "start_time": "2025-12-25T21:02:28.350002",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}