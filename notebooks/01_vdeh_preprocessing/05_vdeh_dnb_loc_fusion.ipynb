{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-Quellen Datenfusion: VDEH + DNB + LoC\n",
    "\n",
    "**Ziel:** VollstÃ¤ndige Fusion aller bibliographischen Daten aus drei Quellen\n",
    "\n",
    "## Datenquellen\n",
    "- **VDEH**: Basis-Katalogdaten (58.305 DatensÃ¤tze)\n",
    "- **DNB**: Deutsche Nationalbibliothek Anreicherung\n",
    "- **LoC**: Library of Congress Anreicherung\n",
    "\n",
    "## Fusion-Strategie\n",
    "- 6 Varianten: DNB-ID (A), DNB-TA (B), DNB-TY (C), LoC-ID (D), LoC-TA (E), LoC-TY (F)\n",
    "- Sprachbasierte Priorisierung (DNB fÃ¼r Deutsch, LoC fÃ¼r Englisch)\n",
    "- KI-gestÃ¼tzte KonfliktauflÃ¶sung\n",
    "- VollstÃ¤ndiges Tracking aller Entscheidungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 12:35:09 - utils.notebook_utils - INFO - Searching for project root...\n",
      "2026-01-02 12:35:09 - utils.notebook_utils - INFO - Project root found: /media/sz/Data/Bibo/analysis\n",
      "2026-01-02 12:35:09 - utils.notebook_utils - INFO - Loading configuration...\n",
      "2026-01-02 12:35:09 - config_loader - INFO - Configuration loaded from /media/sz/Data/Bibo/analysis/config.yaml\n",
      "2026-01-02 12:35:09 - utils.notebook_utils - INFO - Configuration loaded successfully: Dual-Source Bibliothek Bestandsvergleich\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Project root: /media/sz/Data/Bibo/analysis\n",
      "âœ… Project: Dual-Source Bibliothek Bestandsvergleich v2.2.0\n"
     ]
    }
   ],
   "source": [
    "# ðŸ› ï¸ SETUP\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Add src to path\n",
    "project_root = Path.cwd()\n",
    "while not (project_root / 'config.yaml').exists() and project_root.parent != project_root:\n",
    "    project_root = project_root.parent\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "from utils.notebook_utils import setup_notebook\n",
    "from fusion import FusionEngine, OllamaClient\n",
    "\n",
    "project_root, config = setup_notebook()\n",
    "print(f\"âœ… Project root: {project_root}\")\n",
    "print(f\"âœ… Project: {config.get('project.name')} v{config.get('project.version')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ðŸ“‚ LOAD DATA\nprocessed_dir = config.project_root / config.get('paths.data.vdeh.processed')\n\n# Load VDEH base data (with language detection)\nvdeh_path = processed_dir / '03_language_detected_data.parquet'\ndf_vdeh = pd.read_parquet(vdeh_path)\nprint(f\"ðŸ“‚ VDEH: {len(df_vdeh):,} records\")\n\n# Load DNB enriched data\ndnb_path = processed_dir / '04_dnb_enriched_data.parquet'\ndf_dnb = pd.read_parquet(dnb_path)\nprint(f\"ðŸ“‚ DNB:  {len(df_dnb):,} records\")\n\n# Load LoC enriched data\nloc_path = processed_dir / '04b_loc_enriched_data.parquet'\ndf_loc = pd.read_parquet(loc_path)\nprint(f\"ðŸ“‚ LoC:  {len(df_loc):,} records\")\n\n# Check if record counts match\nif len(df_vdeh) != len(df_dnb):\n    print(f\"\\nâš ï¸  WARNING: DNB has {len(df_dnb)} records vs VDEH's {len(df_vdeh)}\")\n    print(f\"   Trimming DNB to match VDEH length...\")\n    df_dnb = df_dnb.iloc[:len(df_vdeh)]\n    \nif len(df_vdeh) != len(df_loc):\n    print(f\"\\nâš ï¸  WARNING: LoC has {len(df_loc)} records vs VDEH's {len(df_vdeh)}\")\n    print(f\"   Trimming LoC to match VDEH length...\")\n    df_loc = df_loc.iloc[:len(df_vdeh)]\n\n# Reset indices to ensure alignment\nprint(f\"\\nðŸ”§ Aligning indices...\")\ndf_vdeh = df_vdeh.reset_index(drop=True)\ndf_dnb = df_dnb.reset_index(drop=True)\ndf_loc = df_loc.reset_index(drop=True)\n\n# Verify indices match\nassert df_vdeh.index.equals(df_dnb.index), \"DNB indices don't match VDEH\"\nassert df_vdeh.index.equals(df_loc.index), \"LoC indices don't match VDEH\"\nprint(f\"âœ… All indices match - {len(df_vdeh):,} records each\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š DATA OVERVIEW\n",
    "print(\"ðŸ“Š DatenÃ¼bersicht:\\n\")\n",
    "\n",
    "# DNB coverage\n",
    "dnb_cols = ['dnb_title', 'dnb_title_ta', 'dnb_title_ty']\n",
    "has_dnb = df_dnb[dnb_cols].notna().any(axis=1)\n",
    "print(f\"DNB Anreicherung:\")\n",
    "print(f\"   DatensÃ¤tze mit DNB: {has_dnb.sum():,} ({has_dnb.sum()/len(df_dnb)*100:.1f}%)\")\n",
    "print(f\"   - ID-basiert:  {df_dnb['dnb_title'].notna().sum():,}\")\n",
    "print(f\"   - Title/Author: {df_dnb['dnb_title_ta'].notna().sum():,}\")\n",
    "print(f\"   - Title/Year:   {df_dnb['dnb_title_ty'].notna().sum():,}\")\n",
    "\n",
    "# LoC coverage\n",
    "loc_cols = ['loc_title', 'loc_title_ta', 'loc_title_ty']\n",
    "has_loc = df_loc[loc_cols].notna().any(axis=1)\n",
    "print(f\"\\nLoC Anreicherung:\")\n",
    "print(f\"   DatensÃ¤tze mit LoC: {has_loc.sum():,} ({has_loc.sum()/len(df_loc)*100:.1f}%)\")\n",
    "print(f\"   - ID-basiert:  {df_loc['loc_title'].notna().sum():,}\")\n",
    "print(f\"   - Title/Author: {df_loc['loc_title_ta'].notna().sum():,}\")\n",
    "print(f\"   - Title/Year:   {df_loc['loc_title_ty'].notna().sum():,}\")\n",
    "\n",
    "# Combined\n",
    "has_both = has_dnb & has_loc\n",
    "has_either = has_dnb | has_loc\n",
    "print(f\"\\nKombiniert:\")\n",
    "print(f\"   Mit DNB oder LoC: {has_either.sum():,} ({has_either.sum()/len(df_vdeh)*100:.1f}%)\")\n",
    "print(f\"   Mit DNB und LoC:  {has_both.sum():,} ({has_both.sum()/len(df_vdeh)*100:.1f}%)\")\n",
    "print(f\"   Nur VDEH:         {(~has_either).sum():,} ({(~has_either).sum()/len(df_vdeh)*100:.1f}%)\")\n",
    "\n",
    "# Language distribution\n",
    "print(f\"\\nðŸŒ Sprachverteilung:\")\n",
    "lang_dist = df_vdeh['detected_language'].value_counts()\n",
    "for lang, count in lang_dist.head(5).items():\n",
    "    print(f\"   {lang}: {count:,} ({count/len(df_vdeh)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”„ MERGE DATA SOURCES\n",
    "print(\"ðŸ”„ ZusammenfÃ¼hren der Datenquellen...\\n\")\n",
    "\n",
    "# DNB columns to merge (DNB has no pages field)\n",
    "dnb_merge_cols = [\n",
    "    # ID variant\n",
    "    'dnb_title', 'dnb_authors', 'dnb_year', 'dnb_publisher', 'dnb_isbn', 'dnb_issn',\n",
    "    # Title/Author variant\n",
    "    'dnb_title_ta', 'dnb_authors_ta', 'dnb_year_ta', 'dnb_publisher_ta', 'dnb_isbn_ta', 'dnb_issn_ta',\n",
    "    # Title/Year variant\n",
    "    'dnb_title_ty', 'dnb_authors_ty', 'dnb_year_ty', 'dnb_publisher_ty', 'dnb_isbn_ty', 'dnb_issn_ty',\n",
    "    # Metadata\n",
    "    'dnb_query_method'\n",
    "]\n",
    "\n",
    "# LoC columns to merge\n",
    "loc_merge_cols = [\n",
    "    # ID variant\n",
    "    'loc_title', 'loc_authors', 'loc_year', 'loc_publisher', 'loc_isbn', 'loc_issn', 'loc_pages',\n",
    "    # Title/Author variant\n",
    "    'loc_title_ta', 'loc_authors_ta', 'loc_year_ta', 'loc_publisher_ta', 'loc_isbn_ta', 'loc_issn_ta', 'loc_pages_ta',\n",
    "    # Title/Year variant\n",
    "    'loc_title_ty', 'loc_authors_ty', 'loc_year_ty', 'loc_publisher_ty', 'loc_isbn_ty', 'loc_issn_ty', 'loc_pages_ty',\n",
    "    # Metadata\n",
    "    'loc_query_method'\n",
    "]\n",
    "\n",
    "# Merge all sources\n",
    "df_merged = df_vdeh.copy()\n",
    "df_merged = df_merged.join(df_dnb[dnb_merge_cols], how='left')\n",
    "df_merged = df_merged.join(df_loc[loc_merge_cols], how='left')\n",
    "\n",
    "print(f\"âœ… Merged dataset: {len(df_merged):,} records, {len(df_merged.columns)} columns\")\n",
    "print(f\"\\nðŸ“‹ Columns: {df_merged.columns.tolist()[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ¤– INITIALIZE FUSION ENGINE\n",
    "print(\"ðŸ¤– Initialisiere FusionEngine...\\n\")\n",
    "\n",
    "# Initialize Ollama client\n",
    "ollama_client = OllamaClient(\n",
    "    api_url=\"http://localhost:11434/api/generate\",\n",
    "    model=\"llama3.3:70b\",\n",
    "    timeout_sec=220,\n",
    "    max_retries=4\n",
    ")\n",
    "\n",
    "# Test connection\n",
    "if ollama_client.test_connection():\n",
    "    print(f\"âœ… Ollama verbunden: {ollama_client.model}\")\n",
    "else:\n",
    "    raise RuntimeError(\"âŒ Ollama nicht erreichbar\")\n",
    "\n",
    "# Initialize FusionEngine\n",
    "engine = FusionEngine(\n",
    "    ollama_client=ollama_client,\n",
    "    enable_loc=True,  # Enable LoC fusion\n",
    "    ty_similarity_threshold=0.7\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… FusionEngine initialisiert\")\n",
    "print(f\"   - 3-Quellen-Fusion: VDEH + DNB + LoC\")\n",
    "print(f\"   - 6 Varianten: A-F (DNB-ID/TA/TY, LoC-ID/TA/TY)\")\n",
    "print(f\"   - Sprachbasierte Priorisierung\")\n",
    "print(f\"   - VollstÃ¤ndiges Tracking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“¦ CHECK FOR EXISTING CHECKPOINTS\n",
    "print(\"ðŸ“¦ PrÃ¼fe auf bestehende Checkpoints...\\n\")\n",
    "\n",
    "# Find all checkpoint files\n",
    "checkpoint_files = sorted(processed_dir.glob('06_fusion_checkpoint_*.parquet'))\n",
    "\n",
    "if checkpoint_files:\n",
    "    # Get the latest checkpoint by extracting the record count from filename\n",
    "    checkpoint_info = []\n",
    "    for cp_file in checkpoint_files:\n",
    "        # Extract number from filename: 06_fusion_checkpoint_15000.parquet -> 15000\n",
    "        num_records = int(cp_file.stem.split('_')[-1])\n",
    "        checkpoint_info.append((num_records, cp_file))\n",
    "    \n",
    "    # Sort by record count and get the latest\n",
    "    checkpoint_info.sort(reverse=True)\n",
    "    latest_count, latest_checkpoint = checkpoint_info[0]\n",
    "    \n",
    "    print(f\"âœ… Gefundene Checkpoints: {len(checkpoint_files)}\")\n",
    "    print(f\"   Neuester Checkpoint: {latest_checkpoint.name}\")\n",
    "    print(f\"   DatensÃ¤tze: {latest_count:,} / {len(df_merged):,} ({latest_count/len(df_merged)*100:.1f}%)\")\n",
    "    print(f\"   Verbleibend: {len(df_merged) - latest_count:,}\")\n",
    "    \n",
    "    # Ask user if they want to resume\n",
    "    resume_from_checkpoint = True  # Set to False to start fresh\n",
    "    \n",
    "    if resume_from_checkpoint:\n",
    "        print(f\"\\nðŸ”„ Lade Checkpoint: {latest_checkpoint.name}\")\n",
    "        df_checkpoint = pd.read_parquet(latest_checkpoint)\n",
    "        \n",
    "        # Convert checkpoint to results list\n",
    "        results = df_checkpoint.to_dict('records')\n",
    "        start_idx = latest_count\n",
    "        \n",
    "        # Calculate stats from checkpoint\n",
    "        stats_from_checkpoint = {\n",
    "            'neither': (df_checkpoint['title_source'] == 'vdeh').sum(),\n",
    "            'only_dnb': df_checkpoint['title_source'].str.contains('dnb', na=False).sum(),\n",
    "            'only_loc': df_checkpoint['title_source'].str.contains('loc', na=False).sum(),\n",
    "            'both': df_checkpoint['fusion_selected_variant'].notna().sum(),\n",
    "            'errors': df_checkpoint['rejection_reason'].notna().sum()\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nâœ… Checkpoint geladen!\")\n",
    "        print(f\"   Bereits verarbeitet: {len(results):,} DatensÃ¤tze\")\n",
    "        print(f\"   Statistik vom Checkpoint:\")\n",
    "        print(f\"      Fast (VDEH):  {stats_from_checkpoint['neither']:,}\")\n",
    "        print(f\"      Nur DNB:      {stats_from_checkpoint['only_dnb']:,}\")\n",
    "        print(f\"      Nur LoC:      {stats_from_checkpoint['only_loc']:,}\")\n",
    "        print(f\"      DNB + LoC:    {stats_from_checkpoint['both']:,}\")\n",
    "        print(f\"      Fehler:       {stats_from_checkpoint['errors']:,}\")\n",
    "        print(f\"\\nâ­ï¸  Starte ab Datensatz {start_idx + 1}\")\n",
    "    else:\n",
    "        print(f\"\\nðŸ†• Starte von vorne (Checkpoint wird ignoriert)\")\n",
    "        results = []\n",
    "        start_idx = 0\n",
    "        stats_from_checkpoint = {'neither': 0, 'only_dnb': 0, 'only_loc': 0, 'both': 0, 'errors': 0}\n",
    "else:\n",
    "    print(\"â„¹ï¸  Keine Checkpoints gefunden - starte von vorne\")\n",
    "    results = []\n",
    "    start_idx = 0\n",
    "    stats_from_checkpoint = {'neither': 0, 'only_dnb': 0, 'only_loc': 0, 'both': 0, 'errors': 0}\n",
    "\n",
    "print(f\"\\n{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ðŸ”¥ RUN FUSION (WITH CHECKPOINT RESUME)\nif 'start_idx' not in locals():\n    # Fallback if cell 6 wasn't run\n    results = []\n    start_idx = 0\n    stats_from_checkpoint = {'neither': 0, 'only_dnb': 0, 'only_loc': 0, 'both': 0, 'errors': 0}\n\nif start_idx == 0:\n    print(\"ðŸ”¥ Starte Datenfusion (Optimiert)...\\n\")\nelse:\n    print(f\"ðŸ”¥ Setze Datenfusion fort ab Datensatz {start_idx + 1}...\\n\")\n\nprint(f\"Verarbeite {len(df_merged):,} DatensÃ¤tze (noch {len(df_merged) - start_idx:,} zu verarbeiten)\")\nprint(f\"Startzeit: {datetime.now().strftime('%H:%M:%S')}\")\n\n# Pre-calculate which records need processing\ndnb_cols = ['dnb_title', 'dnb_title_ta', 'dnb_title_ty']\nhas_dnb = df_merged[dnb_cols].notna().any(axis=1)\n\nloc_cols = ['loc_title', 'loc_title_ta', 'loc_title_ty']\nhas_loc = df_merged[loc_cols].notna().any(axis=1)\n\nhas_both = has_dnb & has_loc\nhas_only_dnb = has_dnb & ~has_loc\nhas_only_loc = has_loc & ~has_dnb\nhas_neither = ~has_dnb & ~has_loc\n\nif start_idx == 0:\n    print(f\"\\nðŸ“Š Verarbeitungsstrategie:\")\n    print(f\"   Keine Anreicherung (nur VDEH):  {has_neither.sum():,} ({has_neither.sum()/len(df_merged)*100:.1f}%) - Fast Copy\")\n    print(f\"   Nur DNB (auto select):          {has_only_dnb.sum():,} ({has_only_dnb.sum()/len(df_merged)*100:.1f}%) - Simple Merge\")\n    print(f\"   Nur LoC (auto select):          {has_only_loc.sum():,} ({has_only_loc.sum()/len(df_merged)*100:.1f}%) - Simple Merge\")\n    print(f\"   DNB + LoC (KI entscheidet):     {has_both.sum():,} ({has_both.sum()/len(df_merged)*100:.1f}%) - Full Engine\")\n\nprint(f\"\\n{'='*80}\\n\")\n\n# Helper function for simple merge (no AI needed)\ndef simple_merge_dnb(row, vdeh_data):\n    \"\"\"Fast merge when only DNB data available.\"\"\"\n    # Prefer ID-based, then TA, then TY\n    if pd.notna(row.get('dnb_title')):\n        return {\n            **vdeh_data,\n            'title': row.get('dnb_title', vdeh_data['title']),\n            'authors': row.get('dnb_authors', vdeh_data['authors']),\n            'year': row.get('dnb_year', vdeh_data['year']),\n            'publisher': row.get('dnb_publisher', vdeh_data['publisher']),\n            'isbn': row.get('dnb_isbn', vdeh_data['isbn']),\n            'issn': row.get('dnb_issn', vdeh_data['issn']),\n            'title_source': 'dnb_id',\n            'authors_source': 'dnb_id',\n            'year_source': 'dnb_id',\n            'publisher_source': 'dnb_id',\n            'pages_source': 'vdeh',\n            'isbn_source': 'dnb_id' if pd.notna(row.get('dnb_isbn')) else 'vdeh',\n            'issn_source': 'dnb_id' if pd.notna(row.get('dnb_issn')) else 'vdeh',\n            'dnb_variant_selected': 'dnb_id'\n        }\n    elif pd.notna(row.get('dnb_title_ta')):\n        return {\n            **vdeh_data,\n            'title': row.get('dnb_title_ta', vdeh_data['title']),\n            'authors': row.get('dnb_authors_ta', vdeh_data['authors']),\n            'year': row.get('dnb_year_ta', vdeh_data['year']),\n            'publisher': row.get('dnb_publisher_ta', vdeh_data['publisher']),\n            'isbn': row.get('dnb_isbn_ta', vdeh_data['isbn']),\n            'issn': row.get('dnb_issn_ta', vdeh_data['issn']),\n            'title_source': 'dnb_title_author',\n            'authors_source': 'dnb_title_author',\n            'year_source': 'dnb_title_author',\n            'publisher_source': 'dnb_title_author',\n            'pages_source': 'vdeh',\n            'isbn_source': 'dnb_title_author' if pd.notna(row.get('dnb_isbn_ta')) else 'vdeh',\n            'issn_source': 'dnb_title_author' if pd.notna(row.get('dnb_issn_ta')) else 'vdeh',\n            'dnb_variant_selected': 'dnb_title_author'\n        }\n    else:\n        return {\n            **vdeh_data,\n            'title': row.get('dnb_title_ty', vdeh_data['title']),\n            'authors': row.get('dnb_authors_ty', vdeh_data['authors']),\n            'year': row.get('dnb_year_ty', vdeh_data['year']),\n            'publisher': row.get('dnb_publisher_ty', vdeh_data['publisher']),\n            'isbn': row.get('dnb_isbn_ty', vdeh_data['isbn']),\n            'issn': row.get('dnb_issn_ty', vdeh_data['issn']),\n            'title_source': 'dnb_title_year',\n            'authors_source': 'dnb_title_year',\n            'year_source': 'dnb_title_year',\n            'publisher_source': 'dnb_title_year',\n            'pages_source': 'vdeh',\n            'isbn_source': 'dnb_title_year' if pd.notna(row.get('dnb_isbn_ty')) else 'vdeh',\n            'issn_source': 'dnb_title_year' if pd.notna(row.get('dnb_issn_ty')) else 'vdeh',\n            'dnb_variant_selected': 'dnb_title_year'\n        }\n\ndef simple_merge_loc(row, vdeh_data):\n    \"\"\"Fast merge when only LoC data available.\"\"\"\n    # Prefer ID-based, then TA, then TY\n    if pd.notna(row.get('loc_title')):\n        return {\n            **vdeh_data,\n            'title': row.get('loc_title', vdeh_data['title']),\n            'authors': row.get('loc_authors', vdeh_data['authors']),\n            'year': row.get('loc_year', vdeh_data['year']),\n            'publisher': row.get('loc_publisher', vdeh_data['publisher']),\n            'pages': row.get('loc_pages', vdeh_data['pages']),\n            'isbn': row.get('loc_isbn', vdeh_data['isbn']),\n            'issn': row.get('loc_issn', vdeh_data['issn']),\n            'title_source': 'loc_id',\n            'authors_source': 'loc_id',\n            'year_source': 'loc_id',\n            'publisher_source': 'loc_id',\n            'pages_source': 'loc_id' if pd.notna(row.get('loc_pages')) else 'vdeh',\n            'isbn_source': 'loc_id' if pd.notna(row.get('loc_isbn')) else 'vdeh',\n            'issn_source': 'loc_id' if pd.notna(row.get('loc_issn')) else 'vdeh'\n        }\n    elif pd.notna(row.get('loc_title_ta')):\n        return {\n            **vdeh_data,\n            'title': row.get('loc_title_ta', vdeh_data['title']),\n            'authors': row.get('loc_authors_ta', vdeh_data['authors']),\n            'year': row.get('loc_year_ta', vdeh_data['year']),\n            'publisher': row.get('loc_publisher_ta', vdeh_data['publisher']),\n            'pages': row.get('loc_pages_ta', vdeh_data['pages']),\n            'isbn': row.get('loc_isbn_ta', vdeh_data['isbn']),\n            'issn': row.get('loc_issn_ta', vdeh_data['issn']),\n            'title_source': 'loc_title_author',\n            'authors_source': 'loc_title_author',\n            'year_source': 'loc_title_author',\n            'publisher_source': 'loc_title_author',\n            'pages_source': 'loc_title_author' if pd.notna(row.get('loc_pages_ta')) else 'vdeh',\n            'isbn_source': 'loc_title_author' if pd.notna(row.get('loc_isbn_ta')) else 'vdeh',\n            'issn_source': 'loc_title_author' if pd.notna(row.get('loc_issn_ta')) else 'vdeh'\n        }\n    else:\n        return {\n            **vdeh_data,\n            'title': row.get('loc_title_ty', vdeh_data['title']),\n            'authors': row.get('loc_authors_ty', vdeh_data['authors']),\n            'year': row.get('loc_year_ty', vdeh_data['year']),\n            'publisher': row.get('loc_publisher_ty', vdeh_data['publisher']),\n            'pages': row.get('loc_pages_ty', vdeh_data['pages']),\n            'isbn': row.get('loc_isbn_ty', vdeh_data['isbn']),\n            'issn': row.get('loc_issn', vdeh_data['issn']),\n            'title_source': 'loc_title_year',\n            'authors_source': 'loc_title_year',\n            'year_source': 'loc_title_year',\n            'publisher_source': 'loc_title_year',\n            'pages_source': 'loc_title_year' if pd.notna(row.get('loc_pages_ty')) else 'vdeh',\n            'isbn_source': 'loc_title_year' if pd.notna(row.get('loc_isbn_ty')) else 'vdeh',\n            'issn_source': 'loc_title_year' if pd.notna(row.get('loc_issn_ty')) else 'vdeh'\n        }\n\n# Initialize tracking\nstart_time = datetime.now()\ncheckpoint_interval = 5000\nstats = stats_from_checkpoint.copy()\nlast_progress_time = start_time\n\n# Process records starting from start_idx\nfor i, (idx, row) in enumerate(df_merged.iloc[start_idx:].iterrows()):\n    # VDEH base data - NOW INCLUDING ISBN/ISSN\n    vdeh_data = {\n        'title': row.get('title'),\n        'authors': row.get('authors_str'),\n        'year': row.get('year'),\n        'publisher': row.get('publisher'),\n        'pages': row.get('pages'),\n        'isbn': row.get('isbn'),\n        'issn': row.get('issn'),\n        'title_source': 'vdeh',\n        'authors_source': 'vdeh',\n        'year_source': 'vdeh',\n        'publisher_source': 'vdeh',\n        'pages_source': 'vdeh',\n        'isbn_source': 'vdeh' if pd.notna(row.get('isbn')) else None,\n        'issn_source': 'vdeh' if pd.notna(row.get('issn')) else None\n    }\n    \n    # Route based on data availability\n    if has_neither.loc[idx]:\n        # Fast path: No enrichment\n        results.append(vdeh_data)\n        stats['neither'] += 1\n        \n    elif has_only_dnb.loc[idx]:\n        # Simple merge: Only DNB\n        results.append(simple_merge_dnb(row, vdeh_data))\n        stats['only_dnb'] += 1\n        \n    elif has_only_loc.loc[idx]:\n        # Simple merge: Only LoC\n        results.append(simple_merge_loc(row, vdeh_data))\n        stats['only_loc'] += 1\n        \n    else:\n        # Complex case: Both DNB and LoC - use full engine\n        try:\n            result = engine.merge_record(row)\n            results.append(result.to_dict())\n            stats['both'] += 1\n            \n            # Detailed progress for AI decisions every 10 records\n            if stats['both'] % 10 == 0:\n                elapsed = (datetime.now() - start_time).total_seconds()\n                ai_since_checkpoint = stats['both'] - stats_from_checkpoint['both']\n                rate = ai_since_checkpoint / elapsed if elapsed > 0 else 0\n                remaining_ai = has_both.sum() - stats['both']\n                eta = remaining_ai / rate if rate > 0 else 0\n                print(f\"\\rðŸ¤– AI: [{stats['both']}/{has_both.sum()}] {stats['both']/has_both.sum()*100:.1f}% | \"\n                      f\"{rate:.2f} rec/s | ETA: {eta/60:.0f}min | \"\n                      f\"Total: {len(results):,}/{len(df_merged):,}\", end='', flush=True)\n                \n        except Exception as e:\n            print(f\"\\nâŒ ERROR at {idx}: {e}\")\n            results.append({**vdeh_data, 'title_source': 'error', 'rejection_reason': str(e)})\n            stats['errors'] += 1\n    \n    # Progress update every 1000 records (more frequent!)\n    if len(results) % 1000 == 0 and len(results) > start_idx:\n        now = datetime.now()\n        total_elapsed = (now - start_time).total_seconds()\n        records_since_checkpoint = len(results) - start_idx\n        overall_rate = records_since_checkpoint / total_elapsed if total_elapsed > 0 else 0\n        overall_eta = (len(df_merged) - len(results)) / overall_rate if overall_rate > 0 else 0\n        \n        print(f\"\\nâ±ï¸  [{now.strftime('%H:%M:%S')}] Progress: {len(results):,}/{len(df_merged):,} ({len(results)/len(df_merged)*100:.1f}%)\")\n        print(f\"   Rate: {overall_rate:.0f} rec/s | ETA: {overall_eta/60:.0f}min | Elapsed: {total_elapsed/60:.1f}min\")\n        print(f\"   âœ“ VDEH: {stats['neither']:,} | DNB: {stats['only_dnb']:,} | LoC: {stats['only_loc']:,} | AI: {stats['both']:,}\")\n    \n    # Detailed summary every 5000 records\n    if len(results) % 5000 == 0 and len(results) > start_idx:\n        total_elapsed = (datetime.now() - start_time).total_seconds()\n        records_since_checkpoint = len(results) - start_idx\n        overall_rate = records_since_checkpoint / total_elapsed if total_elapsed > 0 else 0\n        overall_eta = (len(df_merged) - len(results)) / overall_rate if overall_rate > 0 else 0\n        print(f\"\\n{'='*80}\")\n        print(f\"ðŸ“Š Checkpoint-Fortschritt: [{len(results):,}/{len(df_merged):,}] {len(results)/len(df_merged)*100:.1f}%\")\n        print(f\"   Gesamt-Rate: {overall_rate:.0f} rec/s | ETA: {overall_eta/60:.0f}min\")\n        print(f\"   Fast (VDEH):  {stats['neither']:,}\")\n        print(f\"   Nur DNB:      {stats['only_dnb']:,}\")\n        print(f\"   Nur LoC:      {stats['only_loc']:,}\")\n        print(f\"   DNB + LoC:    {stats['both']:,}\")\n        print(f\"   Fehler:       {stats['errors']:,}\")\n        print(f\"{'='*80}\")\n    \n    # Checkpoint save\n    if len(results) % checkpoint_interval == 0 and len(results) > start_idx:\n        checkpoint_path = processed_dir / f'06_fusion_checkpoint_{len(results)}.parquet'\n        df_checkpoint = pd.DataFrame(results, index=df_merged.index[:len(results)])\n        df_checkpoint.to_parquet(checkpoint_path)\n        print(f\"\\nðŸ’¾ Checkpoint gespeichert: {checkpoint_path.name}\")\n\nprint(f\"\\n\\n{'='*80}\")\nprint(f\"âœ… Fusion abgeschlossen!\")\nprint(f\"   Keine Anreicherung: {stats['neither']:,}\")\nprint(f\"   Nur DNB:            {stats['only_dnb']:,}\")\nprint(f\"   Nur LoC:            {stats['only_loc']:,}\")\nprint(f\"   DNB + LoC (KI):     {stats['both']:,}\")\nprint(f\"   Fehler:             {stats['errors']:,}\")\nprint(f\"Endzeit: {datetime.now().strftime('%H:%M:%S')}\")\nelapsed_total = (datetime.now() - start_time).total_seconds()\nprint(f\"Dauer: {elapsed_total/60:.1f} Minuten\")\nif start_idx > 0:\n    print(f\"Verarbeitet: {len(results) - start_idx:,} neue DatensÃ¤tze\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š CREATE RESULT DATAFRAME\n",
    "print(\"ðŸ“Š Erstelle Ergebnis-DataFrame...\\n\")\n",
    "\n",
    "df_fused = pd.DataFrame(results, index=df_merged.index)\n",
    "\n",
    "print(f\"âœ… Fused dataset: {len(df_fused):,} records, {len(df_fused.columns)} columns\")\n",
    "print(f\"\\nðŸ“‹ Result columns:\")\n",
    "for col in df_fused.columns:\n",
    "    non_null = df_fused[col].notna().sum()\n",
    "    print(f\"   {col:35s}: {non_null:,} ({non_null/len(df_fused)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“ˆ FUSION STATISTICS\n",
    "print(\"ðŸ“ˆ Fusion Statistiken:\\n\")\n",
    "\n",
    "# Source distribution\n",
    "print(\"Datenquellen-Verteilung:\")\n",
    "source_dist = df_fused['title_source'].value_counts()\n",
    "for source, count in source_dist.items():\n",
    "    print(f\"   {source:20s}: {count:,} ({count/len(df_fused)*100:.1f}%)\")\n",
    "\n",
    "# AI usage\n",
    "print(f\"\\nKI-Nutzung:\")\n",
    "ai_decisions = df_fused['fusion_selected_variant'].notna().sum()\n",
    "print(f\"   KI-Entscheidungen: {ai_decisions:,} ({ai_decisions/len(df_fused)*100:.1f}%)\")\n",
    "\n",
    "if ai_decisions > 0:\n",
    "    variant_dist = df_fused['fusion_selected_variant'].value_counts()\n",
    "    print(f\"\\n   Varianten-Auswahl:\")\n",
    "    variant_names = {\n",
    "        'A': 'DNB-ID', 'B': 'DNB-TA', 'C': 'DNB-TY',\n",
    "        'D': 'LoC-ID', 'E': 'LoC-TA', 'F': 'LoC-TY',\n",
    "        'KEINE': 'Abgelehnt'\n",
    "    }\n",
    "    for variant, count in variant_dist.items():\n",
    "        variant_name = variant_names.get(variant, variant)\n",
    "        print(f\"      {variant} ({variant_name:12s}): {count:,} ({count/ai_decisions*100:.1f}%)\")\n",
    "\n",
    "# Conflicts detected\n",
    "conflicts_detected = df_fused['fusion_conflicts_detected'].notna().sum()\n",
    "print(f\"\\nKonflikte:\")\n",
    "print(f\"   DatensÃ¤tze mit Konflikten: {conflicts_detected:,} ({conflicts_detected/len(df_fused)*100:.1f}%)\")\n",
    "\n",
    "# Rejections\n",
    "rejections = df_fused['rejection_reason'].notna().sum()\n",
    "print(f\"\\nAblehnungen:\")\n",
    "print(f\"   Abgelehnte Matches: {rejections:,} ({rejections/len(df_fused)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Š QUALITY METRICS\n",
    "print(\"ðŸ“Š QualitÃ¤tsmetriken:\\n\")\n",
    "\n",
    "# Data completeness\n",
    "print(\"DatenvollstÃ¤ndigkeit:\")\n",
    "for field in ['title', 'authors', 'year', 'publisher', 'pages']:\n",
    "    complete = df_fused[field].notna().sum()\n",
    "    print(f\"   {field:15s}: {complete:,} ({complete/len(df_fused)*100:.1f}%)\")\n",
    "\n",
    "# Title similarity scores (where available)\n",
    "if 'title_similarity_score' in df_fused.columns:\n",
    "    scores = df_fused['title_similarity_score'].dropna()\n",
    "    if len(scores) > 0:\n",
    "        print(f\"\\nTitel-Ã„hnlichkeit (bei {len(scores):,} Matches):\")\n",
    "        print(f\"   Durchschnitt: {scores.mean():.1%}\")\n",
    "        print(f\"   Median:       {scores.median():.1%}\")\n",
    "        print(f\"   Min:          {scores.min():.1%}\")\n",
    "        print(f\"   Max:          {scores.max():.1%}\")\n",
    "\n",
    "# Language-based source selection\n",
    "print(f\"\\nSprachbasierte Quellenwahl:\")\n",
    "lang_source = df_merged[['detected_language']].join(df_fused[['title_source']])\n",
    "lang_source_cross = pd.crosstab(\n",
    "    lang_source['detected_language'], \n",
    "    lang_source['title_source'],\n",
    "    normalize='index'\n",
    ")\n",
    "print(lang_source_cross.head(10).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ’¾ SAVE RESULTS\n",
    "print(\"ðŸ’¾ Speichere Ergebnisse...\\n\")\n",
    "\n",
    "# Save fused data\n",
    "output_path = processed_dir / '06_vdeh_dnb_loc_fused_data.parquet'\n",
    "df_fused.to_parquet(output_path, index=True)\n",
    "print(f\"âœ… Fused data saved: {output_path}\")\n",
    "print(f\"   Size: {output_path.stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# Save fusion statistics\n",
    "stats = {\n",
    "    'total_records': len(df_fused),\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'sources': source_dist.to_dict(),\n",
    "    'ai_decisions': int(ai_decisions),\n",
    "    'conflicts_detected': int(conflicts_detected),\n",
    "    'rejections': int(rejections),\n",
    "    'completeness': {\n",
    "        field: int(df_fused[field].notna().sum())\n",
    "        for field in ['title', 'authors', 'year', 'publisher', 'pages']\n",
    "    }\n",
    "}\n",
    "\n",
    "stats_path = processed_dir / '06_fusion_statistics.json'\n",
    "with open(stats_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(stats, f, indent=2, ensure_ascii=False)\n",
    "print(f\"âœ… Statistics saved: {stats_path}\")\n",
    "\n",
    "# Clean up checkpoints\n",
    "checkpoint_files = list(processed_dir.glob('06_fusion_checkpoint_*.parquet'))\n",
    "if checkpoint_files:\n",
    "    print(f\"\\nðŸ—‘ï¸  Cleaning up {len(checkpoint_files)} checkpoint files...\")\n",
    "    for cp_file in checkpoint_files:\n",
    "        cp_file.unlink()\n",
    "    print(f\"âœ… Checkpoints removed\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"âœ… FUSION COMPLETE!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ” SAMPLE INSPECTION\n",
    "print(\"ðŸ” Stichproben der fusionierten Daten:\\n\")\n",
    "\n",
    "# Show a few examples of different source types\n",
    "print(\"=\"*80)\n",
    "print(\"Beispiel 1: VDEH Original (kein Match)\")\n",
    "print(\"=\"*80)\n",
    "vdeh_only = df_fused[df_fused['title_source'] == 'vdeh'].head(1)\n",
    "if len(vdeh_only) > 0:\n",
    "    idx = vdeh_only.index[0]\n",
    "    print(f\"Index: {idx}\")\n",
    "    print(f\"Title:     {vdeh_only.loc[idx, 'title']}\")\n",
    "    print(f\"Authors:   {vdeh_only.loc[idx, 'authors']}\")\n",
    "    print(f\"Year:      {vdeh_only.loc[idx, 'year']}\")\n",
    "    print(f\"Source:    {vdeh_only.loc[idx, 'title_source']}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Beispiel 2: DNB Match\")\n",
    "print(\"=\"*80)\n",
    "dnb_match = df_fused[df_fused['title_source'].str.contains('dnb', na=False)].head(1)\n",
    "if len(dnb_match) > 0:\n",
    "    idx = dnb_match.index[0]\n",
    "    print(f\"Index: {idx}\")\n",
    "    print(f\"Title:     {dnb_match.loc[idx, 'title']}\")\n",
    "    print(f\"Authors:   {dnb_match.loc[idx, 'authors']}\")\n",
    "    print(f\"Year:      {dnb_match.loc[idx, 'year']}\")\n",
    "    print(f\"Source:    {dnb_match.loc[idx, 'title_source']}\")\n",
    "    if pd.notna(dnb_match.loc[idx, 'title_similarity_score']):\n",
    "        print(f\"Similarity: {dnb_match.loc[idx, 'title_similarity_score']:.1%}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Beispiel 3: LoC Match\")\n",
    "print(\"=\"*80)\n",
    "loc_match = df_fused[df_fused['title_source'].str.contains('loc', na=False)].head(1)\n",
    "if len(loc_match) > 0:\n",
    "    idx = loc_match.index[0]\n",
    "    print(f\"Index: {idx}\")\n",
    "    print(f\"Title:     {loc_match.loc[idx, 'title']}\")\n",
    "    print(f\"Authors:   {loc_match.loc[idx, 'authors']}\")\n",
    "    print(f\"Year:      {loc_match.loc[idx, 'year']}\")\n",
    "    print(f\"Source:    {loc_match.loc[idx, 'title_source']}\")\n",
    "    if pd.notna(loc_match.loc[idx, 'title_similarity_score']):\n",
    "        print(f\"Similarity: {loc_match.loc[idx, 'title_similarity_score']:.1%}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Beispiel 4: KI-Entscheidung\")\n",
    "print(\"=\"*80)\n",
    "ai_decision = df_fused[df_fused['fusion_selected_variant'].notna()].head(1)\n",
    "if len(ai_decision) > 0:\n",
    "    idx = ai_decision.index[0]\n",
    "    print(f\"Index: {idx}\")\n",
    "    print(f\"Title:     {ai_decision.loc[idx, 'title']}\")\n",
    "    print(f\"Authors:   {ai_decision.loc[idx, 'authors']}\")\n",
    "    print(f\"Year:      {ai_decision.loc[idx, 'year']}\")\n",
    "    print(f\"Source:    {ai_decision.loc[idx, 'title_source']}\")\n",
    "    print(f\"Variant:   {ai_decision.loc[idx, 'fusion_selected_variant']}\")\n",
    "    if pd.notna(ai_decision.loc[idx, 'ai_reasoning']):\n",
    "        print(f\"Reasoning: {ai_decision.loc[idx, 'ai_reasoning'][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bibo-analysis-DoEGeq_l-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}