{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {
    "papermill": {
     "duration": 0.005111,
     "end_time": "2025-12-25T20:57:57.999047",
     "exception": false,
     "start_time": "2025-12-25T20:57:57.993936",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# VDEH Data Enrichment - Library of Congress (LoC)\n",
    "\n",
    "**Fokus:** Datenanreicherung englischsprachiger Literatur √ºber Library of Congress (LoC) API\n",
    "\n",
    "## üéØ Ziel\n",
    "- Anreicherung englischsprachiger Datens√§tze via LoC API (ISBN/ISSN)\n",
    "- Erg√§nzung zu DNB-Daten f√ºr internationale Literatur\n",
    "- Fokus auf Records mit `detected_language = 'en'`\n",
    "- Validierung und Qualit√§tsverbesserung\n",
    "\n",
    "## üìö Input/Output\n",
    "- **Input**: `data/vdeh/processed/03_language_detected_data.parquet`\n",
    "- **Output**: `data/vdeh/processed/04b_loc_enriched_data.parquet`\n",
    "\n",
    "## üîó API\n",
    "- **LoC SRU API**: https://www.loc.gov/z3950/\n",
    "- **Endpoint**: https://lx2.loc.gov:210/lcdb\n",
    "- **Abfrage**: ISBN/ISSN basierte Suche, Titel/Autor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T20:57:58.007935Z",
     "iopub.status.busy": "2025-12-25T20:57:58.007628Z",
     "iopub.status.idle": "2025-12-25T20:57:58.382138Z",
     "shell.execute_reply": "2025-12-25T20:57:58.381782Z"
    },
    "papermill": {
     "duration": 0.37927,
     "end_time": "2025-12-25T20:57:58.382834",
     "exception": false,
     "start_time": "2025-12-25T20:57:58.003564",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-25 21:57:58 - utils.notebook_utils - INFO - Searching for project root...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-25 21:57:58 - utils.notebook_utils - INFO - Project root found: /media/sz/Data/Bibo/analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-25 21:57:58 - utils.notebook_utils - INFO - Loading configuration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-25 21:57:58 - config_loader - INFO - Configuration loaded from /media/sz/Data/Bibo/analysis/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-25 21:57:58 - utils.notebook_utils - INFO - Configuration loaded successfully: Dual-Source Bibliothek Bestandsvergleich\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Project root: /media/sz/Data/Bibo/analysis\n",
      "‚úÖ Project: Dual-Source Bibliothek Bestandsvergleich v2.2.0\n",
      "‚úÖ LoC API Funktionen geladen\n"
     ]
    }
   ],
   "source": [
    "# üõ†Ô∏è SETUP UND DATEN LADEN\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd()\n",
    "while not (project_root / 'config.yaml').exists() and project_root.parent != project_root:\n",
    "    project_root = project_root.parent\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "from utils.notebook_utils import setup_notebook\n",
    "\n",
    "project_root, config = setup_notebook()\n",
    "print(f\"‚úÖ Project root: {project_root}\")\n",
    "print(f\"‚úÖ Project: {config.get('project.name')} v{config.get('project.version')}\")\n",
    "\n",
    "# LoC API laden\n",
    "from loc_api import LOC_SRU_BASE, query_loc_by_isbn, query_loc_by_issn, query_loc_by_title_author\n",
    "print(f\"‚úÖ LoC API Funktionen geladen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load_data",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T20:57:58.390197Z",
     "iopub.status.busy": "2025-12-25T20:57:58.389832Z",
     "iopub.status.idle": "2025-12-25T20:57:58.644699Z",
     "shell.execute_reply": "2025-12-25T20:57:58.644345Z"
    },
    "papermill": {
     "duration": 0.260912,
     "end_time": "2025-12-25T20:57:58.645401",
     "exception": false,
     "start_time": "2025-12-25T20:57:58.384489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Daten geladen aus: /media/sz/Data/Bibo/analysis/data/vdeh/processed/03_language_detected_data.parquet\n",
      "üìä Records: 58,305\n",
      "üìã Spalten: ['id', 'title', 'authors', 'authors_affiliation', 'year', 'publisher', 'isbn', 'issn', 'pages', 'language', 'authors_str', 'num_authors', 'authors_affiliation_str', 'num_authors_affiliation', 'isbn_valid', 'isbn_status', 'issn_valid', 'issn_status', 'detected_language', 'detected_language_confidence', 'detected_language_name']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Memory: 54.5 MB\n",
      "\n",
      "üìä Sprachverteilung:\n",
      "   de: 32,543 (55.8%)\n",
      "   en: 16,516 (28.3%)\n",
      "   unknown: 1,446 (2.5%)\n",
      "   fr: 929 (1.6%)\n",
      "   it: 773 (1.3%)\n",
      "   sl: 654 (1.1%)\n",
      "   ro: 625 (1.1%)\n",
      "   af: 520 (0.9%)\n",
      "   nl: 497 (0.9%)\n",
      "   ca: 493 (0.8%)\n"
     ]
    }
   ],
   "source": [
    "# üìÇ DATEN AUS VORHERIGER STUFE LADEN\n",
    "processed_dir = config.project_root / config.get('paths.data.vdeh.processed')\n",
    "input_path = processed_dir / '03_language_detected_data.parquet'\n",
    "metadata_path = processed_dir / '03_metadata.json'\n",
    "\n",
    "if not input_path.exists():\n",
    "    raise FileNotFoundError(f\"Input-Datei nicht gefunden: {input_path}\\n\"\n",
    "                          \"Bitte f√ºhren Sie zuerst 03_vdeh_language_detection.ipynb aus.\")\n",
    "\n",
    "# Daten laden\n",
    "df_vdeh = pd.read_parquet(input_path)\n",
    "\n",
    "# Vorherige Metadaten laden\n",
    "with open(metadata_path, 'r') as f:\n",
    "    prev_metadata = json.load(f)\n",
    "\n",
    "print(f\"üìÇ Daten geladen aus: {input_path}\")\n",
    "print(f\"üìä Records: {len(df_vdeh):,}\")\n",
    "print(f\"üìã Spalten: {list(df_vdeh.columns)}\")\n",
    "print(f\"üíæ Memory: {df_vdeh.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Sprachverteilung\n",
    "if 'detected_language' in df_vdeh.columns:\n",
    "    lang_dist = df_vdeh['detected_language'].value_counts()\n",
    "    print(f\"\\nüìä Sprachverteilung:\")\n",
    "    for lang, count in lang_dist.head(10).items():\n",
    "        print(f\"   {lang}: {count:,} ({count/len(df_vdeh)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "identify_candidates",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T20:57:58.651975Z",
     "iopub.status.busy": "2025-12-25T20:57:58.651815Z",
     "iopub.status.idle": "2025-12-25T20:57:58.691688Z",
     "shell.execute_reply": "2025-12-25T20:57:58.691045Z"
    },
    "papermill": {
     "duration": 0.045301,
     "end_time": "2025-12-25T20:57:58.692506",
     "exception": false,
     "start_time": "2025-12-25T20:57:58.647205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç === KANDIDATEN-IDENTIFIKATION (ENGLISCHSPRACHIGE LITERATUR) ===\n",
      "\n",
      "üìö Englischsprachige Records: 16,516 (28.3%)\n",
      "\n",
      "üìã Kriterium 1: ISBN vorhanden\n",
      "   ISBN-Kandidaten: 3,695\n",
      "\n",
      "üìã Kriterium 2: ISSN vorhanden (nur unvollst√§ndig)\n",
      "   ISSN-Kandidaten: 224\n",
      "\n",
      "üìã Kriterium 3: Ohne ISBN aber mit Titel + Autoren\n",
      "   Kandidaten: 2,112\n",
      "\n",
      "üéØ Finale LoC-Anreicherungs-Kandidaten: 9,900\n",
      "   Mit ISBN: 3,695\n",
      "   Mit ISSN: 103\n",
      "   Mit Titel+Autoren: 4,770\n"
     ]
    }
   ],
   "source": [
    "# üîç KANDIDATEN F√úR LOC-ANREICHERUNG IDENTIFIZIEREN\n",
    "print(\"üîç === KANDIDATEN-IDENTIFIKATION (ENGLISCHSPRACHIGE LITERATUR) ===\\n\")\n",
    "\n",
    "# Filter: Nur englischsprachige Records\n",
    "df_english = df_vdeh[df_vdeh['detected_language'] == 'en'].copy()\n",
    "\n",
    "print(f\"üìö Englischsprachige Records: {len(df_english):,} ({len(df_english)/len(df_vdeh)*100:.1f}%)\")\n",
    "\n",
    "# 1. ISBN-basierte Kandidaten (ALLE mit ISBN)\n",
    "if 'isbn' in df_english.columns:\n",
    "    has_isbn = df_english['isbn'].notna()\n",
    "    isbn_candidates = df_english[has_isbn].copy()\n",
    "    \n",
    "    print(f\"\\nüìã Kriterium 1: ISBN vorhanden\")\n",
    "    print(f\"   ISBN-Kandidaten: {len(isbn_candidates):,}\")\n",
    "\n",
    "# 2. ISSN-basierte Kandidaten (nur unvollst√§ndig)\n",
    "if 'issn' in df_english.columns:\n",
    "    has_issn = df_english['issn'].notna()\n",
    "    missing_title = df_english['title'].isna()\n",
    "    missing_authors = (df_english['authors_str'].isna()) | (df_english['authors_str'] == '')\n",
    "    missing_year = df_english['year'].isna()\n",
    "    \n",
    "    issn_candidates = df_english[\n",
    "        has_issn & (missing_title | missing_authors | missing_year)\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"\\nüìã Kriterium 2: ISSN vorhanden (nur unvollst√§ndig)\")\n",
    "    print(f\"   ISSN-Kandidaten: {len(issn_candidates):,}\")\n",
    "\n",
    "# 3. Titel/Autor-Kandidaten (ohne ISBN aber mit Titel + Autoren)\n",
    "no_isbn_but_searchable = df_english[\n",
    "    (df_english['isbn'].isna()) &\n",
    "    (df_english['title'].notna()) &\n",
    "    (df_english['authors_str'].notna()) &\n",
    "    (df_english['authors_str'] != '')\n",
    "].copy()\n",
    "\n",
    "print(f\"\\nüìã Kriterium 3: Ohne ISBN aber mit Titel + Autoren\")\n",
    "print(f\"   Kandidaten: {len(no_isbn_but_searchable):,}\")\n",
    "\n",
    "# Finale Kandidatenliste\n",
    "final_candidates = df_english[\n",
    "    (df_english['isbn'].notna()) |\n",
    "    (\n",
    "        (df_english['isbn'].isna()) &\n",
    "        (df_english['title'].notna()) &\n",
    "        (\n",
    "            ((df_english['authors_str'].notna()) & (df_english['authors_str'] != '')) |\n",
    "            (df_english['year'].notna())\n",
    "        )\n",
    "    )\n",
    "].copy()\n",
    "\n",
    "print(f\"\\nüéØ Finale LoC-Anreicherungs-Kandidaten: {len(final_candidates):,}\")\n",
    "print(f\"   Mit ISBN: {final_candidates['isbn'].notna().sum():,}\")\n",
    "print(f\"   Mit ISSN: {final_candidates['issn'].notna().sum():,}\")\n",
    "print(f\"   Mit Titel+Autoren: {((final_candidates['title'].notna()) & (final_candidates['authors_str'].notna()) & (final_candidates['authors_str'] != '')).sum():,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "api_status",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T20:57:58.698438Z",
     "iopub.status.busy": "2025-12-25T20:57:58.698248Z",
     "iopub.status.idle": "2025-12-25T20:57:58.701734Z",
     "shell.execute_reply": "2025-12-25T20:57:58.701358Z"
    },
    "papermill": {
     "duration": 0.00797,
     "end_time": "2025-12-25T20:57:58.702421",
     "exception": false,
     "start_time": "2025-12-25T20:57:58.694451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê === LIBRARY OF CONGRESS API STATUS ===\n",
      "\n",
      "‚úÖ LoC API Funktionen aus src/loc_api.py geladen\n",
      "   Base URL: http://lx2.loc.gov:210/lcdb\n",
      "   Schema: MARC21-xml\n",
      "   Verf√ºgbare Funktionen:\n",
      "     - query_loc_by_isbn(isbn, max_records=1)\n",
      "     - query_loc_by_issn(issn, max_records=1)\n",
      "     - query_loc_by_title_author(title, author=None, max_records=1)\n"
     ]
    }
   ],
   "source": [
    "# üåê LOC API STATUS\n",
    "print(\"üåê === LIBRARY OF CONGRESS API STATUS ===\\n\")\n",
    "\n",
    "print(\"‚úÖ LoC API Funktionen aus src/loc_api.py geladen\")\n",
    "print(f\"   Base URL: {LOC_SRU_BASE}\")\n",
    "print(f\"   Schema: MARC21-xml\")\n",
    "print(f\"   Verf√ºgbare Funktionen:\")\n",
    "print(f\"     - query_loc_by_isbn(isbn, max_records=1)\")\n",
    "print(f\"     - query_loc_by_issn(issn, max_records=1)\")\n",
    "print(f\"     - query_loc_by_title_author(title, author=None, max_records=1)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "query_isbn_issn",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T20:57:58.707414Z",
     "iopub.status.busy": "2025-12-25T20:57:58.707186Z",
     "iopub.status.idle": "2025-12-25T20:57:58.732180Z",
     "shell.execute_reply": "2025-12-25T20:57:58.731601Z"
    },
    "papermill": {
     "duration": 0.028703,
     "end_time": "2025-12-25T20:57:58.732911",
     "exception": false,
     "start_time": "2025-12-25T20:57:58.704208",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ === LOC DATENABFRAGE (ISBN/ISSN) ===\n",
      "\n",
      "‚öôÔ∏è  Konfiguration:\n",
      "   Rate Limit: 10.0s pro Anfrage\n",
      "   Save Interval: Alle 50 Queries\n",
      "   Output: loc_raw_data.parquet\n",
      "\n",
      "üìÇ Lade vorhandene LoC-Daten...\n",
      "   Bereits abgefragt: 3,785\n",
      "   Davon erfolgreich: 797\n",
      "\n",
      "üìã Extrahiere ISBN/ISSN aus 9,900 Kandidaten...\n",
      "   ISBN-Queries: 3,695\n",
      "   ISSN-Queries: 90\n",
      "   Gesamt: 3,785\n",
      "\n",
      "üîç Abgleich mit vorhandenen Daten:\n",
      "   Bereits vorhanden: 3,785\n",
      "   Neu abzufragen: 0\n",
      "\n",
      "‚úÖ Alle ISBN/ISSN bereits in LoC-Daten vorhanden - keine neuen Abfragen n√∂tig\n",
      "\n",
      "üìä === GESAMT LOC-DATEN ===\n",
      "   Total Records: 3,785\n",
      "   Erfolgreich: 797\n",
      "   Nicht gefunden: 2,988\n"
     ]
    }
   ],
   "source": [
    "# üöÄ LOC DATENABFRAGE - ISBN/ISSN\n",
    "print(\"üöÄ === LOC DATENABFRAGE (ISBN/ISSN) ===\\n\")\n",
    "\n",
    "# Konfiguration\n",
    "RATE_LIMIT_DELAY = 10.0  # Sekunden zwischen Anfragen (erh√∂ht wegen Verbindungsproblemen)\n",
    "SAVE_INTERVAL = 50  # Speichere alle N Abfragen\n",
    "LOC_DATA_FILE = processed_dir / 'loc_raw_data.parquet'\n",
    "\n",
    "print(f\"‚öôÔ∏è  Konfiguration:\")\n",
    "print(f\"   Rate Limit: {RATE_LIMIT_DELAY}s pro Anfrage\")\n",
    "print(f\"   Save Interval: Alle {SAVE_INTERVAL} Queries\")\n",
    "print(f\"   Output: {LOC_DATA_FILE.name}\")\n",
    "\n",
    "# Lade vorhandene LoC-Daten (falls vorhanden)\n",
    "if LOC_DATA_FILE.exists():\n",
    "    print(f\"\\nüìÇ Lade vorhandene LoC-Daten...\")\n",
    "    loc_data_df = pd.read_parquet(LOC_DATA_FILE)\n",
    "    print(f\"   Bereits abgefragt: {len(loc_data_df):,}\")\n",
    "    print(f\"   Davon erfolgreich: {(loc_data_df['loc_found'] == True).sum():,}\")\n",
    "else:\n",
    "    print(f\"\\nüìÇ Keine vorhandenen LoC-Daten gefunden - starte neue Abfrage\")\n",
    "    loc_data_df = pd.DataFrame(columns=[\n",
    "        'vdeh_id', 'query_type', 'query_value',\n",
    "        'loc_found', 'loc_title', 'loc_authors', 'loc_year', 'loc_publisher',\n",
    "        'loc_isbn', 'loc_issn', 'loc_pages'\n",
    "    ])\n",
    "\n",
    "# Sammle ISBN/ISSN aus englischsprachigen Kandidaten\n",
    "print(f\"\\nüìã Extrahiere ISBN/ISSN aus {len(final_candidates):,} Kandidaten...\")\n",
    "\n",
    "queries_isbn = final_candidates[final_candidates['isbn'].notna()][['id', 'isbn']].copy()\n",
    "queries_isbn.columns = ['vdeh_id', 'query_value']\n",
    "queries_isbn['query_type'] = 'ISBN'\n",
    "\n",
    "queries_issn = final_candidates[\n",
    "    final_candidates['isbn'].isna() & final_candidates['issn'].notna()\n",
    "][['id', 'issn']].copy()\n",
    "queries_issn.columns = ['vdeh_id', 'query_value']\n",
    "queries_issn['query_type'] = 'ISSN'\n",
    "\n",
    "all_queries = pd.concat([queries_isbn, queries_issn], ignore_index=True)\n",
    "\n",
    "print(f\"   ISBN-Queries: {len(queries_isbn):,}\")\n",
    "print(f\"   ISSN-Queries: {len(queries_issn):,}\")\n",
    "print(f\"   Gesamt: {len(all_queries):,}\")\n",
    "\n",
    "# Filtere bereits abgefragte\n",
    "if len(loc_data_df) > 0:\n",
    "    already_queried = set(loc_data_df['query_value'])\n",
    "    new_queries = all_queries[~all_queries['query_value'].isin(already_queried)].copy()\n",
    "    \n",
    "    print(f\"\\nüîç Abgleich mit vorhandenen Daten:\")\n",
    "    print(f\"   Bereits vorhanden: {len(all_queries) - len(new_queries):,}\")\n",
    "    print(f\"   Neu abzufragen: {len(new_queries):,}\")\n",
    "else:\n",
    "    new_queries = all_queries\n",
    "    print(f\"\\nüîç Alle {len(new_queries):,} Queries sind neu\")\n",
    "\n",
    "# Nur abfragen wenn neue Queries vorhanden\n",
    "if len(new_queries) > 0:\n",
    "    print(f\"\\nüîÑ Starte LoC-Abfrage f√ºr {len(new_queries):,} neue Queries...\\n\")\n",
    "    \n",
    "    from tqdm.auto import tqdm\n",
    "    \n",
    "    results = []\n",
    "    stats = {'found': 0, 'not_found': 0}\n",
    "    query_count = 0\n",
    "    \n",
    "    for _, row in tqdm(new_queries.iterrows(), total=len(new_queries), desc=\"üîç LoC API\", unit=\"queries\"):\n",
    "        # API-Abfrage\n",
    "        loc_result = None\n",
    "        if row['query_type'] == 'ISBN':\n",
    "            loc_result = query_loc_by_isbn(row['query_value'])\n",
    "        elif row['query_type'] == 'ISSN':\n",
    "            loc_result = query_loc_by_issn(row['query_value'])\n",
    "        \n",
    "        # Ergebnis speichern\n",
    "        result_row = {\n",
    "            'vdeh_id': row['vdeh_id'],\n",
    "            'query_type': row['query_type'],\n",
    "            'query_value': row['query_value'],\n",
    "            'loc_found': loc_result is not None,\n",
    "            'loc_title': loc_result.get('title') if loc_result else None,\n",
    "            'loc_authors': ', '.join(loc_result.get('authors', [])) if loc_result else None,\n",
    "            'loc_year': loc_result.get('year') if loc_result else None,\n",
    "            'loc_publisher': loc_result.get('publisher') if loc_result else None,\n",
    "            'loc_isbn': loc_result.get('isbn') if loc_result else None,\n",
    "            'loc_issn': loc_result.get('issn') if loc_result else None,\n",
    "            'loc_pages': loc_result.get('pages') if loc_result else None\n",
    "        }\n",
    "        \n",
    "        results.append(result_row)\n",
    "        \n",
    "        if loc_result:\n",
    "            stats['found'] += 1\n",
    "        else:\n",
    "            stats['not_found'] += 1\n",
    "        \n",
    "        query_count += 1\n",
    "        \n",
    "        # Regelm√§√üiges Speichern\n",
    "        if query_count % SAVE_INTERVAL == 0:\n",
    "            new_results_df = pd.DataFrame(results)\n",
    "            loc_data_df = pd.concat([loc_data_df, new_results_df], ignore_index=True)\n",
    "            loc_data_df.to_parquet(LOC_DATA_FILE, index=False)\n",
    "            results = []\n",
    "            print(f\"üíæ Zwischenspeicherung: {query_count}/{len(new_queries)} Queries abgefragt\")\n",
    "        \n",
    "        # Rate Limiting\n",
    "        time.sleep(RATE_LIMIT_DELAY)\n",
    "    \n",
    "    # Finale Speicherung\n",
    "    if len(results) > 0:\n",
    "        new_results_df = pd.DataFrame(results)\n",
    "        loc_data_df = pd.concat([loc_data_df, new_results_df], ignore_index=True)\n",
    "        loc_data_df.to_parquet(LOC_DATA_FILE, index=False)\n",
    "    \n",
    "    print(f\"\\nüíæ LoC-Daten gespeichert: {LOC_DATA_FILE.name}\")\n",
    "    \n",
    "    # Zusammenfassung\n",
    "    print(f\"\\nüìä === NEUE ABFRAGEN ===\")\n",
    "    print(f\"   Neue Queries: {len(new_queries):,}\")\n",
    "    print(f\"   ‚úÖ Gefunden: {stats['found']:,} ({stats['found']/len(new_queries)*100:.1f}%)\")\n",
    "    print(f\"   ‚ùå Nicht gefunden: {stats['not_found']:,} ({stats['not_found']/len(new_queries)*100:.1f}%)\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Alle ISBN/ISSN bereits in LoC-Daten vorhanden - keine neuen Abfragen n√∂tig\")\n",
    "\n",
    "# Gesamtstatistik\n",
    "print(f\"\\nüìä === GESAMT LOC-DATEN ===\")\n",
    "print(f\"   Total Records: {len(loc_data_df):,}\")\n",
    "print(f\"   Erfolgreich: {(loc_data_df['loc_found'] == True).sum():,}\")\n",
    "print(f\"   Nicht gefunden: {(loc_data_df['loc_found'] == False).sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "query_title_author",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T20:57:58.737055Z",
     "iopub.status.busy": "2025-12-25T20:57:58.736905Z",
     "iopub.status.idle": "2025-12-25T20:57:58.767163Z",
     "shell.execute_reply": "2025-12-25T20:57:58.766730Z"
    },
    "papermill": {
     "duration": 0.033247,
     "end_time": "2025-12-25T20:57:58.767936",
     "exception": false,
     "start_time": "2025-12-25T20:57:58.734689",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç === LOC TITEL/AUTOR-SUCHE ===\n",
      "\n",
      "‚öôÔ∏è  Konfiguration:\n",
      "   Rate Limit: 10.0s pro Anfrage\n",
      "   Save Interval: Alle 50 Queries\n",
      "   Output: loc_title_author_data.parquet\n",
      "   TA f√ºr alle Titel+Autoren: True\n",
      "\n",
      "üìÇ Lade vorhandene Titel/Autor-Suchdaten...\n",
      "   Bereits abgefragt: 4,770\n",
      "   Davon erfolgreich: 853\n",
      "\n",
      "üìã Titel/Autor-Kandidaten (englisch): 4,770\n",
      "\n",
      "üîç Abgleich mit vorhandenen Daten:\n",
      "   Bereits vorhanden: 4,770\n",
      "   Neu abzufragen: 0\n",
      "\n",
      "‚úÖ Alle Titel/Autor-Kombinationen bereits abgefragt\n",
      "\n",
      "üìä === GESAMT TITEL/AUTOR-DATEN ===\n",
      "   Total Records: 4,770\n",
      "   Erfolgreich: 853\n",
      "   Nicht gefunden: 3,917\n",
      "   üìà Erfolgsrate: 17.9%\n"
     ]
    }
   ],
   "source": [
    "# üîç LOC TITEL/AUTOR-SUCHE\n",
    "print(\"üîç === LOC TITEL/AUTOR-SUCHE ===\\n\")\n",
    "\n",
    "LOC_TITLE_DATA_FILE = processed_dir / 'loc_title_author_data.parquet'\n",
    "ALWAYS_TA_FOR_ALL_WITH_TITLE_AUTHORS = True\n",
    "\n",
    "print(f\"‚öôÔ∏è  Konfiguration:\")\n",
    "print(f\"   Rate Limit: {RATE_LIMIT_DELAY}s pro Anfrage\")\n",
    "print(f\"   Save Interval: Alle {SAVE_INTERVAL} Queries\")\n",
    "print(f\"   Output: {LOC_TITLE_DATA_FILE.name}\")\n",
    "print(f\"   TA f√ºr alle Titel+Autoren: {ALWAYS_TA_FOR_ALL_WITH_TITLE_AUTHORS}\")\n",
    "\n",
    "# Lade vorhandene Titel/Autor-Suchdaten\n",
    "if LOC_TITLE_DATA_FILE.exists():\n",
    "    print(f\"\\nüìÇ Lade vorhandene Titel/Autor-Suchdaten...\")\n",
    "    loc_title_df = pd.read_parquet(LOC_TITLE_DATA_FILE)\n",
    "    print(f\"   Bereits abgefragt: {len(loc_title_df):,}\")\n",
    "    print(f\"   Davon erfolgreich: {(loc_title_df['loc_found'] == True).sum():,}\")\n",
    "else:\n",
    "    print(f\"\\nüìÇ Keine vorhandenen Titel/Autor-Suchdaten gefunden\")\n",
    "    loc_title_df = pd.DataFrame(columns=[\n",
    "        'vdeh_id', 'query_type', 'title', 'author',\n",
    "        'loc_found', 'loc_title', 'loc_authors', 'loc_year', 'loc_publisher',\n",
    "        'loc_isbn', 'loc_issn', 'loc_pages'\n",
    "    ])\n",
    "\n",
    "# Identifiziere Kandidaten (nur englischsprachig!)\n",
    "if ALWAYS_TA_FOR_ALL_WITH_TITLE_AUTHORS:\n",
    "    title_author_candidates = df_english[\n",
    "        (df_english['title'].notna()) &\n",
    "        (df_english['authors_str'].notna()) &\n",
    "        (df_english['authors_str'] != '')\n",
    "    ].copy()\n",
    "else:\n",
    "    title_author_candidates = df_english[\n",
    "        (df_english['isbn'].isna()) &\n",
    "        (df_english['issn'].isna()) &\n",
    "        (df_english['title'].notna()) &\n",
    "        (df_english['authors_str'].notna()) &\n",
    "        (df_english['authors_str'] != '')\n",
    "    ].copy()\n",
    "\n",
    "print(f\"\\nüìã Titel/Autor-Kandidaten (englisch): {len(title_author_candidates):,}\")\n",
    "\n",
    "# Erstelle Query-Liste\n",
    "title_queries = title_author_candidates[['id', 'title', 'authors_str']].copy()\n",
    "title_queries.columns = ['vdeh_id', 'title', 'author']\n",
    "title_queries['query_type'] = 'TITLE_AUTHOR'\n",
    "\n",
    "# Filtere bereits abgefragte\n",
    "if len(loc_title_df) > 0:\n",
    "    already_queried = set(loc_title_df['vdeh_id'])\n",
    "    new_title_queries = title_queries[~title_queries['vdeh_id'].isin(already_queried)].copy()\n",
    "    \n",
    "    print(f\"\\nüîç Abgleich mit vorhandenen Daten:\")\n",
    "    print(f\"   Bereits vorhanden: {len(title_queries) - len(new_title_queries):,}\")\n",
    "    print(f\"   Neu abzufragen: {len(new_title_queries):,}\")\n",
    "else:\n",
    "    new_title_queries = title_queries\n",
    "    print(f\"\\nüîç Alle {len(new_title_queries):,} Titel/Autor-Queries sind neu\")\n",
    "\n",
    "# Nur abfragen wenn neue Queries vorhanden\n",
    "if len(new_title_queries) > 0:\n",
    "    print(f\"\\nüîÑ Starte LoC Titel/Autor-Abfrage f√ºr {len(new_title_queries):,} neue Queries...\\n\")\n",
    "    \n",
    "    from tqdm.auto import tqdm\n",
    "    \n",
    "    results = []\n",
    "    stats = {'found': 0, 'not_found': 0}\n",
    "    query_count = 0\n",
    "    \n",
    "    for _, row in tqdm(new_title_queries.iterrows(), total=len(new_title_queries), desc=\"üîç LoC Titel/Autor\", unit=\"queries\"):\n",
    "        # API-Abfrage\n",
    "        loc_result = query_loc_by_title_author(row['title'], row['author'])\n",
    "        \n",
    "        # Ergebnis speichern\n",
    "        result_row = {\n",
    "            'vdeh_id': row['vdeh_id'],\n",
    "            'query_type': row['query_type'],\n",
    "            'title': row['title'],\n",
    "            'author': row['author'],\n",
    "            'loc_found': loc_result is not None,\n",
    "            'loc_title': loc_result.get('title') if loc_result else None,\n",
    "            'loc_authors': ', '.join(loc_result.get('authors', [])) if loc_result else None,\n",
    "            'loc_year': loc_result.get('year') if loc_result else None,\n",
    "            'loc_publisher': loc_result.get('publisher') if loc_result else None,\n",
    "            'loc_isbn': loc_result.get('isbn') if loc_result else None,\n",
    "            'loc_issn': loc_result.get('issn') if loc_result else None,\n",
    "            'loc_pages': loc_result.get('pages') if loc_result else None\n",
    "        }\n",
    "        \n",
    "        results.append(result_row)\n",
    "        \n",
    "        if loc_result:\n",
    "            stats['found'] += 1\n",
    "        else:\n",
    "            stats['not_found'] += 1\n",
    "        \n",
    "        query_count += 1\n",
    "        \n",
    "        # Regelm√§√üiges Speichern\n",
    "        if query_count % SAVE_INTERVAL == 0:\n",
    "            new_results_df = pd.DataFrame(results)\n",
    "            loc_title_df = pd.concat([loc_title_df, new_results_df], ignore_index=True)\n",
    "            loc_title_df.to_parquet(LOC_TITLE_DATA_FILE, index=False)\n",
    "            results = []\n",
    "            \n",
    "            current_rate = stats['found'] / query_count * 100\n",
    "            print(f\"üíæ Zwischenstand: {query_count}/{len(new_title_queries)} | Erfolgsrate: {current_rate:.1f}%\")\n",
    "        \n",
    "        # Rate Limiting\n",
    "        time.sleep(RATE_LIMIT_DELAY)\n",
    "    \n",
    "    # Finale Speicherung\n",
    "    if len(results) > 0:\n",
    "        new_results_df = pd.DataFrame(results)\n",
    "        loc_title_df = pd.concat([loc_title_df, new_results_df], ignore_index=True)\n",
    "        loc_title_df.to_parquet(LOC_TITLE_DATA_FILE, index=False)\n",
    "    \n",
    "    print(f\"\\nüíæ LoC Titel/Autor-Daten gespeichert: {LOC_TITLE_DATA_FILE.name}\")\n",
    "    \n",
    "    print(f\"\\nüìä === NEUE TITEL/AUTOR-ABFRAGEN ===\")\n",
    "    print(f\"   Neue Queries: {len(new_title_queries):,}\")\n",
    "    print(f\"   ‚úÖ Gefunden: {stats['found']:,} ({stats['found']/len(new_title_queries)*100:.1f}%)\")\n",
    "    print(f\"   ‚ùå Nicht gefunden: {stats['not_found']:,} ({stats['not_found']/len(new_title_queries)*100:.1f}%)\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Alle Titel/Autor-Kombinationen bereits abgefragt\")\n",
    "\n",
    "# Gesamtstatistik\n",
    "print(f\"\\nüìä === GESAMT TITEL/AUTOR-DATEN ===\")\n",
    "print(f\"   Total Records: {len(loc_title_df):,}\")\n",
    "print(f\"   Erfolgreich: {(loc_title_df['loc_found'] == True).sum():,}\")\n",
    "print(f\"   Nicht gefunden: {(loc_title_df['loc_found'] == False).sum():,}\")\n",
    "if len(loc_title_df) > 0:\n",
    "    print(f\"   üìà Erfolgsrate: {(loc_title_df['loc_found'] == True).sum()/len(loc_title_df)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "merge_data",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T20:57:58.833299Z",
     "iopub.status.busy": "2025-12-25T20:57:58.833116Z",
     "iopub.status.idle": "2025-12-25T20:57:58.968246Z",
     "shell.execute_reply": "2025-12-25T20:57:58.967839Z"
    },
    "papermill": {
     "duration": 0.199011,
     "end_time": "2025-12-25T20:57:58.968982",
     "exception": false,
     "start_time": "2025-12-25T20:57:58.769971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó === LOC-DATEN MERGE ===\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ISBN/ISSN-basierte LoC-Daten (ID) gemerged\n",
      "   ID-Matches: 797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Titel/Autor-basierte LoC-Daten (TA) gemerged\n",
      "   TA-Matches: 853\n",
      "\n",
      "üìä === MERGE ZUSAMMENFASSUNG ===\n",
      "   Total Records: 58,305\n",
      "   Mit ID-LoC: 797\n",
      "   Mit TA-LoC: 853\n"
     ]
    }
   ],
   "source": [
    "# üîó LOC-DATEN MIT VDEH-DATEN ZUSAMMENF√úHREN\n",
    "print(\"üîó === LOC-DATEN MERGE ===\\n\")\n",
    "\n",
    "# Starte mit VDEH-Daten\n",
    "df_enriched = df_vdeh.copy()\n",
    "\n",
    "# 1. Merge ISBN/ISSN-basierte LoC-Daten\n",
    "if len(loc_data_df) > 0:\n",
    "    cols_to_merge = ['vdeh_id', 'query_type', 'loc_title', 'loc_authors', 'loc_year', 'loc_publisher']\n",
    "    if 'loc_isbn' in loc_data_df.columns:\n",
    "        cols_to_merge.extend(['loc_isbn', 'loc_issn', 'loc_pages'])\n",
    "    \n",
    "    loc_isbn_issn = loc_data_df[loc_data_df['loc_found'] == True][cols_to_merge].rename(\n",
    "        columns={'query_type': 'loc_query_method'}\n",
    "    )\n",
    "    \n",
    "    df_enriched = df_enriched.merge(\n",
    "        loc_isbn_issn,\n",
    "        left_on='id',\n",
    "        right_on='vdeh_id',\n",
    "        how='left',\n",
    "        suffixes=('', '_dup')\n",
    "    )\n",
    "    if 'vdeh_id' in df_enriched.columns:\n",
    "        df_enriched.drop(columns=['vdeh_id'], inplace=True)\n",
    "    \n",
    "    print(f\"‚úÖ ISBN/ISSN-basierte LoC-Daten (ID) gemerged\")\n",
    "    print(f\"   ID-Matches: {df_enriched['loc_query_method'].notna().sum():,}\")\n",
    "\n",
    "# 2. Merge Titel/Autor-basierte LoC-Daten als separate Variante (_ta)\n",
    "if len(loc_title_df) > 0:\n",
    "    cols_to_merge_ta = ['vdeh_id', 'loc_title', 'loc_authors', 'loc_year', 'loc_publisher']\n",
    "    if 'loc_isbn' in loc_title_df.columns:\n",
    "        cols_to_merge_ta.extend(['loc_isbn', 'loc_issn', 'loc_pages'])\n",
    "    \n",
    "    loc_title_matches = loc_title_df[loc_title_df['loc_found'] == True][cols_to_merge_ta].copy()\n",
    "    \n",
    "    rename_map = {\n",
    "        'loc_title': 'loc_title_ta',\n",
    "        'loc_authors': 'loc_authors_ta',\n",
    "        'loc_year': 'loc_year_ta',\n",
    "        'loc_publisher': 'loc_publisher_ta'\n",
    "    }\n",
    "    if 'loc_isbn' in cols_to_merge_ta:\n",
    "        rename_map.update({\n",
    "            'loc_isbn': 'loc_isbn_ta',\n",
    "            'loc_issn': 'loc_issn_ta',\n",
    "            'loc_pages': 'loc_pages_ta'\n",
    "        })\n",
    "    \n",
    "    loc_title_matches = loc_title_matches.rename(columns=rename_map)\n",
    "    \n",
    "    df_enriched = df_enriched.merge(\n",
    "        loc_title_matches,\n",
    "        left_on='id',\n",
    "        right_on='vdeh_id',\n",
    "        how='left'\n",
    "    )\n",
    "    if 'vdeh_id' in df_enriched.columns:\n",
    "        df_enriched.drop(columns=['vdeh_id'], inplace=True)\n",
    "    \n",
    "    print(f\"‚úÖ Titel/Autor-basierte LoC-Daten (TA) gemerged\")\n",
    "    print(f\"   TA-Matches: {df_enriched[['loc_title_ta','loc_authors_ta','loc_year_ta','loc_publisher_ta']].notna().any(axis=1).sum():,}\")\n",
    "\n",
    "# Zusammenfassung\n",
    "print(f\"\\nüìä === MERGE ZUSAMMENFASSUNG ===\")\n",
    "print(f\"   Total Records: {len(df_enriched):,}\")\n",
    "print(f\"   Mit ID-LoC: {df_enriched['loc_query_method'].notna().sum() if 'loc_query_method' in df_enriched.columns else 0:,}\")\n",
    "print(f\"   Mit TA-LoC: {df_enriched[['loc_title_ta','loc_authors_ta','loc_year_ta','loc_publisher_ta']].notna().any(axis=1).sum() if 'loc_title_ta' in df_enriched.columns else 0:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "normalize_types",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T20:57:58.976181Z",
     "iopub.status.busy": "2025-12-25T20:57:58.975920Z",
     "iopub.status.idle": "2025-12-25T20:57:58.991602Z",
     "shell.execute_reply": "2025-12-25T20:57:58.991238Z"
    },
    "papermill": {
     "duration": 0.021402,
     "end_time": "2025-12-25T20:57:58.992305",
     "exception": false,
     "start_time": "2025-12-25T20:57:58.970903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß === DATENTYP-NORMALISIERUNG ===\n",
      "\n",
      "   year: 33,313 ‚Üí 33,313 (Int64)\n",
      "   loc_year: 311 ‚Üí 311 (Int64)\n",
      "   loc_year_ta: 454 ‚Üí 454 (Int64)\n",
      "\n",
      "‚úÖ Datentypen normalisiert\n"
     ]
    }
   ],
   "source": [
    "# üîß DATENTYP-NORMALISIERUNG\n",
    "print(\"üîß === DATENTYP-NORMALISIERUNG ===\\n\")\n",
    "\n",
    "# Konvertiere Jahr-Spalten zu Int64\n",
    "year_columns = ['year', 'loc_year', 'loc_year_ta']\n",
    "\n",
    "for col in year_columns:\n",
    "    if col in df_enriched.columns:\n",
    "        original_count = df_enriched[col].notna().sum()\n",
    "        df_enriched[col] = pd.to_numeric(df_enriched[col], errors='coerce').astype('Int64')\n",
    "        new_count = df_enriched[col].notna().sum()\n",
    "        \n",
    "        print(f\"   {col}: {original_count:,} ‚Üí {new_count:,} (Int64)\")\n",
    "        \n",
    "        if original_count != new_count:\n",
    "            print(f\"      ‚ö†Ô∏è  {original_count - new_count:,} Werte konnten nicht konvertiert werden\")\n",
    "\n",
    "print(f\"\\n‚úÖ Datentypen normalisiert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "save_data",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-25T20:57:58.997771Z",
     "iopub.status.busy": "2025-12-25T20:57:58.997535Z",
     "iopub.status.idle": "2025-12-25T20:57:59.000862Z",
     "shell.execute_reply": "2025-12-25T20:57:59.000180Z"
    },
    "papermill": {
     "duration": 0.006829,
     "end_time": "2025-12-25T20:57:59.001514",
     "exception": false,
     "start_time": "2025-12-25T20:57:58.994685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# üíæ DATEN SPEICHERNprint(\"üíæ === DATEN SPEICHERN ===\\n\")# Output-Pfadeoutput_path = processed_dir / '04b_loc_enriched_data.parquet'metadata_output = processed_dir / '04b_metadata.json'# 1. Parquet speicherndf_enriched.to_parquet(output_path, index=False)print(f\"‚úÖ LoC-angereicherte Daten gespeichert: {output_path.name}\")print(f\"   Records: {len(df_enriched):,}\")print(f\"   Spalten: {len(df_enriched.columns)}\")print(f\"   Gr√∂√üe: {output_path.stat().st_size / 1024**2:.1f} MB\")# 2. Metadaten erstellenmetadata = {    'step': '04b_loc_enrichment',    'input_file': '03_language_detected_data.parquet',    'output_file': '04b_loc_enriched_data.parquet',    'timestamp': pd.Timestamp.now().isoformat(),    'record_count': len(df_enriched),    'english_records': len(df_english),    'columns': list(df_enriched.columns),        'loc_queries': {        'isbn_issn': {            'total_queries': len(loc_data_df) if len(loc_data_df) > 0 else 0,            'successful': int((loc_data_df['loc_found'] == True).sum()) if len(loc_data_df) > 0 else 0,            'failed': int((loc_data_df['loc_found'] == False).sum()) if len(loc_data_df) > 0 else 0        },        'title_author': {            'total_queries': len(loc_title_df) if len(loc_title_df) > 0 else 0,            'successful': int((loc_title_df['loc_found'] == True).sum()) if len(loc_title_df) > 0 else 0,            'failed': int((loc_title_df['loc_found'] == False).sum()) if len(loc_title_df) > 0 else 0        }    },        'loc_variants': {        'id_available': int(df_enriched['loc_query_method'].notna().sum()) if 'loc_query_method' in df_enriched.columns else 0,        'ta_available': int(df_enriched[['loc_title_ta','loc_authors_ta','loc_year_ta','loc_publisher_ta']].notna().any(axis=1).sum()) if 'loc_title_ta' in df_enriched.columns else 0    }}# Metadaten speichernwith open(metadata_output, 'w', encoding='utf-8') as f:    json.dump(metadata, f, indent=2, ensure_ascii=False)print(f\"\\n‚úÖ Metadaten gespeichert: {metadata_output.name}\")# 3. Zusammenfassungprint(f\"\\nüìä === LOC ENRICHMENT ABGESCHLOSSEN ===\")print(f\"   Input: {len(df_vdeh):,} VDEH Records\")print(f\"   Englischsprachig: {len(df_english):,}\")print(f\"   Output: {len(df_enriched):,} Records mit LoC-Daten\")print(f\"   ID-Variante verf√ºgbar: {metadata['loc_variants']['id_available']:,}\")print(f\"   TA-Variante verf√ºgbar: {metadata['loc_variants']['ta_available']:,}\")print(f\"\\n‚û°Ô∏è  N√§chster Schritt: Integration mit DNB-Daten in 05_vdeh_data_fusion.ipynb\")print(f\"\\nüéâ LoC Enrichment erfolgreich abgeschlossen!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2.276434,
   "end_time": "2025-12-25T20:57:59.321128",
   "environment_variables": {},
   "exception": null,
   "input_path": "04b_vdeh_loc_enrichment.ipynb",
   "output_path": "04b_vdeh_loc_enrichment_executed.ipynb",
   "parameters": {},
   "start_time": "2025-12-25T20:57:57.044694",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}