{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1450aed3",
   "metadata": {},
   "source": [
    "# VDEH Data Fusion Pipeline\n",
    "\n",
    "**Fokus:** KI-gestÃ¼tzte Fusion von VDEH und DNB Daten + Dual-Source Language Fusion\n",
    "\n",
    "## ğŸ¯ Ziel\n",
    "- Intelligente Fusion von VDEH-Original und DNB-Daten\n",
    "- KonfliktauflÃ¶sung via Ollama LLM\n",
    "- **Dual-Source Language Fusion**: MARC21 Sprache + langdetect Erkennung\n",
    "- VollstÃ¤ndige Nachvollziehbarkeit aller Entscheidungen\n",
    "- QualitÃ¤tsverbesserung durch Datenanreicherung\n",
    "\n",
    "## ğŸ“š Input/Output\n",
    "- **Input**: `data/vdeh/processed/04_dnb_enriched_data.parquet`\n",
    "- **Output**: `data/vdeh/processed/05_fused_data.parquet`\n",
    "\n",
    "## ğŸ¤– KI-Modell\n",
    "- **Ollama**: Lokales LLM (llama3.3:70b)\n",
    "- **API**: http://localhost:11434"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8895ae5a",
   "metadata": {},
   "source": [
    "## ğŸ”„ Fusion-Architektur\n",
    "\n",
    "**Drei Fusion-Strategien:**\n",
    "1. **Keine DNB-Daten** â†’ VDEH behalten\n",
    "2. **Keine Konflikte** â†’ Einfacher Merge (VDEH priorisiert, DNB ergÃ¤nzt)\n",
    "3. **Konflikte vorhanden** â†’ KI-Entscheidung via Ollama\n",
    "\n",
    "**VollstÃ¤ndige Nachvollziehbarkeit:**\n",
    "- `fusion_*_source`: Welche Quelle fÃ¼r jedes Feld\n",
    "- `fusion_conflicts`: JSON mit allen erkannten Konflikten\n",
    "- `fusion_ai_reasoning`: KI-BegrÃ¼ndung der Entscheidung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf981b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-11 13:08:41 - utils.notebook_utils - INFO - Searching for project root...\n",
      "2025-12-11 13:08:41 - utils.notebook_utils - INFO - Project root found: /media/sz/Data/Bibo/analysis\n",
      "2025-12-11 13:08:41 - utils.notebook_utils - INFO - Loading configuration...\n",
      "2025-12-11 13:08:41 - config_loader - INFO - Configuration loaded from /media/sz/Data/Bibo/analysis/config.yaml\n",
      "2025-12-11 13:08:41 - utils.notebook_utils - INFO - Configuration loaded successfully: Dual-Source Bibliothek Bestandsvergleich\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Project root: /media/sz/Data/Bibo/analysis\n",
      "âœ… Project: Dual-Source Bibliothek Bestandsvergleich v2.0.0\n"
     ]
    }
   ],
   "source": [
    "# ğŸ› ï¸ SETUP UND DATEN LADEN\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Add src to path (temporary until utils is imported)\n",
    "project_root = Path.cwd()\n",
    "while not (project_root / 'config.yaml').exists() and project_root.parent != project_root:\n",
    "    project_root = project_root.parent\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "# Now use the utility function\n",
    "from utils.notebook_utils import setup_notebook\n",
    "\n",
    "project_root, config = setup_notebook()\n",
    "print(f\"âœ… Project root: {project_root}\")\n",
    "print(f\"âœ… Project: {config.get('project.name')} v{config.get('project.version')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0063e59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Daten geladen aus: /media/sz/Data/Bibo/analysis/data/vdeh/processed/04_dnb_enriched_data.parquet\n",
      "ğŸ“Š Records: 58,305\n",
      "ğŸ’¾ Memory: 73.6 MB\n",
      "\n",
      "ğŸ“Š DNB-Daten vorhanden: 5,770 (9.9%)\n",
      "   ISBN: 5,770\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“‚ DNB-ANGEREICHERTE DATEN LADEN\n",
    "processed_dir = config.project_root / config.get('paths.data.vdeh.processed')\n",
    "input_path = processed_dir / '04_dnb_enriched_data.parquet'\n",
    "metadata_path = processed_dir / '04_metadata.json'\n",
    "\n",
    "if not input_path.exists():\n",
    "    raise FileNotFoundError(f\"Input-Datei nicht gefunden: {input_path}\\n\"\n",
    "                          \"Bitte fÃ¼hren Sie zuerst 04_vdeh_data_enrichment.ipynb aus.\")\n",
    "\n",
    "# Daten laden\n",
    "df_enriched = pd.read_parquet(input_path)\n",
    "\n",
    "# Vorherige Metadaten laden\n",
    "with open(metadata_path, 'r') as f:\n",
    "    prev_metadata = json.load(f)\n",
    "\n",
    "print(f\"ğŸ“‚ Daten geladen aus: {input_path}\")\n",
    "print(f\"ğŸ“Š Records: {len(df_enriched):,}\")\n",
    "print(f\"ğŸ’¾ Memory: {df_enriched.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# DNB-Daten Statistiken\n",
    "if 'dnb_query_method' in df_enriched.columns:\n",
    "    dnb_records = df_enriched['dnb_query_method'].notna().sum()\n",
    "    print(f\"\\nğŸ“Š DNB-Daten vorhanden: {dnb_records:,} ({dnb_records/len(df_enriched)*100:.1f}%)\")\n",
    "    \n",
    "    method_counts = df_enriched['dnb_query_method'].value_counts()\n",
    "    for method, count in method_counts.items():\n",
    "        print(f\"   {method}: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "65df10de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ === FUSION-SETUP ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-11 13:10:00 - fusion.ollama_client - INFO - Ollama connected: llama3.3:70b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ollama verbunden: llama3.3:70b\n",
      "âš™ï¸  Timeout: 220s | Retries: 4\n",
      "ğŸ¤– Aktives Modell: llama3.3:70b\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“‹ FUSION-SETUP\n",
    "from fusion import OllamaClient, FusionEngine\n",
    "\n",
    "print(\"ğŸ“‹ === FUSION-SETUP ===\\n\")\n",
    "\n",
    "# Ollama-Client initialisieren\n",
    "ollama_client = OllamaClient(\n",
    "    api_url=\"http://localhost:11434/api/generate\",\n",
    "    model=\"llama3.3:70b\",\n",
    "    timeout_sec=220,\n",
    "    max_retries=4,\n",
    "    retry_backoff_base_sec=2,\n",
    "    abort_on_timeout=True,\n",
    "    enable_fallback=True,\n",
    "    fallback_model=\"llama3.2\"\n",
    ")\n",
    "\n",
    "# Test connection\n",
    "if ollama_client.test_connection():\n",
    "    print(f\"âœ… Ollama verbunden: {ollama_client.model}\")\n",
    "else:\n",
    "    raise RuntimeError(\"âŒ Ollama nicht erreichbar! Stellen Sie sicher, dass Ollama lÃ¤uft: ollama serve\")\n",
    "\n",
    "# Fusion-Engine initialisieren\n",
    "fusion_engine = FusionEngine(\n",
    "    ollama_client=ollama_client,\n",
    "    variant_priority=[\"id\", \"title_author\"]\n",
    ")\n",
    "\n",
    "print(f\"âš™ï¸  Timeout: {ollama_client.timeout_sec}s | Retries: {ollama_client.max_retries}\")\n",
    "print(f\"ğŸ¤– Aktives Modell: {ollama_client.model}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5366010d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ === FUSION AUSFÃœHREN ===\n",
      "\n",
      "ğŸ“‚ Lade Fortschritt aus Progress-Datei...\n",
      "   âœ… 8,603 Records bereits fusioniert\n",
      "   âœ… Gesamte Daten wiederhergestellt aus Progress-Datei\n",
      "\n",
      "ğŸ“Š VollstÃ¤ndigkeit VOR Fusion:\n",
      "   title: 58,242 (99.9%)\n",
      "   authors: 17,694 (30.3%)\n",
      "   year: 33,452 (57.4%)\n",
      "   publisher: 23,881 (41.0%)\n",
      "\n",
      "ğŸ”„ Records mit DNB-Varianten: 9,446\n",
      "ğŸ”„ Verbleibende Records: 843\n",
      "   (Bereits fusioniert: 8,603)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2384996a5f94adb9894be6d50dd3320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ğŸ”„ Fusion:   0%|          | 0/843 [00:00<?, ?records/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ Zwischenstand: 8,653 Records fusioniert\n",
      "\n",
      "ğŸ’¾ Zwischenstand: 8,703 Records fusioniert\n",
      "\n",
      "ğŸ’¾ Zwischenstand: 8,753 Records fusioniert\n",
      "\n",
      "ğŸ’¾ Zwischenstand: 8,803 Records fusioniert\n",
      "\n",
      "ğŸ’¾ Zwischenstand: 8,853 Records fusioniert\n",
      "\n",
      "ğŸ’¾ Zwischenstand: 8,903 Records fusioniert\n",
      "\n",
      "ğŸ’¾ Zwischenstand: 8,953 Records fusioniert\n",
      "\n",
      "ğŸ’¾ Zwischenstand: 9,003 Records fusioniert\n",
      "\n",
      "ğŸ’¾ Zwischenstand: 9,053 Records fusioniert\n",
      "\n",
      "ğŸ’¾ Zwischenstand: 9,103 Records fusioniert\n",
      "\n",
      "ğŸ’¾ Zwischenstand: 9,153 Records fusioniert\n",
      "\n",
      "ğŸ’¾ Zwischenstand: 9,203 Records fusioniert\n",
      "\n",
      "ğŸ’¾ Zwischenstand: 9,253 Records fusioniert\n",
      "\n",
      "ğŸ’¾ Zwischenstand: 9,303 Records fusioniert\n",
      "\n",
      "ğŸ’¾ Zwischenstand: 9,353 Records fusioniert\n",
      "\n",
      "ğŸ’¾ Zwischenstand: 9,403 Records fusioniert\n",
      "\n",
      "ğŸ’¾ Finaler Stand gespeichert\n",
      "\n",
      "âœ… Fusion abgeschlossen\n"
     ]
    }
   ],
   "source": [
    "# ğŸš€ FUSION AUSFÃœHREN\n",
    "from tqdm.auto import tqdm\n",
    "from fusion import OllamaUnavailableError\n",
    "\n",
    "print(\"ğŸš€ === FUSION AUSFÃœHREN ===\\n\")\n",
    "\n",
    "# Configuration\n",
    "RESET_FUSION = False  # Set to True to reset all fusion results\n",
    "SAVE_INTERVAL = 50    # Save progress every N records\n",
    "\n",
    "# Optional limit for testing\n",
    "FUSION_LIMIT = None\n",
    "try:\n",
    "    FUSION_LIMIT = int(config.get('debug.fusion_limit', 0))\n",
    "    if FUSION_LIMIT <= 0:\n",
    "        FUSION_LIMIT = None\n",
    "except Exception:\n",
    "    FUSION_LIMIT = None\n",
    "\n",
    "# Progress tracking files\n",
    "progress_file = processed_dir / '05_fused_data_progress.parquet'\n",
    "retry_queue_file = processed_dir / '05_fused_retry_queue.json'\n",
    "\n",
    "# Reset if requested\n",
    "if RESET_FUSION:\n",
    "    if progress_file.exists():\n",
    "        progress_file.unlink()\n",
    "    if retry_queue_file.exists():\n",
    "        retry_queue_file.unlink()\n",
    "    print(\"ğŸ—‘ï¸ Fusion-Ergebnisse zurÃ¼ckgesetzt\\n\")\n",
    "\n",
    "# WICHTIG: Progress-Datei hat Vorrang Ã¼ber Input-Datei\n",
    "# Wenn wir resumed, laden wir den kompletten Zwischenstand\n",
    "if not RESET_FUSION and progress_file.exists():\n",
    "    print(\"ğŸ“‚ Lade Fortschritt aus Progress-Datei...\")\n",
    "    df_enriched = pd.read_parquet(progress_file)\n",
    "    \n",
    "    if not df_enriched.index.is_unique:\n",
    "        df_enriched = df_enriched[~df_enriched.index.duplicated(keep='last')]\n",
    "    \n",
    "    # Bestimme bereits fusionierte Records\n",
    "    if 'fusion_title_source' in df_enriched.columns:\n",
    "        already_fused = set(df_enriched[df_enriched['fusion_title_source'].notna()].index)\n",
    "    else:\n",
    "        already_fused = set()\n",
    "    \n",
    "    print(f\"   âœ… {len(already_fused):,} Records bereits fusioniert\")\n",
    "    print(f\"   âœ… Gesamte Daten wiederhergestellt aus Progress-Datei\")\n",
    "else:\n",
    "    already_fused = set()\n",
    "\n",
    "# Statistics BEFORE fusion\n",
    "print(\"\\nğŸ“Š VollstÃ¤ndigkeit VOR Fusion:\")\n",
    "before_stats = {\n",
    "    'title': df_enriched['title'].notna().sum(),\n",
    "    'authors': (df_enriched['authors_str'].notna() & (df_enriched['authors_str'] != '')).sum(),\n",
    "    'year': df_enriched['year'].notna().sum(),\n",
    "    'publisher': df_enriched['publisher'].notna().sum()\n",
    "}\n",
    "for field, count in before_stats.items():\n",
    "    print(f\"   {field}: {count:,} ({count/len(df_enriched)*100:.1f}%)\")\n",
    "\n",
    "# Identify records to process (those with any DNB variant)\n",
    "has_id = df_enriched[['dnb_title','dnb_authors','dnb_year','dnb_publisher']].notna().any(axis=1) if 'dnb_title' in df_enriched.columns else False\n",
    "has_ta = df_enriched[['dnb_title_ta','dnb_authors_ta','dnb_year_ta','dnb_publisher_ta']].notna().any(axis=1) if 'dnb_title_ta' in df_enriched.columns else False\n",
    "records_to_process = df_enriched[has_id | has_ta].copy()\n",
    "\n",
    "total_with_dnb = len(records_to_process)\n",
    "print(f\"\\nğŸ”„ Records mit DNB-Varianten: {total_with_dnb:,}\")\n",
    "\n",
    "# Filter already processed\n",
    "records_to_process = records_to_process[~records_to_process.index.isin(already_fused)]\n",
    "\n",
    "# Apply limit if in test mode\n",
    "if FUSION_LIMIT and FUSION_LIMIT > 0:\n",
    "    print(f\"ğŸ§ª Testmodus aktiv â€“ verarbeite nur die ersten {FUSION_LIMIT} Records.\")\n",
    "    records_to_process = records_to_process.head(FUSION_LIMIT)\n",
    "\n",
    "# Load and prioritize retry queue\n",
    "retry_indices = []\n",
    "if retry_queue_file.exists():\n",
    "    try:\n",
    "        with open(retry_queue_file, 'r', encoding='utf-8') as f:\n",
    "            retry_indices = json.load(f)\n",
    "    except Exception:\n",
    "        retry_indices = []\n",
    "\n",
    "retry_indices = [i for i in retry_indices if i in records_to_process.index]\n",
    "if len(retry_indices) > 0:\n",
    "    print(f\"ğŸ” Retry-Queue: {len(retry_indices):,} Records werden zuerst verarbeitet\")\n",
    "    retry_df = records_to_process.loc[records_to_process.index.isin(retry_indices)]\n",
    "    fresh_df = records_to_process.loc[~records_to_process.index.isin(retry_indices)]\n",
    "    records_to_process = pd.concat([retry_df, fresh_df], axis=0)\n",
    "\n",
    "print(f\"ğŸ”„ Verbleibende Records: {len(records_to_process):,}\")\n",
    "print(f\"   (Bereits fusioniert: {len(already_fused):,})\\n\")\n",
    "\n",
    "# Initialize statistics\n",
    "fusion_stats = {\n",
    "    'total_processed': len(already_fused),\n",
    "    'conflicts_found': 0,\n",
    "    'dnb_preferred': 0,\n",
    "    'simple_merges': 0,\n",
    "    'errors': 0,\n",
    "    'dnb_matches_rejected': 0,\n",
    "    'ai_decisions': 0,\n",
    "    'variant_id': 0,\n",
    "    'variant_title_author': 0,\n",
    "    'variant_none': 0\n",
    "}\n",
    "\n",
    "fusion_count = 0\n",
    "aborted = False\n",
    "\n",
    "# Main fusion loop\n",
    "for idx, row in tqdm(records_to_process.iterrows(), total=len(records_to_process), desc=\"ğŸ”„ Fusion\", unit=\"records\"):\n",
    "    try:\n",
    "        # Perform fusion\n",
    "        result = fusion_engine.merge_record(row)\n",
    "        result_dict = result.to_dict()\n",
    "        \n",
    "        # Update statistics\n",
    "        variant = result_dict.get('dnb_variant_selected')\n",
    "        if variant == 'id':\n",
    "            fusion_stats['variant_id'] += 1\n",
    "        elif variant == 'title_author':\n",
    "            fusion_stats['variant_title_author'] += 1\n",
    "        else:\n",
    "            fusion_stats['variant_none'] += 1\n",
    "        \n",
    "        # Store results in DataFrame\n",
    "        df_enriched.loc[idx, 'title'] = result_dict.get('title')\n",
    "        df_enriched.loc[idx, 'authors_str'] = result_dict.get('authors')\n",
    "        \n",
    "        # Convert year to numeric\n",
    "        year_val = result_dict.get('year')\n",
    "        if pd.notna(year_val):\n",
    "            try:\n",
    "                df_enriched.loc[idx, 'year'] = pd.to_numeric(year_val, errors='coerce')\n",
    "            except:\n",
    "                df_enriched.loc[idx, 'year'] = year_val\n",
    "        \n",
    "        df_enriched.loc[idx, 'publisher'] = result_dict.get('publisher')\n",
    "        df_enriched.loc[idx, 'fusion_title_source'] = result_dict.get('title_source')\n",
    "        df_enriched.loc[idx, 'fusion_authors_source'] = result_dict.get('authors_source')\n",
    "        df_enriched.loc[idx, 'fusion_year_source'] = result_dict.get('year_source')\n",
    "        df_enriched.loc[idx, 'fusion_publisher_source'] = result_dict.get('publisher_source')\n",
    "        df_enriched.loc[idx, 'fusion_conflicts'] = result_dict.get('conflicts')\n",
    "        df_enriched.loc[idx, 'fusion_confirmations'] = result_dict.get('confirmations')\n",
    "        df_enriched.loc[idx, 'fusion_ai_reasoning'] = result_dict.get('ai_reasoning')\n",
    "        df_enriched.loc[idx, 'fusion_dnb_match_rejected'] = result_dict.get('dnb_match_rejected', False)\n",
    "        df_enriched.loc[idx, 'fusion_rejection_reason'] = result_dict.get('rejection_reason')\n",
    "        df_enriched.loc[idx, 'fusion_dnb_variant_selected'] = result_dict.get('dnb_variant_selected')\n",
    "        \n",
    "        # Clear retry flag if set\n",
    "        if 'fusion_needs_retry' in df_enriched.columns:\n",
    "            df_enriched.loc[idx, 'fusion_needs_retry'] = False\n",
    "        if idx in retry_indices:\n",
    "            retry_indices = [i for i in retry_indices if i != idx]\n",
    "        \n",
    "        # Update statistics\n",
    "        fusion_stats['total_processed'] += 1\n",
    "        fusion_count += 1\n",
    "        fusion_stats['ai_decisions'] += 1\n",
    "        \n",
    "        if result_dict.get('dnb_match_rejected'):\n",
    "            fusion_stats['dnb_matches_rejected'] += 1\n",
    "        elif result_dict.get('conflicts'):\n",
    "            fusion_stats['conflicts_found'] += 1\n",
    "            fusion_stats['dnb_preferred'] += 1\n",
    "        else:\n",
    "            fusion_stats['simple_merges'] += 1\n",
    "        \n",
    "        # Incremental save\n",
    "        if fusion_count % SAVE_INTERVAL == 0:\n",
    "            df_enriched.to_parquet(progress_file, index=True)\n",
    "            with open(retry_queue_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(retry_indices, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"\\nğŸ’¾ Zwischenstand: {fusion_stats['total_processed']:,} Records fusioniert\")\n",
    "    \n",
    "    except OllamaUnavailableError as e:\n",
    "        print(f\"\\nâŒ Ollama nicht erreichbar: {e}\")\n",
    "        print(\"ğŸ‘‰ Record wird in die Retry-Queue gelegt\")\n",
    "        fusion_stats['errors'] += 1\n",
    "        aborted = True\n",
    "        \n",
    "        df_enriched.loc[idx, 'fusion_needs_retry'] = True\n",
    "        if idx not in retry_indices:\n",
    "            retry_indices.append(idx)\n",
    "        \n",
    "        # Save immediately\n",
    "        df_enriched.to_parquet(progress_file, index=True)\n",
    "        with open(retry_queue_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(retry_indices, f, ensure_ascii=False, indent=2)\n",
    "        break\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâš ï¸ Fehler bei Record {idx}: {e}\")\n",
    "        fusion_stats['errors'] += 1\n",
    "\n",
    "# Final save\n",
    "if fusion_count % SAVE_INTERVAL != 0 or fusion_count == 0:\n",
    "    df_enriched.to_parquet(progress_file, index=True)\n",
    "    with open(retry_queue_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(retry_indices, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"\\nğŸ’¾ Finaler Stand gespeichert\")\n",
    "\n",
    "if aborted:\n",
    "    print(\"\\nâ›”ï¸ Lauf abgebrochen (Ollama-Timeout)\")\n",
    "\n",
    "print(\"\\nâœ… Fusion abgeschlossen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y9ismt9zsql",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸŒ === LANGUAGE FUSION ===\n",
      "\n",
      "ğŸ“Š Applying dual-source language fusion...\n",
      "\n",
      "ğŸ“Š Language Fusion Results:\n",
      "   Total with language: 57,103 (97.9%)\n",
      "   From MARC21: 30,173 (51.8%)\n",
      "   From langdetect: 26,930 (46.2%)\n",
      "   No language: 1,202 (2.1%)\n",
      "\n",
      "ğŸŒ Top 10 Languages (final):\n",
      "   ger       : 20,910 ( 36.6%)\n",
      "   de        : 12,653 ( 22.2%)\n",
      "   en        :  8,309 ( 14.6%)\n",
      "   eng       :  7,699 ( 13.5%)\n",
      "   sl        :    614 (  1.1%)\n",
      "   fr        :    546 (  1.0%)\n",
      "   ro        :    540 (  0.9%)\n",
      "   it        :    510 (  0.9%)\n",
      "   ca        :    418 (  0.7%)\n",
      "   af        :    410 (  0.7%)\n",
      "\n",
      "âœ… Language fusion complete\n"
     ]
    }
   ],
   "source": [
    "# ğŸŒ LANGUAGE FUSION (Dual-Source Strategie)\n",
    "print(\"\\nğŸŒ === LANGUAGE FUSION ===\\n\")\n",
    "\n",
    "def merge_language(row):\n",
    "    \"\"\"\n",
    "    Merge MARC21 language and langdetect results.\n",
    "    \n",
    "    Priority:\n",
    "    1. MARC21 language (from catalog metadata - most reliable)\n",
    "    2. langdetect detected_language (from title analysis)\n",
    "    3. None if neither available\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (language_final, language_source, language_confidence)\n",
    "    \"\"\"\n",
    "    marc21_lang = row.get('language')\n",
    "    detected_lang = row.get('detected_language')\n",
    "    detected_conf = row.get('detected_language_confidence', 0.0)\n",
    "    \n",
    "    # MARC21 has priority\n",
    "    if pd.notna(marc21_lang) and str(marc21_lang).strip() not in ['', 'unknown']:\n",
    "        return str(marc21_lang).strip(), 'marc21', 1.0\n",
    "    \n",
    "    # Fallback to langdetect\n",
    "    elif pd.notna(detected_lang) and str(detected_lang).strip() not in ['', 'unknown']:\n",
    "        return str(detected_lang).strip(), 'langdetect', float(detected_conf) if pd.notna(detected_conf) else 0.0\n",
    "    \n",
    "    # No language information\n",
    "    else:\n",
    "        return None, None, 0.0\n",
    "\n",
    "# Apply language fusion\n",
    "if 'language' in df_enriched.columns or 'detected_language' in df_enriched.columns:\n",
    "    print(\"ğŸ“Š Applying dual-source language fusion...\")\n",
    "    \n",
    "    # Create new columns for merged language\n",
    "    df_enriched[['language_final', 'language_source', 'language_confidence']] = df_enriched.apply(\n",
    "        merge_language, axis=1, result_type='expand'\n",
    "    )\n",
    "    \n",
    "    # Statistics\n",
    "    marc21_count = df_enriched[df_enriched['language_source'] == 'marc21'].shape[0]\n",
    "    langdetect_count = df_enriched[df_enriched['language_source'] == 'langdetect'].shape[0]\n",
    "    total_with_lang = df_enriched['language_final'].notna().sum()\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Language Fusion Results:\")\n",
    "    print(f\"   Total with language: {total_with_lang:,} ({total_with_lang/len(df_enriched)*100:.1f}%)\")\n",
    "    print(f\"   From MARC21: {marc21_count:,} ({marc21_count/len(df_enriched)*100:.1f}%)\")\n",
    "    print(f\"   From langdetect: {langdetect_count:,} ({langdetect_count/len(df_enriched)*100:.1f}%)\")\n",
    "    print(f\"   No language: {len(df_enriched) - total_with_lang:,} ({(len(df_enriched) - total_with_lang)/len(df_enriched)*100:.1f}%)\")\n",
    "    \n",
    "    # Language distribution\n",
    "    print(f\"\\nğŸŒ Top 10 Languages (final):\")\n",
    "    lang_dist = df_enriched['language_final'].value_counts().head(10)\n",
    "    for lang, count in lang_dist.items():\n",
    "        if pd.notna(lang):\n",
    "            pct = count/total_with_lang*100 if total_with_lang > 0 else 0\n",
    "            print(f\"   {str(lang):10}: {count:6,} ({pct:5.1f}%)\")\n",
    "    \n",
    "    print(\"\\nâœ… Language fusion complete\")\n",
    "else:\n",
    "    print(\"âš ï¸ No language columns found - skipping language fusion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc12345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š === FUSION-ERGEBNISSE ===\n",
      "\n",
      "ğŸ“Š VollstÃ¤ndigkeit NACH Fusion:\n",
      "   title: 58,242 (99.9%) [+0]\n",
      "   authors: 17,667 (30.3%) [+62]\n",
      "   year: 33,426 (57.3%) [+40]\n",
      "   publisher: 23,849 (40.9%) [+87]\n",
      "\n",
      "ğŸ“Š Fusion-Statistiken:\n",
      "   Verarbeitet: 6,756\n",
      "   Einfache Merges: 217\n",
      "   DNB gewÃ¤hlt: 2,051\n",
      "   Konflikte: 2,051\n",
      "   ğŸš« DNB verworfen: 452\n",
      "   KI-Entscheidungen: 2,720\n",
      "   Variante ID: 1,576\n",
      "   Variante Titel/Autor: 692\n",
      "\n",
      "ğŸ“Š Datenquellen:\n",
      "\n",
      "   TITLE:\n",
      "     confirmed: 2,967\n",
      "     dnb_id: 2,224\n",
      "     vdeh: 1,038\n",
      "     dnb_title_author: 527\n",
      "\n",
      "   AUTHORS:\n",
      "     confirmed: 3,229\n",
      "     dnb_id: 1,665\n",
      "     vdeh: 1,038\n",
      "     dnb_title_author: 824\n",
      "\n",
      "   YEAR:\n",
      "     confirmed: 4,167\n",
      "     vdeh: 2,181\n",
      "     dnb_title_author: 238\n",
      "     dnb_id: 149\n",
      "\n",
      "   PUBLISHER:\n",
      "     dnb_id: 3,487\n",
      "     vdeh: 1,152\n",
      "     dnb_title_author: 1,061\n",
      "     confirmed: 1\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“Š FUSION-STATISTIKEN\n",
    "print(\"ğŸ“Š === FUSION-ERGEBNISSE ===\\n\")\n",
    "\n",
    "# Statistics AFTER fusion\n",
    "print(\"ğŸ“Š VollstÃ¤ndigkeit NACH Fusion:\")\n",
    "after_stats = {\n",
    "    'title': df_enriched['title'].notna().sum(),\n",
    "    'authors': (df_enriched['authors_str'].notna() & (df_enriched['authors_str'] != '')).sum(),\n",
    "    'year': df_enriched['year'].notna().sum(),\n",
    "    'publisher': df_enriched['publisher'].notna().sum()\n",
    "}\n",
    "for field, count in after_stats.items():\n",
    "    improvement = count - before_stats[field]\n",
    "    print(f\"   {field}: {count:,} ({count/len(df_enriched)*100:.1f}%) [+{improvement:,}]\")\n",
    "\n",
    "# Fusion statistics\n",
    "print(f\"\\nğŸ“Š Fusion-Statistiken:\")\n",
    "print(f\"   Verarbeitet: {fusion_stats['total_processed']:,}\")\n",
    "print(f\"   Einfache Merges: {fusion_stats['simple_merges']:,}\")\n",
    "print(f\"   DNB gewÃ¤hlt: {fusion_stats['dnb_preferred']:,}\")\n",
    "print(f\"   Konflikte: {fusion_stats['conflicts_found']:,}\")\n",
    "print(f\"   ğŸš« DNB verworfen: {fusion_stats['dnb_matches_rejected']:,}\")\n",
    "print(f\"   KI-Entscheidungen: {fusion_stats['ai_decisions']:,}\")\n",
    "print(f\"   Variante ID: {fusion_stats['variant_id']:,}\")\n",
    "print(f\"   Variante Titel/Autor: {fusion_stats['variant_title_author']:,}\")\n",
    "\n",
    "# Source distribution\n",
    "print(f\"\\nğŸ“Š Datenquellen:\")\n",
    "for field in ['title', 'authors', 'year', 'publisher']:\n",
    "    source_col = f'fusion_{field}_source'\n",
    "    if source_col in df_enriched.columns:\n",
    "        sources = df_enriched[source_col].value_counts()\n",
    "        print(f\"\\n   {field.upper()}:\")\n",
    "        for source, count in sources.items():\n",
    "            if source:\n",
    "                print(f\"     {source}: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gap_filling_logic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“ GAP FILLING: Fehlende Felder aus DNB-Daten ergÃ¤nzen\n",
    "print(\"\\nğŸ“ === GAP FILLING ===\\n\")\n",
    "\n",
    "print(\"FÃ¼lle fehlende Metadaten aus DNB-Daten...\\n\")\n",
    "\n",
    "# Statistics BEFORE gap filling\n",
    "before_gap_filling = {\n",
    "    'isbn': df_enriched['isbn'].notna().sum(),\n",
    "    'issn': df_enriched['issn'].notna().sum() if 'issn' in df_enriched.columns else 0,\n",
    "}\n",
    "\n",
    "filled_count = {\n",
    "    'isbn': 0,\n",
    "    'issn': 0,\n",
    "    'authors': 0,\n",
    "    'year': 0,\n",
    "    'publisher': 0\n",
    "}\n",
    "\n",
    "# 1. ISBN Gap Filling\n",
    "# Priority: dnb_isbn_ta (from title/author search - finds new ISBNs) > dnb_isbn (from ISBN search - only duplicates)\n",
    "if 'dnb_isbn_ta' in df_enriched.columns:\n",
    "    # Records with no ISBN but DNB has one\n",
    "    no_isbn = df_enriched['isbn'].isna()\n",
    "    has_dnb_isbn_ta = df_enriched['dnb_isbn_ta'].notna()\n",
    "    \n",
    "    fill_isbn_mask = no_isbn & has_dnb_isbn_ta\n",
    "    filled_count['isbn'] = fill_isbn_mask.sum()\n",
    "    \n",
    "    if filled_count['isbn'] > 0:\n",
    "        df_enriched.loc[fill_isbn_mask, 'isbn'] = df_enriched.loc[fill_isbn_mask, 'dnb_isbn_ta']\n",
    "        # Mark source\n",
    "        if 'isbn_source' not in df_enriched.columns:\n",
    "            df_enriched['isbn_source'] = None\n",
    "        df_enriched.loc[fill_isbn_mask, 'isbn_source'] = 'dnb_title_author'\n",
    "        \n",
    "        print(f\"   ISBN: {filled_count['isbn']:,} neu gefÃ¼llt aus dnb_isbn_ta\")\n",
    "\n",
    "# 2. ISSN Gap Filling\n",
    "if 'issn' in df_enriched.columns and 'dnb_issn_ta' in df_enriched.columns:\n",
    "    no_issn = df_enriched['issn'].isna()\n",
    "    has_dnb_issn_ta = df_enriched['dnb_issn_ta'].notna()\n",
    "    \n",
    "    fill_issn_mask = no_issn & has_dnb_issn_ta\n",
    "    filled_count['issn'] = fill_issn_mask.sum()\n",
    "    \n",
    "    if filled_count['issn'] > 0:\n",
    "        df_enriched.loc[fill_issn_mask, 'issn'] = df_enriched.loc[fill_issn_mask, 'dnb_issn_ta']\n",
    "        # Mark source\n",
    "        if 'issn_source' not in df_enriched.columns:\n",
    "            df_enriched['issn_source'] = None\n",
    "        df_enriched.loc[fill_issn_mask, 'issn_source'] = 'dnb_title_author'\n",
    "        \n",
    "        print(f\"   ISSN: {filled_count['issn']:,} neu gefÃ¼llt aus dnb_issn_ta\")\n",
    "\n",
    "# 3. Authors Gap Filling (from DNB where fusion didn't already fill)\n",
    "# This fills authors that were NOT handled by fusion (e.g., records without fusion)\n",
    "no_authors = (df_enriched['authors_str'].isna() | (df_enriched['authors_str'] == ''))\n",
    "not_fused = df_enriched['fusion_authors_source'].isna()\n",
    "\n",
    "# Try dnb_authors_ta first\n",
    "if 'dnb_authors_ta' in df_enriched.columns:\n",
    "    has_dnb_authors_ta = (df_enriched['dnb_authors_ta'].notna() & (df_enriched['dnb_authors_ta'] != ''))\n",
    "    fill_authors_mask = no_authors & not_fused & has_dnb_authors_ta\n",
    "    \n",
    "    if fill_authors_mask.sum() > 0:\n",
    "        df_enriched.loc[fill_authors_mask, 'authors_str'] = df_enriched.loc[fill_authors_mask, 'dnb_authors_ta']\n",
    "        df_enriched.loc[fill_authors_mask, 'fusion_authors_source'] = 'dnb_title_author_gap_fill'\n",
    "        filled_count['authors'] += fill_authors_mask.sum()\n",
    "\n",
    "# Then try dnb_authors\n",
    "if 'dnb_authors' in df_enriched.columns:\n",
    "    no_authors = (df_enriched['authors_str'].isna() | (df_enriched['authors_str'] == ''))\n",
    "    not_fused = df_enriched['fusion_authors_source'].isna()\n",
    "    has_dnb_authors = (df_enriched['dnb_authors'].notna() & (df_enriched['dnb_authors'] != ''))\n",
    "    fill_authors_mask = no_authors & not_fused & has_dnb_authors\n",
    "    \n",
    "    if fill_authors_mask.sum() > 0:\n",
    "        df_enriched.loc[fill_authors_mask, 'authors_str'] = df_enriched.loc[fill_authors_mask, 'dnb_authors']\n",
    "        df_enriched.loc[fill_authors_mask, 'fusion_authors_source'] = 'dnb_id_gap_fill'\n",
    "        filled_count['authors'] += fill_authors_mask.sum()\n",
    "\n",
    "if filled_count['authors'] > 0:\n",
    "    print(f\"   Authors: {filled_count['authors']:,} neu gefÃ¼llt aus DNB\")\n",
    "\n",
    "# 4. Year Gap Filling\n",
    "no_year = df_enriched['year'].isna()\n",
    "not_fused = df_enriched['fusion_year_source'].isna()\n",
    "\n",
    "# Try dnb_year_ta first\n",
    "if 'dnb_year_ta' in df_enriched.columns:\n",
    "    has_dnb_year_ta = df_enriched['dnb_year_ta'].notna()\n",
    "    fill_year_mask = no_year & not_fused & has_dnb_year_ta\n",
    "    \n",
    "    if fill_year_mask.sum() > 0:\n",
    "        df_enriched.loc[fill_year_mask, 'year'] = df_enriched.loc[fill_year_mask, 'dnb_year_ta']\n",
    "        df_enriched.loc[fill_year_mask, 'fusion_year_source'] = 'dnb_title_author_gap_fill'\n",
    "        filled_count['year'] += fill_year_mask.sum()\n",
    "\n",
    "# Then try dnb_year\n",
    "if 'dnb_year' in df_enriched.columns:\n",
    "    no_year = df_enriched['year'].isna()\n",
    "    not_fused = df_enriched['fusion_year_source'].isna()\n",
    "    has_dnb_year = df_enriched['dnb_year'].notna()\n",
    "    fill_year_mask = no_year & not_fused & has_dnb_year\n",
    "    \n",
    "    if fill_year_mask.sum() > 0:\n",
    "        df_enriched.loc[fill_year_mask, 'year'] = df_enriched.loc[fill_year_mask, 'dnb_year']\n",
    "        df_enriched.loc[fill_year_mask, 'fusion_year_source'] = 'dnb_id_gap_fill'\n",
    "        filled_count['year'] += fill_year_mask.sum()\n",
    "\n",
    "if filled_count['year'] > 0:\n",
    "    print(f\"   Year: {filled_count['year']:,} neu gefÃ¼llt aus DNB\")\n",
    "\n",
    "# 5. Publisher Gap Filling\n",
    "no_publisher = df_enriched['publisher'].isna()\n",
    "not_fused = df_enriched['fusion_publisher_source'].isna()\n",
    "\n",
    "# Try dnb_publisher_ta first\n",
    "if 'dnb_publisher_ta' in df_enriched.columns:\n",
    "    has_dnb_pub_ta = df_enriched['dnb_publisher_ta'].notna()\n",
    "    fill_pub_mask = no_publisher & not_fused & has_dnb_pub_ta\n",
    "    \n",
    "    if fill_pub_mask.sum() > 0:\n",
    "        df_enriched.loc[fill_pub_mask, 'publisher'] = df_enriched.loc[fill_pub_mask, 'dnb_publisher_ta']\n",
    "        df_enriched.loc[fill_pub_mask, 'fusion_publisher_source'] = 'dnb_title_author_gap_fill'\n",
    "        filled_count['publisher'] += fill_pub_mask.sum()\n",
    "\n",
    "# Then try dnb_publisher\n",
    "if 'dnb_publisher' in df_enriched.columns:\n",
    "    no_publisher = df_enriched['publisher'].isna()\n",
    "    not_fused = df_enriched['fusion_publisher_source'].isna()\n",
    "    has_dnb_pub = df_enriched['dnb_publisher'].notna()\n",
    "    fill_pub_mask = no_publisher & not_fused & has_dnb_pub\n",
    "    \n",
    "    if fill_pub_mask.sum() > 0:\n",
    "        df_enriched.loc[fill_pub_mask, 'publisher'] = df_enriched.loc[fill_pub_mask, 'dnb_publisher']\n",
    "        df_enriched.loc[fill_pub_mask, 'fusion_publisher_source'] = 'dnb_id_gap_fill'\n",
    "        filled_count['publisher'] += fill_pub_mask.sum()\n",
    "\n",
    "if filled_count['publisher'] > 0:\n",
    "    print(f\"   Publisher: {filled_count['publisher']:,} neu gefÃ¼llt aus DNB\")\n",
    "\n",
    "# Statistics AFTER gap filling\n",
    "after_gap_filling = {\n",
    "    'isbn': df_enriched['isbn'].notna().sum(),\n",
    "    'issn': df_enriched['issn'].notna().sum() if 'issn' in df_enriched.columns else 0,\n",
    "}\n",
    "\n",
    "print(\"\\nğŸ“Š Gap Filling Zusammenfassung:\")\n",
    "total_filled = sum(filled_count.values())\n",
    "print(f\"   Gesamt neu gefÃ¼llt: {total_filled:,} Felder\")\n",
    "print(f\"   ISBN: {before_gap_filling['isbn']:,} â†’ {after_gap_filling['isbn']:,} (+{filled_count['isbn']:,})\")\n",
    "print(f\"   ISSN: {before_gap_filling['issn']:,} â†’ {after_gap_filling['issn']:,} (+{filled_count['issn']:,})\")\n",
    "\n",
    "print(\"\\nâœ… Gap Filling abgeschlossen\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def45678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Fusionierte Daten gespeichert: /media/sz/Data/Bibo/analysis/data/vdeh/processed/05_fused_data.parquet\n",
      "   GrÃ¶ÃŸe: 6.9 MB\n",
      "ğŸ“‹ Metadaten gespeichert: /media/sz/Data/Bibo/analysis/data/vdeh/processed/05_metadata.json\n",
      "\n",
      "âœ… Pipeline-Stufe 05 abgeschlossen!\n"
     ]
    }
   ],
   "source": [
    "# ğŸ’¾ FINALE AUSGABE SPEICHERN\n",
    "import numpy as np\n",
    "\n",
    "output_path = processed_dir / '05_fused_data.parquet'\n",
    "output_metadata_path = processed_dir / '05_metadata.json'\n",
    "\n",
    "# Save fused data\n",
    "df_enriched.to_parquet(output_path, index=True)\n",
    "print(f\"ğŸ’¾ Fusionierte Daten gespeichert: {output_path}\")\n",
    "print(f\"   GrÃ¶ÃŸe: {output_path.stat().st_size / 1024**2:.1f} MB\")\n",
    "\n",
    "# Helper function to convert numpy/pandas types to native Python types\n",
    "def convert_to_native(obj):\n",
    "    \"\"\"Recursively convert numpy/pandas types to native Python types for JSON serialization.\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_to_native(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_native(item) for item in obj]\n",
    "    elif isinstance(obj, (np.integer, np.int64)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif pd.isna(obj):\n",
    "        return None\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Save metadata with type conversion\n",
    "metadata = {\n",
    "    'notebook': '05_vdeh_data_fusion',\n",
    "    'timestamp': pd.Timestamp.now().isoformat(),\n",
    "    'input_file': str(input_path),\n",
    "    'output_file': str(output_path),\n",
    "    'total_records': int(len(df_enriched)),\n",
    "    'fusion_statistics': convert_to_native(fusion_stats),\n",
    "    'completeness_before': convert_to_native(before_stats),\n",
    "    'completeness_after': convert_to_native(after_stats),\n",
    "    'previous_metadata': prev_metadata\n",
    "}\n",
    "\n",
    "with open(output_metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"ğŸ“‹ Metadaten gespeichert: {output_metadata_path}\")\n",
    "print(f\"\\nâœ… Pipeline-Stufe 05 abgeschlossen!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bibo-analysis-DoEGeq_l-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}