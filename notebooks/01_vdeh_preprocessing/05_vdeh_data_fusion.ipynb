{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1450aed3",
   "metadata": {},
   "source": [
    "# VDEH Data Fusion Pipeline\n",
    "\n",
    "**Fokus:** KI-gest√ºtzte Fusion von VDEH und DNB Daten\n",
    "\n",
    "## üéØ Ziel\n",
    "- Intelligente Fusion von VDEH-Original und DNB-Daten\n",
    "- Konfliktaufl√∂sung via Ollama LLM\n",
    "- Vollst√§ndige Nachvollziehbarkeit aller Entscheidungen\n",
    "- Qualit√§tsverbesserung durch Datenanreicherung\n",
    "\n",
    "## üìö Input/Output\n",
    "- **Input**: `data/vdeh/processed/04_dnb_enriched_data.parquet`\n",
    "- **Output**: `data/vdeh/processed/05_fused_data.parquet`\n",
    "\n",
    "## ü§ñ KI-Modell\n",
    "- **Ollama**: Lokales LLM (llama3.3:70b)\n",
    "- **API**: http://localhost:11434"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8895ae5a",
   "metadata": {},
   "source": [
    "## üîÑ Fusion-Architektur\n",
    "\n",
    "**Drei Fusion-Strategien:**\n",
    "1. **Keine DNB-Daten** ‚Üí VDEH behalten\n",
    "2. **Keine Konflikte** ‚Üí Einfacher Merge (VDEH priorisiert, DNB erg√§nzt)\n",
    "3. **Konflikte vorhanden** ‚Üí KI-Entscheidung via Ollama\n",
    "\n",
    "**Vollst√§ndige Nachvollziehbarkeit:**\n",
    "- `fusion_*_source`: Welche Quelle f√ºr jedes Feld\n",
    "- `fusion_conflicts`: JSON mit allen erkannten Konflikten\n",
    "- `fusion_ai_reasoning`: KI-Begr√ºndung der Entscheidung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf981b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ†Ô∏è SETUP UND DATEN LADEN\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from utils.notebook_utils import setup_notebook\n",
    "\n",
    "project_root, config = setup_notebook()\n",
    "print(f\"‚úÖ Project root: {project_root}\")\n",
    "print(f\"‚úÖ Project: {config.get('project.name')} v{config.get('project.version')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0063e59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÇ DNB-ANGEREICHERTE DATEN LADEN\n",
    "processed_dir = config.project_root / config.get('paths.data.vdeh.processed')\n",
    "input_path = processed_dir / '04_dnb_enriched_data.parquet'\n",
    "metadata_path = processed_dir / '04_metadata.json'\n",
    "\n",
    "if not input_path.exists():\n",
    "    raise FileNotFoundError(f\"Input-Datei nicht gefunden: {input_path}\\n\"\n",
    "                          \"Bitte f√ºhren Sie zuerst 04_vdeh_data_enrichment.ipynb aus.\")\n",
    "\n",
    "# Daten laden\n",
    "df_enriched = pd.read_parquet(input_path)\n",
    "\n",
    "# Vorherige Metadaten laden\n",
    "with open(metadata_path, 'r') as f:\n",
    "    prev_metadata = json.load(f)\n",
    "\n",
    "print(f\"üìÇ Daten geladen aus: {input_path}\")\n",
    "print(f\"üìä Records: {len(df_enriched):,}\")\n",
    "print(f\"üíæ Memory: {df_enriched.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# DNB-Daten Statistiken\n",
    "if 'dnb_query_method' in df_enriched.columns:\n",
    "    dnb_records = df_enriched['dnb_query_method'].notna().sum()\n",
    "    print(f\"\\nüìä DNB-Daten vorhanden: {dnb_records:,} ({dnb_records/len(df_enriched)*100:.1f}%)\")\n",
    "    \n",
    "    method_counts = df_enriched['dnb_query_method'].value_counts()\n",
    "    for method, count in method_counts.items():\n",
    "        print(f\"   {method}: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65df10de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìã FUSION-SETUP\n",
    "from fusion import OllamaClient, FusionEngine\n",
    "\n",
    "print(\"üìã === FUSION-SETUP ===\\n\")\n",
    "\n",
    "# Ollama-Client initialisieren\n",
    "ollama_client = OllamaClient(\n",
    "    api_url=\"http://localhost:11434/api/generate\",\n",
    "    model=\"llama3.3:70b\",\n",
    "    timeout_sec=220,\n",
    "    max_retries=4,\n",
    "    retry_backoff_base_sec=2,\n",
    "    abort_on_timeout=True,\n",
    "    enable_fallback=True,\n",
    "    fallback_model=\"llama3.2\"\n",
    ")\n",
    "\n",
    "# Test connection\n",
    "if ollama_client.test_connection():\n",
    "    print(f\"‚úÖ Ollama verbunden: {ollama_client.model}\")\n",
    "else:\n",
    "    raise RuntimeError(\"‚ùå Ollama nicht erreichbar! Stellen Sie sicher, dass Ollama l√§uft: ollama serve\")\n",
    "\n",
    "# Fusion-Engine initialisieren\n",
    "fusion_engine = FusionEngine(\n",
    "    ollama_client=ollama_client,\n",
    "    variant_priority=[\"id\", \"title_author\"]\n",
    ")\n",
    "\n",
    "print(f\"‚öôÔ∏è  Timeout: {ollama_client.timeout_sec}s | Retries: {ollama_client.max_retries}\")\n",
    "print(f\"ü§ñ Aktives Modell: {ollama_client.model}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5366010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ FUSION AUSF√úHREN\n",
    "from tqdm.auto import tqdm\n",
    "from fusion import OllamaUnavailableError\n",
    "\n",
    "print(\"üöÄ === FUSION AUSF√úHREN ===\\n\")\n",
    "\n",
    "# Configuration\n",
    "RESET_FUSION = False  # Set to True to reset all fusion results\n",
    "SAVE_INTERVAL = 50    # Save progress every N records\n",
    "\n",
    "# Optional limit for testing\n",
    "FUSION_LIMIT = None\n",
    "try:\n",
    "    FUSION_LIMIT = int(config.get('debug.fusion_limit', 0))\n",
    "    if FUSION_LIMIT <= 0:\n",
    "        FUSION_LIMIT = None\n",
    "except Exception:\n",
    "    FUSION_LIMIT = None\n",
    "\n",
    "# Statistics BEFORE fusion\n",
    "print(\"üìä Vollst√§ndigkeit VOR Fusion:\")\n",
    "before_stats = {\n",
    "    'title': df_enriched['title'].notna().sum(),\n",
    "    'authors': (df_enriched['authors_str'].notna() & (df_enriched['authors_str'] != '')).sum(),\n",
    "    'year': df_enriched['year'].notna().sum(),\n",
    "    'publisher': df_enriched['publisher'].notna().sum()\n",
    "}\n",
    "for field, count in before_stats.items():\n",
    "    print(f\"   {field}: {count:,} ({count/len(df_enriched)*100:.1f}%)\")\n",
    "\n",
    "# Identify records to process (those with any DNB variant)\n",
    "has_id = df_enriched[['dnb_title','dnb_authors','dnb_year','dnb_publisher']].notna().any(axis=1) if 'dnb_title' in df_enriched.columns else False\n",
    "has_ta = df_enriched[['dnb_title_ta','dnb_authors_ta','dnb_year_ta','dnb_publisher_ta']].notna().any(axis=1) if 'dnb_title_ta' in df_enriched.columns else False\n",
    "records_to_process = df_enriched[has_id | has_ta].copy()\n",
    "\n",
    "print(f\"\\nüîÑ Verarbeite {len(records_to_process):,} Records mit DNB-Varianten...\\n\")\n",
    "\n",
    "# Progress tracking files\n",
    "progress_file = processed_dir / '05_fused_data_progress.parquet'\n",
    "retry_queue_file = processed_dir / '05_fused_retry_queue.json'\n",
    "\n",
    "# Reset if requested\n",
    "if RESET_FUSION:\n",
    "    fusion_cols = [\n",
    "        'title', 'authors_str', 'year', 'publisher',\n",
    "        'fusion_title_source', 'fusion_authors_source', 'fusion_year_source', 'fusion_publisher_source',\n",
    "        'fusion_conflicts', 'fusion_confirmations', 'fusion_ai_reasoning',\n",
    "        'fusion_dnb_match_rejected', 'fusion_rejection_reason', 'fusion_dnb_variant_selected',\n",
    "        'fusion_needs_retry', 'fusion_decision_needed'\n",
    "    ]\n",
    "    for col in fusion_cols:\n",
    "        if col in df_enriched.columns:\n",
    "            df_enriched[col] = None\n",
    "    \n",
    "    if progress_file.exists():\n",
    "        progress_file.unlink()\n",
    "    print(\"üóëÔ∏è Fusion-Ergebnisse zur√ºckgesetzt\\n\")\n",
    "\n",
    "# Load progress if exists\n",
    "already_fused = set()\n",
    "if progress_file.exists():\n",
    "    df_progress = pd.read_parquet(progress_file)\n",
    "    if not df_progress.index.is_unique:\n",
    "        df_progress = df_progress[~df_progress.index.duplicated(keep='last')]\n",
    "    \n",
    "    if 'fusion_title_source' in df_progress.columns:\n",
    "        already_fused = set(df_progress[df_progress['fusion_title_source'].notna()].index)\n",
    "    \n",
    "    print(f\"üìÇ Fortschritt geladen: {len(already_fused):,} Records bereits fusioniert\")\n",
    "    \n",
    "    # Restore fused data\n",
    "    common_cols = [c for c in df_progress.columns if c in df_enriched.columns]\n",
    "    if len(common_cols) > 0 and len(already_fused) > 0:\n",
    "        idxs = [i for i in already_fused if i in df_enriched.index]\n",
    "        if len(idxs) > 0:\n",
    "            df_enriched.loc[idxs, common_cols] = df_progress.loc[idxs, common_cols].values\n",
    "        print(f\"   Fusionsdaten wiederhergestellt\\n\")\n",
    "\n",
    "# Filter already processed\n",
    "records_to_process = records_to_process[~records_to_process.index.isin(already_fused)]\n",
    "\n",
    "# Apply limit if in test mode\n",
    "if FUSION_LIMIT and FUSION_LIMIT > 0:\n",
    "    print(f\"üß™ Testmodus aktiv ‚Äì verarbeite nur die ersten {FUSION_LIMIT} Records.\")\n",
    "    records_to_process = records_to_process.head(FUSION_LIMIT)\n",
    "\n",
    "# Load and prioritize retry queue\n",
    "retry_indices = []\n",
    "if retry_queue_file.exists():\n",
    "    try:\n",
    "        with open(retry_queue_file, 'r', encoding='utf-8') as f:\n",
    "            retry_indices = json.load(f)\n",
    "    except Exception:\n",
    "        retry_indices = []\n",
    "\n",
    "retry_indices = [i for i in retry_indices if i in records_to_process.index]\n",
    "if len(retry_indices) > 0:\n",
    "    print(f\"üîÅ Retry-Queue: {len(retry_indices):,} Records werden zuerst verarbeitet\")\n",
    "    retry_df = records_to_process.loc[records_to_process.index.isin(retry_indices)]\n",
    "    fresh_df = records_to_process.loc[~records_to_process.index.isin(retry_indices)]\n",
    "    records_to_process = pd.concat([retry_df, fresh_df], axis=0)\n",
    "\n",
    "print(f\"üîÑ Verbleibende Records: {len(records_to_process):,}\\n\")\n",
    "\n",
    "# Initialize statistics\n",
    "fusion_stats = {\n",
    "    'total_processed': len(already_fused),\n",
    "    'conflicts_found': 0,\n",
    "    'dnb_preferred': 0,\n",
    "    'simple_merges': 0,\n",
    "    'errors': 0,\n",
    "    'dnb_matches_rejected': 0,\n",
    "    'ai_decisions': 0,\n",
    "    'variant_id': 0,\n",
    "    'variant_title_author': 0,\n",
    "    'variant_none': 0\n",
    "}\n",
    "\n",
    "fusion_count = 0\n",
    "aborted = False\n",
    "\n",
    "# Main fusion loop\n",
    "for idx, row in tqdm(records_to_process.iterrows(), total=len(records_to_process), desc=\"üîÑ Fusion\", unit=\"records\"):\n",
    "    try:\n",
    "        # Perform fusion\n",
    "        result = fusion_engine.merge_record(row)\n",
    "        result_dict = result.to_dict()\n",
    "        \n",
    "        # Update statistics\n",
    "        variant = result_dict.get('dnb_variant_selected')\n",
    "        if variant == 'id':\n",
    "            fusion_stats['variant_id'] += 1\n",
    "        elif variant == 'title_author':\n",
    "            fusion_stats['variant_title_author'] += 1\n",
    "        else:\n",
    "            fusion_stats['variant_none'] += 1\n",
    "        \n",
    "        # Store results in DataFrame\n",
    "        df_enriched.loc[idx, 'title'] = result_dict.get('title')\n",
    "        df_enriched.loc[idx, 'authors_str'] = result_dict.get('authors')\n",
    "        \n",
    "        # Convert year to numeric\n",
    "        year_val = result_dict.get('year')\n",
    "        if pd.notna(year_val):\n",
    "            try:\n",
    "                df_enriched.loc[idx, 'year'] = pd.to_numeric(year_val, errors='coerce')\n",
    "            except:\n",
    "                df_enriched.loc[idx, 'year'] = year_val\n",
    "        \n",
    "        df_enriched.loc[idx, 'publisher'] = result_dict.get('publisher')\n",
    "        df_enriched.loc[idx, 'fusion_title_source'] = result_dict.get('title_source')\n",
    "        df_enriched.loc[idx, 'fusion_authors_source'] = result_dict.get('authors_source')\n",
    "        df_enriched.loc[idx, 'fusion_year_source'] = result_dict.get('year_source')\n",
    "        df_enriched.loc[idx, 'fusion_publisher_source'] = result_dict.get('publisher_source')\n",
    "        df_enriched.loc[idx, 'fusion_conflicts'] = result_dict.get('conflicts')\n",
    "        df_enriched.loc[idx, 'fusion_confirmations'] = result_dict.get('confirmations')\n",
    "        df_enriched.loc[idx, 'fusion_ai_reasoning'] = result_dict.get('ai_reasoning')\n",
    "        df_enriched.loc[idx, 'fusion_dnb_match_rejected'] = result_dict.get('dnb_match_rejected', False)\n",
    "        df_enriched.loc[idx, 'fusion_rejection_reason'] = result_dict.get('rejection_reason')\n",
    "        df_enriched.loc[idx, 'fusion_dnb_variant_selected'] = result_dict.get('dnb_variant_selected')\n",
    "        \n",
    "        # Clear retry flag if set\n",
    "        if 'fusion_needs_retry' in df_enriched.columns:\n",
    "            df_enriched.loc[idx, 'fusion_needs_retry'] = False\n",
    "        if idx in retry_indices:\n",
    "            retry_indices = [i for i in retry_indices if i != idx]\n",
    "        \n",
    "        # Update statistics\n",
    "        fusion_stats['total_processed'] += 1\n",
    "        fusion_count += 1\n",
    "        fusion_stats['ai_decisions'] += 1\n",
    "        \n",
    "        if result_dict.get('dnb_match_rejected'):\n",
    "            fusion_stats['dnb_matches_rejected'] += 1\n",
    "        elif result_dict.get('conflicts'):\n",
    "            fusion_stats['conflicts_found'] += 1\n",
    "            fusion_stats['dnb_preferred'] += 1\n",
    "        else:\n",
    "            fusion_stats['simple_merges'] += 1\n",
    "        \n",
    "        # Incremental save\n",
    "        if fusion_count % SAVE_INTERVAL == 0:\n",
    "            df_enriched.to_parquet(progress_file, index=True)\n",
    "            with open(retry_queue_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(retry_indices, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"\\nüíæ Zwischenstand: {fusion_stats['total_processed']:,} Records fusioniert\")\n",
    "    \n",
    "    except OllamaUnavailableError as e:\n",
    "        print(f\"\\n‚ùå Ollama nicht erreichbar: {e}\")\n",
    "        print(\"üëâ Record wird in die Retry-Queue gelegt\")\n",
    "        fusion_stats['errors'] += 1\n",
    "        aborted = True\n",
    "        \n",
    "        df_enriched.loc[idx, 'fusion_needs_retry'] = True\n",
    "        if idx not in retry_indices:\n",
    "            retry_indices.append(idx)\n",
    "        \n",
    "        # Save immediately\n",
    "        df_enriched.to_parquet(progress_file, index=True)\n",
    "        with open(retry_queue_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(retry_indices, f, ensure_ascii=False, indent=2)\n",
    "        break\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è Fehler bei Record {idx}: {e}\")\n",
    "        fusion_stats['errors'] += 1\n",
    "\n",
    "# Final save\n",
    "if fusion_count % SAVE_INTERVAL != 0 or fusion_count == 0:\n",
    "    df_enriched.to_parquet(progress_file, index=True)\n",
    "    with open(retry_queue_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(retry_indices, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"\\nüíæ Finaler Stand gespeichert\")\n",
    "\n",
    "if aborted:\n",
    "    print(\"\\n‚õîÔ∏è Lauf abgebrochen (Ollama-Timeout)\")\n",
    "\n",
    "print(\"\\n‚úÖ Fusion abgeschlossen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc12345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä FUSION-STATISTIKEN\n",
    "print(\"üìä === FUSION-ERGEBNISSE ===\\n\")\n",
    "\n",
    "# Statistics AFTER fusion\n",
    "print(\"üìä Vollst√§ndigkeit NACH Fusion:\")\n",
    "after_stats = {\n",
    "    'title': df_enriched['title'].notna().sum(),\n",
    "    'authors': (df_enriched['authors_str'].notna() & (df_enriched['authors_str'] != '')).sum(),\n",
    "    'year': df_enriched['year'].notna().sum(),\n",
    "    'publisher': df_enriched['publisher'].notna().sum()\n",
    "}\n",
    "for field, count in after_stats.items():\n",
    "    improvement = count - before_stats[field]\n",
    "    print(f\"   {field}: {count:,} ({count/len(df_enriched)*100:.1f}%) [+{improvement:,}]\")\n",
    "\n",
    "# Fusion statistics\n",
    "print(f\"\\nüìä Fusion-Statistiken:\")\n",
    "print(f\"   Verarbeitet: {fusion_stats['total_processed']:,}\")\n",
    "print(f\"   Einfache Merges: {fusion_stats['simple_merges']:,}\")\n",
    "print(f\"   DNB gew√§hlt: {fusion_stats['dnb_preferred']:,}\")\n",
    "print(f\"   Konflikte: {fusion_stats['conflicts_found']:,}\")\n",
    "print(f\"   üö´ DNB verworfen: {fusion_stats['dnb_matches_rejected']:,}\")\n",
    "print(f\"   KI-Entscheidungen: {fusion_stats['ai_decisions']:,}\")\n",
    "print(f\"   Variante ID: {fusion_stats['variant_id']:,}\")\n",
    "print(f\"   Variante Titel/Autor: {fusion_stats['variant_title_author']:,}\")\n",
    "\n",
    "# Source distribution\n",
    "print(f\"\\nüìä Datenquellen:\")\n",
    "for field in ['title', 'authors', 'year', 'publisher']:\n",
    "    source_col = f'fusion_{field}_source'\n",
    "    if source_col in df_enriched.columns:\n",
    "        sources = df_enriched[source_col].value_counts()\n",
    "        print(f\"\\n   {field.upper()}:\")\n",
    "        for source, count in sources.items():\n",
    "            if source:\n",
    "                print(f\"     {source}: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def45678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üíæ FINALE AUSGABE SPEICHERN\n",
    "output_path = processed_dir / '05_fused_data.parquet'\n",
    "output_metadata_path = processed_dir / '05_metadata.json'\n",
    "\n",
    "# Save fused data\n",
    "df_enriched.to_parquet(output_path, index=True)\n",
    "print(f\"üíæ Fusionierte Daten gespeichert: {output_path}\")\n",
    "print(f\"   Gr√∂√üe: {output_path.stat().st_size / 1024**2:.1f} MB\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'notebook': '05_vdeh_data_fusion',\n",
    "    'timestamp': pd.Timestamp.now().isoformat(),\n",
    "    'input_file': str(input_path),\n",
    "    'output_file': str(output_path),\n",
    "    'total_records': len(df_enriched),\n",
    "    'fusion_statistics': fusion_stats,\n",
    "    'completeness_before': before_stats,\n",
    "    'completeness_after': after_stats,\n",
    "    'previous_metadata': prev_metadata\n",
    "}\n",
    "\n",
    "with open(output_metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"üìã Metadaten gespeichert: {output_metadata_path}\")\n",
    "print(f\"\\n‚úÖ Pipeline-Stufe 05 abgeschlossen!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
