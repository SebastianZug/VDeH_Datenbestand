{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed4e9ef6",
   "metadata": {},
   "source": [
    "# VDEH Language Detection Pipeline\n",
    "\n",
    "**Fokus:** Professionelle Sprach-Erkennung mit langdetect\n",
    "\n",
    "## \ud83c\udfaf Ziel\n",
    "- Sprach-Erkennung f\u00fcr alle VDEH Titel\n",
    "- Confidence-Scoring und Qualit\u00e4tsfilter\n",
    "- Sprachname-Mapping und Kategorisierung\n",
    "- Export der sprach-angereicherten Daten\n",
    "\n",
    "## \ud83d\udcda Input/Output\n",
    "- **Input**: `data/vdeh/processed/02_preprocessed_data.parquet`\n",
    "- **Output**: `data/vdeh/processed/03_language_detected_data.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2f1bed",
   "metadata": {},
   "outputs": [],
   "source": "# \ud83d\udee0\ufe0f SETUP: Initialize notebook environment\nimport sys\nfrom pathlib import Path\n\n# Add src to path (temporary until utils is imported)\nproject_root = Path.cwd()\nwhile not (project_root / 'config.yaml').exists() and project_root.parent != project_root:\n    project_root = project_root.parent\nsys.path.insert(0, str(project_root / 'src'))\n\n# Now use the utility function\nfrom utils.notebook_utils import setup_notebook\n\nproject_root, config = setup_notebook()\nprint(f\"\u2705 Project root: {project_root}\")\nprint(f\"\u2705 Project: {config.get('project.name')} v{config.get('project.version')}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87c85489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udcc2 Daten geladen aus: /media/sz/Data/Bibo/analysis/data/vdeh/processed/02_preprocessed_data.parquet\n",
      "\ud83d\udcca Records: 58,760\n",
      "\ud83d\udccb Spalten: ['id', 'title', 'authors', 'authors_affiliation', 'year', 'publisher', 'isbn', 'issn', 'authors_str', 'num_authors', 'authors_affiliation_str', 'num_authors_affiliation', 'isbn_valid', 'isbn_status', 'issn_valid', 'issn_status']\n",
      "\ud83d\udcc5 Vorherige Verarbeitung: 2025-11-06T12:51:10.287634\n"
     ]
    }
   ],
   "source": "# \ud83d\udcc2 DATEN AUS VORHERIGER STUFE LADEN\nimport pandas as pd\nimport json\n\nprocessed_dir = config.project_root / config.get('paths.data.vdeh.processed')\ninput_path = processed_dir / '02_preprocessed_data.parquet'\nmetadata_path = processed_dir / '02_metadata.json'\n\nif not input_path.exists():\n    raise FileNotFoundError(f\"Input-Datei nicht gefunden: {input_path}\\n\"\n                          \"Bitte f\u00fchren Sie zuerst 02_vdeh_data_preprocessing.ipynb aus.\")\n\n# Daten laden\ndf_vdeh = pd.read_parquet(input_path)\n\n# Vorherige Metadaten laden\nwith open(metadata_path, 'r') as f:\n    prev_metadata = json.load(f)\n\nprint(f\"\ud83d\udcc2 Daten geladen aus: {input_path}\")\nprint(f\"\ud83d\udcca Records: {len(df_vdeh):,}\")\nprint(f\"\ud83d\udccb Spalten: {list(df_vdeh.columns)}\")\nprint(f\"\ud83d\udcc5 Vorherige Verarbeitung: {prev_metadata['processing_date']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e39145",
   "metadata": {},
   "outputs": [],
   "source": "# \ud83c\udf0d SPRACH-ERKENNUNG SETUP\nimport pandas as pd\nimport numpy as np\nimport json\n\n# Import language detection and progress bar\nfrom langdetect import detect_langs, LangDetectException\nfrom tqdm import tqdm\n\nprint(\"\ud83c\udf0d === SPRACH-ERKENNUNG SETUP ===\")\nprint(\"\u2705 langdetect imported\")\nprint(\"\u2705 tqdm imported f\u00fcr Progress-Anzeige\")\nprint(\"\u2705 Sprach-Erkennung Setup abgeschlossen\")"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "686315f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udd0d Filtere Titel f\u00fcr Spracherkennung...\n",
      "\ud83d\udcca Urspr\u00fcngliche Anzahl Titel: 58,760\n",
      "\ud83d\udcca Titel f\u00fcr Spracherkennung: 40,544\n",
      "\u23ed\ufe0f  \u00dcberspringe 18,216 leere/zu kurze Titel\n"
     ]
    }
   ],
   "source": [
    "# \ud83c\udfaf TITEL FILTERN \n",
    "print(\"\ud83d\udd0d Filtere Titel f\u00fcr Spracherkennung...\")\n",
    "\n",
    "# Minimum Textl\u00e4nge aus Config\n",
    "min_length = config.get('data_processing.language_detection.min_text_length', 10)\n",
    "\n",
    "# Leere oder zu kurze Titel filtern\n",
    "mask = df_vdeh['title'].notna() & (df_vdeh['title'].str.len() >= min_length)\n",
    "titles_to_process = df_vdeh[mask]\n",
    "\n",
    "print(f\"\ud83d\udcca Urspr\u00fcngliche Anzahl Titel: {len(df_vdeh):,}\")\n",
    "print(f\"\ud83d\udcca Titel f\u00fcr Spracherkennung: {len(titles_to_process):,}\")\n",
    "print(f\"\u23ed\ufe0f  \u00dcberspringe {len(df_vdeh) - len(titles_to_process):,} leere/zu kurze Titel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab259440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Sprach-Erkennungsfunktion definiert\n",
      "\ud83d\udccf Min. Textl\u00e4nge: 10\n",
      "\n",
      "\ud83e\uddea === FUNKTIONSTEST ===\n",
      "   'Das ist ein deutscher Titel...' \u2192 de (1.000) - German\n",
      "   'This is an English title...' \u2192 en (1.000) - English\n",
      "   'Ceci est un titre fran\u00e7ais...' \u2192 fr (1.000) - French\n"
     ]
    }
   ],
   "source": [
    "# \ud83c\udfaf SPRACH-ERKENNUNG FUNKTION\n",
    "def detect_language_professional(text):\n",
    "    \"\"\"\n",
    "    Professionelle Sprach-Erkennung mit langdetect (konfigurationsbasiert)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (language_code, confidence, full_language_name)\n",
    "    \"\"\"\n",
    "    min_length = config.get('data_processing.language_detection.min_text_length', 10)\n",
    "    \n",
    "    if not text or pd.isna(text) or len(str(text).strip()) < min_length:\n",
    "        return 'unknown', 0.0, 'Unknown'\n",
    "    \n",
    "    try:\n",
    "        # Text bereinigen\n",
    "        clean_text = str(text).encode('utf-8', errors='ignore').decode('utf-8').strip()\n",
    "        \n",
    "        if len(clean_text) < min_length:\n",
    "            return 'unknown', 0.0, 'Unknown'\n",
    "        \n",
    "        # Sprach-Erkennung\n",
    "        lang_probs = detect_langs(clean_text)\n",
    "        best_match = lang_probs[0]\n",
    "        \n",
    "        lang_code = best_match.lang\n",
    "        confidence = round(best_match.prob, 3)\n",
    "        \n",
    "        # Standard-Mapping f\u00fcr h\u00e4ufige Sprachen\n",
    "        supported_langs = {\n",
    "            'de': 'German',\n",
    "            'en': 'English', \n",
    "            'fr': 'French',\n",
    "            'es': 'Spanish',\n",
    "            'it': 'Italian',\n",
    "            'nl': 'Dutch',\n",
    "            'pl': 'Polish',\n",
    "            'ru': 'Russian',\n",
    "            'ja': 'Japanese',\n",
    "            'zh': 'Chinese',\n",
    "            'uk': 'Ukrainian',\n",
    "            'cs': 'Czech',\n",
    "            'hu': 'Hungarian',\n",
    "            'da': 'Danish',\n",
    "            'fi': 'Finnish',\n",
    "            'no': 'Norwegian',\n",
    "            'sv': 'Swedish'  \n",
    "        }\n",
    "        \n",
    "        # Erg\u00e4nze durch Konfiguration\n",
    "        config_langs = config.get('data_processing.language_detection.supported_languages', {})\n",
    "        if isinstance(config_langs, dict):\n",
    "            supported_langs.update(config_langs)\n",
    "        elif isinstance(config_langs, list):\n",
    "            for lang_item in config_langs:\n",
    "                if isinstance(lang_item, dict):\n",
    "                    supported_langs.update(lang_item)\n",
    "        \n",
    "        # Finde den vollst\u00e4ndigen Namen\n",
    "        full_name = supported_langs.get(lang_code, lang_code.upper())\n",
    "        \n",
    "        return lang_code, confidence, full_name\n",
    "        \n",
    "    except (LangDetectException, Exception):\n",
    "        return 'unknown', 0.0, 'Unknown'\n",
    "\n",
    "print(\"\u2705 Sprach-Erkennungsfunktion definiert\")\n",
    "print(f\"\ud83d\udccf Min. Textl\u00e4nge: {config.get('data_processing.language_detection.min_text_length')}\")\n",
    "\n",
    "# Test der Funktion\n",
    "test_titles = [\"Das ist ein deutscher Titel\", \"This is an English title\", \"Ceci est un titre fran\u00e7ais\"]\n",
    "print(f\"\\n\ud83e\uddea === FUNKTIONSTEST ===\")\n",
    "for title in test_titles:\n",
    "    lang_code, confidence, lang_name = detect_language_professional(title)\n",
    "    print(f\"   '{title[:30]}...' \u2192 {lang_code} ({confidence:.3f}) - {lang_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e828b66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83c\udf0d F\u00fchre Spracherkennung durch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\ud83c\udf0d Sprach-Erkennung: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40544/40544 [01:31<00:00, 445.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u2705 Spracherkennung f\u00fcr 40,544 Titel abgeschlossen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# \ud83c\udf0d SPRACHERKENNUNG DURCHF\u00dcHREN\n",
    "print(\"\ud83c\udf0d F\u00fchre Spracherkennung durch...\")\n",
    "\n",
    "# Arrays f\u00fcr Ergebnisse initialisieren\n",
    "language_codes = []\n",
    "confidence_scores = []\n",
    "language_names = []\n",
    "\n",
    "# Progress Bar\n",
    "with tqdm(titles_to_process['title'], desc=\"\ud83c\udf0d Sprach-Erkennung\") as pbar:\n",
    "    for title in pbar:\n",
    "        lang_code, confidence, lang_name = detect_language_professional(title)\n",
    "        language_codes.append(lang_code)\n",
    "        confidence_scores.append(confidence)\n",
    "        language_names.append(lang_name)\n",
    "\n",
    "print(f\"\\n\u2705 Spracherkennung f\u00fcr {len(titles_to_process):,} Titel abgeschlossen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0428f022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udcca === SPRACH-DATEN INTEGRATION ===\n",
      "\u2705 Sprach-Spalten hinzugef\u00fcgt\n",
      "\ud83d\udcca DataFrame: 19 Spalten, 58,760 Zeilen\n",
      "\ud83c\udfaf Hohe Konfidenz (>=0.3): 40,526 Titel\n",
      "\n",
      "\ud83c\udf0d Top 10 Sprachen:\n",
      "   German         : 25,184 ( 62.1%)\n",
      "   English        : 10,916 ( 26.9%)\n",
      "   French         :    639 (  1.6%)\n",
      "   Italian        :    533 (  1.3%)\n",
      "   CA             :    319 (  0.8%)\n",
      "   SL             :    303 (  0.7%)\n",
      "   Dutch          :    279 (  0.7%)\n",
      "   AF             :    270 (  0.7%)\n",
      "   RO             :    267 (  0.7%)\n",
      "   ET             :    204 (  0.5%)\n"
     ]
    }
   ],
   "source": [
    "# \ud83d\udcca SPRACH-DATEN ZUM DATAFRAME HINZUF\u00dcGEN\n",
    "print(\"\ud83d\udcca === SPRACH-DATEN INTEGRATION ===\")\n",
    "\n",
    "# Neue Spalten initialisieren\n",
    "df_vdeh['lang_code'] = 'unknown'\n",
    "df_vdeh['lang_confidence'] = 0.0\n",
    "df_vdeh['lang_name'] = 'Unknown'\n",
    "\n",
    "# Ergebnisse einf\u00fcgen (nur f\u00fcr verarbeitete Titel)\n",
    "df_vdeh.loc[titles_to_process.index, 'lang_code'] = language_codes\n",
    "df_vdeh.loc[titles_to_process.index, 'lang_confidence'] = confidence_scores\n",
    "df_vdeh.loc[titles_to_process.index, 'lang_name'] = language_names\n",
    "\n",
    "print(f\"\u2705 Sprach-Spalten hinzugef\u00fcgt\")\n",
    "print(f\"\ud83d\udcca DataFrame: {len(df_vdeh.columns)} Spalten, {len(df_vdeh):,} Zeilen\")\n",
    "\n",
    "# Qualit\u00e4ts-Statistiken\n",
    "min_confidence = config.get('analysis.quality_filters.min_confidence_score', 0.3)\n",
    "high_confidence_count = sum(1 for c in confidence_scores if c >= min_confidence)\n",
    "print(f\"\ud83c\udfaf Hohe Konfidenz (>={min_confidence}): {high_confidence_count:,} Titel\")\n",
    "\n",
    "# Sprach-Verteilung\n",
    "lang_dist = pd.Series(language_names).value_counts().head(10)\n",
    "print(f\"\\n\ud83c\udf0d Top 10 Sprachen:\")\n",
    "for lang, count in lang_dist.items():\n",
    "    pct = count/len(language_names)*100 if len(language_names) > 0 else 0\n",
    "    print(f\"   {lang:15}: {count:6,} ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2561bf7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udcbe === LANGUAGE DETECTION ABGESCHLOSSEN ===\n",
      "\u2705 Language-detected Daten exportiert: /media/sz/Data/Bibo/analysis/data/vdeh/processed/03_language_detected_data.parquet\n",
      "\ud83d\udcca Records: 58,760\n",
      "\ud83d\udccb Spalten: 19 (inkl. 3 Sprach-Spalten)\n",
      "\ud83d\udcbe Dateigr\u00f6\u00dfe: 52.8 MB\n"
     ]
    }
   ],
   "source": [
    "# \ud83d\udcbe LANGUAGE-DETECTED DATEN EXPORTIEREN\n",
    "output_path = processed_dir / '03_language_detected_data.parquet'\n",
    "df_vdeh.to_parquet(output_path, index=False)\n",
    "\n",
    "print(f\"\ud83d\udcbe === LANGUAGE DETECTION ABGESCHLOSSEN ===\")\n",
    "print(f\"\u2705 Language-detected Daten exportiert: {output_path}\")\n",
    "print(f\"\ud83d\udcca Records: {len(df_vdeh):,}\")\n",
    "print(f\"\ud83d\udccb Spalten: {len(df_vdeh.columns)} (inkl. {len([c for c in df_vdeh.columns if 'lang' in c])} Sprach-Spalten)\")\n",
    "print(f\"\ud83d\udcbe Dateigr\u00f6\u00dfe: {df_vdeh.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Detaillierte Sprach-Statistiken f\u00fcr Metadaten\n",
    "lang_stats = {\n",
    "    'total_titles_analyzed': int(len(titles_to_process)),  # titles_to_process statt all_titles\n",
    "    'high_confidence_count': int(high_confidence_count),\n",
    "    'confidence_threshold': float(min_confidence),\n",
    "    'top_languages': {k: int(v) for k,v in dict(lang_dist.head(5)).items()},\n",
    "    'confidence_distribution': {\n",
    "        'mean': float(np.mean(confidence_scores)),\n",
    "        'median': float(np.median(confidence_scores)),\n",
    "        'std': float(np.std(confidence_scores))\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c090a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udcc4 Metadaten gespeichert: /media/sz/Data/Bibo/analysis/data/vdeh/processed/03_metadata.json\n",
      "\n",
      "\u27a1\ufe0f  N\u00e4chster Schritt: 04_vdeh_quality_analysis.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Statistische Auswertung erstellen\n",
    "from datetime import datetime\n",
    "\n",
    "lang_stats = {\n",
    "    'processing_date': datetime.now().isoformat(),\n",
    "    'num_total': int(len(df_vdeh)),\n",
    "    'num_processed': int(df_vdeh['lang_code'].notna().sum()),\n",
    "    'num_errors': int(df_vdeh.get('detect_error', pd.Series(dtype='bool')).notna().sum()),\n",
    "    'language_analysis': {\n",
    "        'total_titles_analyzed': int(len(titles_to_process)),\n",
    "        'high_confidence_count': int(sum(df_vdeh['lang_confidence'] >= config.get('data_processing.language_detection.confidence_threshold', 0.5)))\n",
    "    },\n",
    "    'lang_distribution': {\n",
    "        str(lang): int(count.item()) if hasattr(count, 'item') else int(count)  # Konvertiere NumPy Typen\n",
    "        for lang, count in df_vdeh['lang_code'].value_counts().to_dict().items()\n",
    "        if not pd.isna(lang)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Metadaten speichern\n",
    "metadata_path = processed_dir / '03_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(lang_stats, f, indent=2)\n",
    "\n",
    "print(f\"\ud83d\udcc4 Metadaten gespeichert: {metadata_path}\")\n",
    "print(f\"\\n\u27a1\ufe0f  N\u00e4chster Schritt: 04_vdeh_quality_analysis.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce421d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bibo-analysis-DoEGeq_l-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}