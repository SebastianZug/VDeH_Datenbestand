{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed4e9ef6",
   "metadata": {},
   "source": [
    "# VDEH Language Detection Pipeline\n",
    "\n",
    "**Fokus:** Professionelle Sprach-Erkennung mit langdetect\n",
    "\n",
    "## ğŸ¯ Ziel\n",
    "- Sprach-Erkennung fÃ¼r alle VDEH Titel\n",
    "- Confidence-Scoring und QualitÃ¤tsfilter\n",
    "- Sprachname-Mapping und Kategorisierung\n",
    "- Export der sprach-angereicherten Daten\n",
    "\n",
    "## ğŸ“š Input/Output\n",
    "- **Input**: `data/vdeh/processed/02_preprocessed_data.parquet`\n",
    "- **Output**: `data/vdeh/processed/03_language_detected_data.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2f1bed",
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ› ï¸ SETUP: Initialize notebook environment\nimport sys\nfrom pathlib import Path\n\n# Add src to path (temporary until utils is imported)\nproject_root = Path.cwd()\nwhile not (project_root / 'config.yaml').exists() and project_root.parent != project_root:\n    project_root = project_root.parent\nsys.path.insert(0, str(project_root / 'src'))\n\n# Now use the utility function\nfrom utils.notebook_utils import setup_notebook\n\nproject_root, config = setup_notebook()\nprint(f\"âœ… Project root: {project_root}\")\nprint(f\"âœ… Project: {config.get('project.name')} v{config.get('project.version')}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87c85489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Daten geladen aus: /media/sz/Data/Bibo/analysis/data/vdeh/processed/02_preprocessed_data.parquet\n",
      "ğŸ“Š Records: 58,760\n",
      "ğŸ“‹ Spalten: ['id', 'title', 'authors', 'authors_affiliation', 'year', 'publisher', 'isbn', 'issn', 'authors_str', 'num_authors', 'authors_affiliation_str', 'num_authors_affiliation', 'isbn_valid', 'isbn_status', 'issn_valid', 'issn_status']\n",
      "ğŸ“… Vorherige Verarbeitung: 2025-11-06T12:51:10.287634\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“‚ DATEN AUS VORHERIGER STUFE LADEN\n",
    "processed_dir = config.project_root / config.get('paths.data.vdeh.processed')\n",
    "input_path = processed_dir / '02_preprocessed_data.parquet'\n",
    "metadata_path = processed_dir / '02_metadata.json'\n",
    "\n",
    "if not input_path.exists():\n",
    "    raise FileNotFoundError(f\"Input-Datei nicht gefunden: {input_path}\\n\"\n",
    "                          \"Bitte fÃ¼hren Sie zuerst 02_vdeh_data_preprocessing.ipynb aus.\")\n",
    "\n",
    "# Daten laden\n",
    "df_vdeh = pd.read_parquet(input_path)\n",
    "\n",
    "# Vorherige Metadaten laden\n",
    "with open(metadata_path, 'r') as f:\n",
    "    prev_metadata = json.load(f)\n",
    "\n",
    "print(f\"ğŸ“‚ Daten geladen aus: {input_path}\")\n",
    "print(f\"ğŸ“Š Records: {len(df_vdeh):,}\")\n",
    "print(f\"ğŸ“‹ Spalten: {list(df_vdeh.columns)}\")\n",
    "print(f\"ğŸ“… Vorherige Verarbeitung: {prev_metadata['processing_date']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e39145",
   "metadata": {},
   "outputs": [],
   "source": "# ğŸŒ SPRACH-ERKENNUNG SETUP\nimport pandas as pd\nimport numpy as np\nimport json\n\n# Import language detection and progress bar\nfrom langdetect import detect_langs, LangDetectException\nfrom tqdm import tqdm\n\nprint(\"ğŸŒ === SPRACH-ERKENNUNG SETUP ===\")\nprint(\"âœ… langdetect imported\")\nprint(\"âœ… tqdm imported fÃ¼r Progress-Anzeige\")\nprint(\"âœ… Sprach-Erkennung Setup abgeschlossen\")"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "686315f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Filtere Titel fÃ¼r Spracherkennung...\n",
      "ğŸ“Š UrsprÃ¼ngliche Anzahl Titel: 58,760\n",
      "ğŸ“Š Titel fÃ¼r Spracherkennung: 40,544\n",
      "â­ï¸  Ãœberspringe 18,216 leere/zu kurze Titel\n"
     ]
    }
   ],
   "source": [
    "# ğŸ¯ TITEL FILTERN \n",
    "print(\"ğŸ” Filtere Titel fÃ¼r Spracherkennung...\")\n",
    "\n",
    "# Minimum TextlÃ¤nge aus Config\n",
    "min_length = config.get('data_processing.language_detection.min_text_length', 10)\n",
    "\n",
    "# Leere oder zu kurze Titel filtern\n",
    "mask = df_vdeh['title'].notna() & (df_vdeh['title'].str.len() >= min_length)\n",
    "titles_to_process = df_vdeh[mask]\n",
    "\n",
    "print(f\"ğŸ“Š UrsprÃ¼ngliche Anzahl Titel: {len(df_vdeh):,}\")\n",
    "print(f\"ğŸ“Š Titel fÃ¼r Spracherkennung: {len(titles_to_process):,}\")\n",
    "print(f\"â­ï¸  Ãœberspringe {len(df_vdeh) - len(titles_to_process):,} leere/zu kurze Titel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab259440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Sprach-Erkennungsfunktion definiert\n",
      "ğŸ“ Min. TextlÃ¤nge: 10\n",
      "\n",
      "ğŸ§ª === FUNKTIONSTEST ===\n",
      "   'Das ist ein deutscher Titel...' â†’ de (1.000) - German\n",
      "   'This is an English title...' â†’ en (1.000) - English\n",
      "   'Ceci est un titre franÃ§ais...' â†’ fr (1.000) - French\n"
     ]
    }
   ],
   "source": [
    "# ğŸ¯ SPRACH-ERKENNUNG FUNKTION\n",
    "def detect_language_professional(text):\n",
    "    \"\"\"\n",
    "    Professionelle Sprach-Erkennung mit langdetect (konfigurationsbasiert)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (language_code, confidence, full_language_name)\n",
    "    \"\"\"\n",
    "    min_length = config.get('data_processing.language_detection.min_text_length', 10)\n",
    "    \n",
    "    if not text or pd.isna(text) or len(str(text).strip()) < min_length:\n",
    "        return 'unknown', 0.0, 'Unknown'\n",
    "    \n",
    "    try:\n",
    "        # Text bereinigen\n",
    "        clean_text = str(text).encode('utf-8', errors='ignore').decode('utf-8').strip()\n",
    "        \n",
    "        if len(clean_text) < min_length:\n",
    "            return 'unknown', 0.0, 'Unknown'\n",
    "        \n",
    "        # Sprach-Erkennung\n",
    "        lang_probs = detect_langs(clean_text)\n",
    "        best_match = lang_probs[0]\n",
    "        \n",
    "        lang_code = best_match.lang\n",
    "        confidence = round(best_match.prob, 3)\n",
    "        \n",
    "        # Standard-Mapping fÃ¼r hÃ¤ufige Sprachen\n",
    "        supported_langs = {\n",
    "            'de': 'German',\n",
    "            'en': 'English', \n",
    "            'fr': 'French',\n",
    "            'es': 'Spanish',\n",
    "            'it': 'Italian',\n",
    "            'nl': 'Dutch',\n",
    "            'pl': 'Polish',\n",
    "            'ru': 'Russian',\n",
    "            'ja': 'Japanese',\n",
    "            'zh': 'Chinese',\n",
    "            'uk': 'Ukrainian',\n",
    "            'cs': 'Czech',\n",
    "            'hu': 'Hungarian',\n",
    "            'da': 'Danish',\n",
    "            'fi': 'Finnish',\n",
    "            'no': 'Norwegian',\n",
    "            'sv': 'Swedish'  \n",
    "        }\n",
    "        \n",
    "        # ErgÃ¤nze durch Konfiguration\n",
    "        config_langs = config.get('data_processing.language_detection.supported_languages', {})\n",
    "        if isinstance(config_langs, dict):\n",
    "            supported_langs.update(config_langs)\n",
    "        elif isinstance(config_langs, list):\n",
    "            for lang_item in config_langs:\n",
    "                if isinstance(lang_item, dict):\n",
    "                    supported_langs.update(lang_item)\n",
    "        \n",
    "        # Finde den vollstÃ¤ndigen Namen\n",
    "        full_name = supported_langs.get(lang_code, lang_code.upper())\n",
    "        \n",
    "        return lang_code, confidence, full_name\n",
    "        \n",
    "    except (LangDetectException, Exception):\n",
    "        return 'unknown', 0.0, 'Unknown'\n",
    "\n",
    "print(\"âœ… Sprach-Erkennungsfunktion definiert\")\n",
    "print(f\"ğŸ“ Min. TextlÃ¤nge: {config.get('data_processing.language_detection.min_text_length')}\")\n",
    "\n",
    "# Test der Funktion\n",
    "test_titles = [\"Das ist ein deutscher Titel\", \"This is an English title\", \"Ceci est un titre franÃ§ais\"]\n",
    "print(f\"\\nğŸ§ª === FUNKTIONSTEST ===\")\n",
    "for title in test_titles:\n",
    "    lang_code, confidence, lang_name = detect_language_professional(title)\n",
    "    print(f\"   '{title[:30]}...' â†’ {lang_code} ({confidence:.3f}) - {lang_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e828b66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ FÃ¼hre Spracherkennung durch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸŒ Sprach-Erkennung: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40544/40544 [01:31<00:00, 445.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Spracherkennung fÃ¼r 40,544 Titel abgeschlossen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ğŸŒ SPRACHERKENNUNG DURCHFÃœHREN\n",
    "print(\"ğŸŒ FÃ¼hre Spracherkennung durch...\")\n",
    "\n",
    "# Arrays fÃ¼r Ergebnisse initialisieren\n",
    "language_codes = []\n",
    "confidence_scores = []\n",
    "language_names = []\n",
    "\n",
    "# Progress Bar\n",
    "with tqdm(titles_to_process['title'], desc=\"ğŸŒ Sprach-Erkennung\") as pbar:\n",
    "    for title in pbar:\n",
    "        lang_code, confidence, lang_name = detect_language_professional(title)\n",
    "        language_codes.append(lang_code)\n",
    "        confidence_scores.append(confidence)\n",
    "        language_names.append(lang_name)\n",
    "\n",
    "print(f\"\\nâœ… Spracherkennung fÃ¼r {len(titles_to_process):,} Titel abgeschlossen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0428f022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š === SPRACH-DATEN INTEGRATION ===\n",
      "âœ… Sprach-Spalten hinzugefÃ¼gt\n",
      "ğŸ“Š DataFrame: 19 Spalten, 58,760 Zeilen\n",
      "ğŸ¯ Hohe Konfidenz (>=0.3): 40,526 Titel\n",
      "\n",
      "ğŸŒ Top 10 Sprachen:\n",
      "   German         : 25,184 ( 62.1%)\n",
      "   English        : 10,916 ( 26.9%)\n",
      "   French         :    639 (  1.6%)\n",
      "   Italian        :    533 (  1.3%)\n",
      "   CA             :    319 (  0.8%)\n",
      "   SL             :    303 (  0.7%)\n",
      "   Dutch          :    279 (  0.7%)\n",
      "   AF             :    270 (  0.7%)\n",
      "   RO             :    267 (  0.7%)\n",
      "   ET             :    204 (  0.5%)\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“Š SPRACH-DATEN ZUM DATAFRAME HINZUFÃœGEN\n",
    "print(\"ğŸ“Š === SPRACH-DATEN INTEGRATION ===\")\n",
    "\n",
    "# Neue Spalten initialisieren\n",
    "df_vdeh['lang_code'] = 'unknown'\n",
    "df_vdeh['lang_confidence'] = 0.0\n",
    "df_vdeh['lang_name'] = 'Unknown'\n",
    "\n",
    "# Ergebnisse einfÃ¼gen (nur fÃ¼r verarbeitete Titel)\n",
    "df_vdeh.loc[titles_to_process.index, 'lang_code'] = language_codes\n",
    "df_vdeh.loc[titles_to_process.index, 'lang_confidence'] = confidence_scores\n",
    "df_vdeh.loc[titles_to_process.index, 'lang_name'] = language_names\n",
    "\n",
    "print(f\"âœ… Sprach-Spalten hinzugefÃ¼gt\")\n",
    "print(f\"ğŸ“Š DataFrame: {len(df_vdeh.columns)} Spalten, {len(df_vdeh):,} Zeilen\")\n",
    "\n",
    "# QualitÃ¤ts-Statistiken\n",
    "min_confidence = config.get('analysis.quality_filters.min_confidence_score', 0.3)\n",
    "high_confidence_count = sum(1 for c in confidence_scores if c >= min_confidence)\n",
    "print(f\"ğŸ¯ Hohe Konfidenz (>={min_confidence}): {high_confidence_count:,} Titel\")\n",
    "\n",
    "# Sprach-Verteilung\n",
    "lang_dist = pd.Series(language_names).value_counts().head(10)\n",
    "print(f\"\\nğŸŒ Top 10 Sprachen:\")\n",
    "for lang, count in lang_dist.items():\n",
    "    pct = count/len(language_names)*100 if len(language_names) > 0 else 0\n",
    "    print(f\"   {lang:15}: {count:6,} ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2561bf7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ === LANGUAGE DETECTION ABGESCHLOSSEN ===\n",
      "âœ… Language-detected Daten exportiert: /media/sz/Data/Bibo/analysis/data/vdeh/processed/03_language_detected_data.parquet\n",
      "ğŸ“Š Records: 58,760\n",
      "ğŸ“‹ Spalten: 19 (inkl. 3 Sprach-Spalten)\n",
      "ğŸ’¾ DateigrÃ¶ÃŸe: 52.8 MB\n"
     ]
    }
   ],
   "source": [
    "# ğŸ’¾ LANGUAGE-DETECTED DATEN EXPORTIEREN\n",
    "output_path = processed_dir / '03_language_detected_data.parquet'\n",
    "df_vdeh.to_parquet(output_path, index=False)\n",
    "\n",
    "print(f\"ğŸ’¾ === LANGUAGE DETECTION ABGESCHLOSSEN ===\")\n",
    "print(f\"âœ… Language-detected Daten exportiert: {output_path}\")\n",
    "print(f\"ğŸ“Š Records: {len(df_vdeh):,}\")\n",
    "print(f\"ğŸ“‹ Spalten: {len(df_vdeh.columns)} (inkl. {len([c for c in df_vdeh.columns if 'lang' in c])} Sprach-Spalten)\")\n",
    "print(f\"ğŸ’¾ DateigrÃ¶ÃŸe: {df_vdeh.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Detaillierte Sprach-Statistiken fÃ¼r Metadaten\n",
    "lang_stats = {\n",
    "    'total_titles_analyzed': int(len(titles_to_process)),  # titles_to_process statt all_titles\n",
    "    'high_confidence_count': int(high_confidence_count),\n",
    "    'confidence_threshold': float(min_confidence),\n",
    "    'top_languages': {k: int(v) for k,v in dict(lang_dist.head(5)).items()},\n",
    "    'confidence_distribution': {\n",
    "        'mean': float(np.mean(confidence_scores)),\n",
    "        'median': float(np.median(confidence_scores)),\n",
    "        'std': float(np.std(confidence_scores))\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c090a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Metadaten gespeichert: /media/sz/Data/Bibo/analysis/data/vdeh/processed/03_metadata.json\n",
      "\n",
      "â¡ï¸  NÃ¤chster Schritt: 04_vdeh_quality_analysis.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Statistische Auswertung erstellen\n",
    "from datetime import datetime\n",
    "\n",
    "lang_stats = {\n",
    "    'processing_date': datetime.now().isoformat(),\n",
    "    'num_total': int(len(df_vdeh)),\n",
    "    'num_processed': int(df_vdeh['lang_code'].notna().sum()),\n",
    "    'num_errors': int(df_vdeh.get('detect_error', pd.Series(dtype='bool')).notna().sum()),\n",
    "    'language_analysis': {\n",
    "        'total_titles_analyzed': int(len(titles_to_process)),\n",
    "        'high_confidence_count': int(sum(df_vdeh['lang_confidence'] >= config.get('data_processing.language_detection.confidence_threshold', 0.5)))\n",
    "    },\n",
    "    'lang_distribution': {\n",
    "        str(lang): int(count.item()) if hasattr(count, 'item') else int(count)  # Konvertiere NumPy Typen\n",
    "        for lang, count in df_vdeh['lang_code'].value_counts().to_dict().items()\n",
    "        if not pd.isna(lang)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Metadaten speichern\n",
    "metadata_path = processed_dir / '03_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(lang_stats, f, indent=2)\n",
    "\n",
    "print(f\"ğŸ“„ Metadaten gespeichert: {metadata_path}\")\n",
    "print(f\"\\nâ¡ï¸  NÃ¤chster Schritt: 04_vdeh_quality_analysis.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce421d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bibo-analysis-DoEGeq_l-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}