{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# VDEH-Sammlung Bestandsabgleich mit UB TU Freiberg\n",
    "\n",
    "**Ziel:** √úberpr√ºfung, welche B√ºcher der VDEH-Sammlung im Bestand der UB TU Freiberg vorhanden sind\n",
    "\n",
    "## üéØ Aufgabe\n",
    "- **Vorher-Nachher-Vergleich**: Evaluation mit Original-VDEH vs. Fusionierte VDEH-Daten\n",
    "- Lade UB MAB2-Katalogdaten\n",
    "- Matching-Strategien: ISBN, Titel+Autor\n",
    "- Identifiziere Matches und Nicht-Matches\n",
    "- Vergleiche Matching-Erfolg vor und nach Fusion\n",
    "\n",
    "## üìö Datenquellen\n",
    "- **VDEH Original**: `data/vdeh/processed/03_language_detected_data.parquet` (vor DNB/LoC Anreicherung)\n",
    "- **VDEH Fused**: `data/vdeh/processed/06_vdeh_dnb_loc_fused_data.parquet` (nach DNB/LoC Anreicherung)\n",
    "- **UB MAB2**: `data/ub_tubaf/processed/01_loaded_data.parquet` (518.946 Datens√§tze)\n",
    "\n",
    "## üîç Matching-Strategien\n",
    "1. **ISBN-Match**: Exakte ISBN-√úbereinstimmung (h√∂chste Pr√§zision)\n",
    "2. **Titel+Autor-Match**: Fuzzy-Matching auf normalisierte Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 15:40:44 - utils.notebook_utils - INFO - Searching for project root...\n",
      "2026-01-02 15:40:44 - utils.notebook_utils - INFO - Project root found: /media/sz/Data/Bibo/analysis\n",
      "2026-01-02 15:40:44 - utils.notebook_utils - INFO - Loading configuration...\n",
      "2026-01-02 15:40:44 - config_loader - INFO - Configuration loaded from /media/sz/Data/Bibo/analysis/config.yaml\n",
      "2026-01-02 15:40:44 - utils.notebook_utils - INFO - Configuration loaded successfully: Dual-Source Bibliothek Bestandsvergleich\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Project root: /media/sz/Data/Bibo/analysis\n",
      "‚úÖ Project: Dual-Source Bibliothek Bestandsvergleich v2.2.0\n"
     ]
    }
   ],
   "source": [
    "# üõ†Ô∏è SETUP\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "from rapidfuzz import fuzz\n",
    "import re\n",
    "\n",
    "# Add src to path\n",
    "project_root = Path.cwd()\n",
    "while not (project_root / 'config.yaml').exists() and project_root.parent != project_root:\n",
    "    project_root = project_root.parent\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "from utils.notebook_utils import setup_notebook\n",
    "\n",
    "project_root, config = setup_notebook()\n",
    "print(f\"‚úÖ Project root: {project_root}\")\n",
    "print(f\"‚úÖ Project: {config.get('project.name')} v{config.get('project.version')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Lade Daten...\n",
      "\n",
      "üìö VDEH (ORIGINAL - vor Fusion): 58,305 records\n",
      "   Spalten: ['id', 'title', 'authors', 'authors_affiliation', 'year', 'publisher', 'isbn', 'issn', 'pages', 'language']...\n",
      "üìö VDEH (FUSIONIERT - nach DNB+LoC): 58,305 records\n",
      "   Spalten: ['title', 'authors', 'year', 'publisher', 'pages', 'isbn', 'issn', 'title_source', 'authors_source', 'year_source']...\n",
      "üìö UB TU Freiberg:                   518,946 records\n",
      "   Spalten: ['id', 'source', 'title', 'authors', 'authors_str', 'year', 'isbn', 'place', 'physical_desc']\n",
      "\n",
      "‚úÖ Daten geladen\n"
     ]
    }
   ],
   "source": [
    "# üìÇ LOAD DATA\n",
    "print(\"üìÇ Lade Daten...\\n\")\n",
    "\n",
    "vdeh_processed_dir = config.project_root / config.get('paths.data.vdeh.processed')\n",
    "\n",
    "# Load ORIGINAL VDEH data (BEFORE fusion)\n",
    "vdeh_original_path = vdeh_processed_dir / '03_language_detected_data.parquet'\n",
    "if not vdeh_original_path.exists():\n",
    "    raise FileNotFoundError(f\"VDEH Original-Daten nicht gefunden: {vdeh_original_path}\")\n",
    "\n",
    "df_vdeh_original = pd.read_parquet(vdeh_original_path)\n",
    "print(f\"üìö VDEH (ORIGINAL - vor Fusion): {len(df_vdeh_original):,} records\")\n",
    "print(f\"   Spalten: {list(df_vdeh_original.columns)[:10]}...\")\n",
    "\n",
    "# Load FUSED VDEH data (AFTER DNB + LoC enrichment)\n",
    "vdeh_fused_path = vdeh_processed_dir / '06_vdeh_dnb_loc_fused_data.parquet'\n",
    "if not vdeh_fused_path.exists():\n",
    "    raise FileNotFoundError(f\"VDEH fusionierte Daten nicht gefunden: {vdeh_fused_path}\")\n",
    "\n",
    "df_vdeh_fused = pd.read_parquet(vdeh_fused_path)\n",
    "print(f\"üìö VDEH (FUSIONIERT - nach DNB+LoC): {len(df_vdeh_fused):,} records\")\n",
    "print(f\"   Spalten: {list(df_vdeh_fused.columns)[:10]}...\")\n",
    "\n",
    "# Load UB catalog data\n",
    "ub_processed_dir = config.project_root / 'data/ub_tubaf/processed'\n",
    "ub_path = ub_processed_dir / '01_loaded_data.parquet'\n",
    "\n",
    "if not ub_path.exists():\n",
    "    raise FileNotFoundError(f\"UB Katalogdaten nicht gefunden: {ub_path}\")\n",
    "\n",
    "df_ub = pd.read_parquet(ub_path)\n",
    "print(f\"üìö UB TU Freiberg:                   {len(df_ub):,} records\")\n",
    "print(f\"   Spalten: {list(df_ub.columns)}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Daten geladen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "data_overview",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Daten√ºbersicht - VORHER/NACHHER Vergleich:\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üîµ VDEH ORIGINAL (vor Fusion):\n",
      "   Total:      58,305\n",
      "   Mit Titel:  58,242 (99.9%)\n",
      "   Mit Autor:  58,305 (100.0%)\n",
      "   Mit Jahr:   33,313 (57.1%)\n",
      "   ISBN-Spalten: ['isbn', 'isbn_valid', 'isbn_status']\n",
      "   Mit ISBN:   31,521\n",
      "\n",
      "üü¢ VDEH FUSIONIERT (nach DNB+LoC):\n",
      "   Total:      58,305\n",
      "   Mit Titel:  58,249 (99.9%)\n",
      "   Mit Autor:  58,305 (100.0%)\n",
      "   Mit Jahr:   34,694 (59.5%)\n",
      "   ISBN-Spalten: ['isbn', 'isbn_source']\n",
      "\n",
      "üìñ UB TU Freiberg:\n",
      "   Total:      518,946\n",
      "   Mit Titel:  497,797 (95.9%)\n",
      "   Mit Autor:  451,331 (87.0%)\n",
      "   Mit Jahr:   496,993 (95.8%)\n",
      "   Mit ISBN:   289,292 (55.7%)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# üìä DATA OVERVIEW - COMPARISON\n",
    "print(\"üìä Daten√ºbersicht - VORHER/NACHHER Vergleich:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüîµ VDEH ORIGINAL (vor Fusion):\")\n",
    "print(f\"   Total:      {len(df_vdeh_original):,}\")\n",
    "print(f\"   Mit Titel:  {df_vdeh_original['title'].notna().sum():,} ({df_vdeh_original['title'].notna().sum()/len(df_vdeh_original)*100:.1f}%)\")\n",
    "print(f\"   Mit Autor:  {df_vdeh_original['authors'].notna().sum():,} ({df_vdeh_original['authors'].notna().sum()/len(df_vdeh_original)*100:.1f}%)\")\n",
    "print(f\"   Mit Jahr:   {df_vdeh_original['year'].notna().sum():,} ({df_vdeh_original['year'].notna().sum()/len(df_vdeh_original)*100:.1f}%)\")\n",
    "\n",
    "# Check for ISBN in original VDEH\n",
    "vdeh_orig_isbn_cols = [col for col in df_vdeh_original.columns if 'isbn' in col.lower()]\n",
    "print(f\"   ISBN-Spalten: {vdeh_orig_isbn_cols}\")\n",
    "if vdeh_orig_isbn_cols:\n",
    "    isbn_count_orig = sum(df_vdeh_original[col].notna().sum() for col in vdeh_orig_isbn_cols if col in df_vdeh_original.columns)\n",
    "    print(f\"   Mit ISBN:   {isbn_count_orig:,}\")\n",
    "\n",
    "print(\"\\nüü¢ VDEH FUSIONIERT (nach DNB+LoC):\")\n",
    "print(f\"   Total:      {len(df_vdeh_fused):,}\")\n",
    "print(f\"   Mit Titel:  {df_vdeh_fused['title'].notna().sum():,} ({df_vdeh_fused['title'].notna().sum()/len(df_vdeh_fused)*100:.1f}%)\")\n",
    "print(f\"   Mit Autor:  {df_vdeh_fused['authors'].notna().sum():,} ({df_vdeh_fused['authors'].notna().sum()/len(df_vdeh_fused)*100:.1f}%)\")\n",
    "print(f\"   Mit Jahr:   {df_vdeh_fused['year'].notna().sum():,} ({df_vdeh_fused['year'].notna().sum()/len(df_vdeh_fused)*100:.1f}%)\")\n",
    "\n",
    "# Check for ISBN in fused VDEH - k√∂nnte aus verschiedenen Quellen stammen\n",
    "vdeh_fused_isbn_cols = [col for col in df_vdeh_fused.columns if 'isbn' in col.lower()]\n",
    "print(f\"   ISBN-Spalten: {vdeh_fused_isbn_cols}\")\n",
    "\n",
    "print(\"\\nüìñ UB TU Freiberg:\")\n",
    "print(f\"   Total:      {len(df_ub):,}\")\n",
    "print(f\"   Mit Titel:  {df_ub['title'].notna().sum():,} ({df_ub['title'].notna().sum()/len(df_ub)*100:.1f}%)\")\n",
    "print(f\"   Mit Autor:  {df_ub['authors_str'].notna().sum():,} ({df_ub['authors_str'].notna().sum()/len(df_ub)*100:.1f}%)\")\n",
    "print(f\"   Mit Jahr:   {df_ub['year'].notna().sum():,} ({df_ub['year'].notna().sum()/len(df_ub)*100:.1f}%)\")\n",
    "print(f\"   Mit ISBN:   {df_ub['isbn'].notna().sum():,} ({df_ub['isbn'].notna().sum()/len(df_ub)*100:.1f}%)\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "helper_functions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Definiere Hilfsfunktionen...\n",
      "\n",
      "‚úÖ Hilfsfunktionen definiert\n"
     ]
    }
   ],
   "source": [
    "# üîß HELPER FUNCTIONS\n",
    "print(\"üîß Definiere Hilfsfunktionen...\\n\")\n",
    "\n",
    "def normalize_isbn(isbn):\n",
    "    \"\"\"Normalisiert ISBN: entfernt Bindestriche, Leerzeichen, konvertiert zu String.\"\"\"\n",
    "    if pd.isna(isbn) or isbn is None:\n",
    "        return None\n",
    "    isbn_str = str(isbn).strip()\n",
    "    # Entferne alle Nicht-Ziffern au√üer X (f√ºr ISBN-10)\n",
    "    isbn_clean = re.sub(r'[^0-9X]', '', isbn_str.upper())\n",
    "    if len(isbn_clean) == 0:\n",
    "        return None\n",
    "    return isbn_clean\n",
    "\n",
    "def isbn_10_to_13(isbn10):\n",
    "    \"\"\"Konvertiert ISBN-10 zu ISBN-13\"\"\"\n",
    "    if len(isbn10) != 10:\n",
    "        return None\n",
    "    base = '978' + isbn10[:9]\n",
    "    check_sum = 0\n",
    "    for i, digit in enumerate(base):\n",
    "        if i % 2 == 0:\n",
    "            check_sum += int(digit)\n",
    "        else:\n",
    "            check_sum += int(digit) * 3\n",
    "    check_digit = (10 - (check_sum % 10)) % 10\n",
    "    return base + str(check_digit)\n",
    "\n",
    "def isbn_13_to_10(isbn13):\n",
    "    \"\"\"Konvertiert ISBN-13 zu ISBN-10 (nur wenn 978-Pr√§fix)\"\"\"\n",
    "    if len(isbn13) != 13 or not isbn13.startswith('978'):\n",
    "        return None\n",
    "    base = isbn13[3:12]\n",
    "    check_sum = 0\n",
    "    for i in range(9):\n",
    "        check_sum += int(base[i]) * (10 - i)\n",
    "    check_digit = (11 - (check_sum % 11)) % 11\n",
    "    return base + ('X' if check_digit == 10 else str(check_digit))\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalisiert Text f√ºr Fuzzy-Matching: Kleinbuchstaben, entfernt Sonderzeichen.\"\"\"\n",
    "    # Handle None\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    # Handle NaN (check type first to avoid ValueError)\n",
    "    if isinstance(text, float) and pd.isna(text):\n",
    "        return \"\"\n",
    "    # Handle lists/arrays (z.B. authors als Liste)\n",
    "    if isinstance(text, (list, np.ndarray)):\n",
    "        if len(text) == 0:\n",
    "            return \"\"\n",
    "        # Join list elements\n",
    "        text = '; '.join(str(item) for item in text if item is not None and (not isinstance(item, float) or not pd.isna(item)))\n",
    "        if not text:\n",
    "            return \"\"\n",
    "    text_str = str(text).lower()\n",
    "    # Entferne Sonderzeichen, behalte nur Buchstaben, Zahlen, Leerzeichen\n",
    "    text_clean = re.sub(r'[^a-z0-9\\s]', ' ', text_str)\n",
    "    # Mehrfache Leerzeichen durch einzelne ersetzen\n",
    "    text_clean = re.sub(r'\\s+', ' ', text_clean).strip()\n",
    "    return text_clean\n",
    "\n",
    "def get_isbn_from_vdeh(row, isbn_cols):\n",
    "    \"\"\"Extrahiert erste verf√ºgbare ISBN aus VDEH-Datenzeile.\"\"\"\n",
    "    for col in isbn_cols:\n",
    "        if col in row.index and pd.notna(row[col]):\n",
    "            normalized = normalize_isbn(row[col])\n",
    "            if normalized:\n",
    "                return normalized\n",
    "    return None\n",
    "\n",
    "def perform_matching(df_vdeh, vdeh_name, df_ub, ub_isbn_index, ub_for_fuzzy):\n",
    "    \"\"\"F√ºhrt ISBN und Fuzzy Matching durch und gibt Ergebnisse zur√ºck.\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üîç MATCHING: {vdeh_name}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Normalisiere VDEH Felder\n",
    "    vdeh_isbn_cols = [col for col in df_vdeh.columns if 'isbn' in col.lower()]\n",
    "    df_vdeh_work = df_vdeh.copy()\n",
    "    df_vdeh_work['isbn_normalized'] = df_vdeh_work.apply(lambda row: get_isbn_from_vdeh(row, vdeh_isbn_cols), axis=1)\n",
    "    df_vdeh_work['title_normalized'] = df_vdeh_work['title'].apply(normalize_text)\n",
    "    df_vdeh_work['authors_normalized'] = df_vdeh_work['authors'].apply(normalize_text)\n",
    "    \n",
    "    print(f\"Normalisierung:\")\n",
    "    print(f\"   ISBNs gefunden: {df_vdeh_work['isbn_normalized'].notna().sum():,} ({df_vdeh_work['isbn_normalized'].notna().sum()/len(df_vdeh_work)*100:.1f}%)\")\n",
    "    print(f\"   Normalisierte Titel: {df_vdeh_work['title_normalized'].str.len().gt(0).sum():,}\")\n",
    "    print(f\"   Normalisierte Autoren: {df_vdeh_work['authors_normalized'].str.len().gt(0).sum():,}\")\n",
    "    \n",
    "    # === STRATEGIE 1: ISBN-MATCHING ===\n",
    "    print(f\"\\nüîç STRATEGIE 1: ISBN-MATCHING (mit ISBN-10/13 Konvertierung)\")\n",
    "    vdeh_with_isbn = df_vdeh_work[df_vdeh_work['isbn_normalized'].notna()].copy()\n",
    "    print(f\"   VDEH Datens√§tze mit ISBN: {len(vdeh_with_isbn):,}\")\n",
    "    \n",
    "    isbn_matches = []\n",
    "    for idx, row in vdeh_with_isbn.iterrows():\n",
    "        isbn = row['isbn_normalized']\n",
    "        if isbn in ub_isbn_index.index:\n",
    "            ub_matches = ub_isbn_index.loc[isbn]\n",
    "            if isinstance(ub_matches, pd.DataFrame):\n",
    "                ub_match = ub_matches.iloc[0]\n",
    "                match_count = len(ub_matches)\n",
    "            else:\n",
    "                ub_match = ub_matches\n",
    "                match_count = 1\n",
    "            \n",
    "            isbn_matches.append({\n",
    "                'vdeh_index': idx,\n",
    "                'vdeh_title': row['title'],\n",
    "                'vdeh_authors': row['authors'],\n",
    "                'vdeh_year': row['year'],\n",
    "                'isbn': isbn,\n",
    "                'ub_id': ub_match['id'],\n",
    "                'ub_title': ub_match['title'],\n",
    "                'ub_authors': ub_match['authors_str'],\n",
    "                'ub_year': ub_match['year'],\n",
    "                'match_method': 'ISBN',\n",
    "                'ub_match_count': match_count\n",
    "            })\n",
    "    \n",
    "    df_isbn_matches = pd.DataFrame(isbn_matches)\n",
    "    print(f\"   ‚úÖ ISBN-Matches: {len(df_isbn_matches):,}\")\n",
    "    if len(vdeh_with_isbn) > 0:\n",
    "        print(f\"   Match-Rate (mit ISBN): {len(df_isbn_matches)/len(vdeh_with_isbn)*100:.1f}%\")\n",
    "    print(f\"   Match-Rate (gesamt): {len(df_isbn_matches)/len(df_vdeh_work)*100:.1f}%\")\n",
    "    \n",
    "    # === STRATEGIE 2: FUZZY MATCHING ===\n",
    "    print(f\"\\nüîç STRATEGIE 2: TITEL+AUTOR FUZZY MATCHING\")\n",
    "    matched_indices = set(df_isbn_matches['vdeh_index'].values) if len(df_isbn_matches) > 0 else set()\n",
    "    vdeh_no_isbn_match = df_vdeh_work[~df_vdeh_work.index.isin(matched_indices)].copy()\n",
    "    \n",
    "    vdeh_for_fuzzy = vdeh_no_isbn_match[\n",
    "        (vdeh_no_isbn_match['title_normalized'].str.len() > 0) &\n",
    "        (vdeh_no_isbn_match['authors_normalized'].str.len() > 0)\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"   VDEH ohne ISBN-Match: {len(vdeh_no_isbn_match):,}\")\n",
    "    print(f\"   Mit Titel+Autor: {len(vdeh_for_fuzzy):,}\")\n",
    "    \n",
    "    # Fuzzy Matching-Parameter\n",
    "    TITLE_THRESHOLD = 85\n",
    "    AUTHOR_THRESHOLD = 80\n",
    "    MAX_CANDIDATES_PER_VDEH = 100\n",
    "    \n",
    "    fuzzy_matches = []\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    for i, (vdeh_idx, vdeh_row) in enumerate(vdeh_for_fuzzy.iterrows()):\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            elapsed = (datetime.now() - start_time).total_seconds()\n",
    "            rate = (i + 1) / elapsed if elapsed > 0 else 0\n",
    "            remaining = len(vdeh_for_fuzzy) - (i + 1)\n",
    "            eta = remaining / rate if rate > 0 else 0\n",
    "            print(f\"\\r   [{i+1:,}/{len(vdeh_for_fuzzy):,}] {(i+1)/len(vdeh_for_fuzzy)*100:.1f}% | \"\n",
    "                  f\"{rate:.1f} rec/s | ETA: {eta/60:.1f} min | Matches: {len(fuzzy_matches):,}\", end='', flush=True)\n",
    "        \n",
    "        vdeh_title = vdeh_row['title_normalized']\n",
    "        vdeh_author = vdeh_row['authors_normalized']\n",
    "        \n",
    "        title_similarities = ub_for_fuzzy['title_normalized'].apply(\n",
    "            lambda x: fuzz.ratio(vdeh_title, x)\n",
    "        )\n",
    "        \n",
    "        title_candidates = title_similarities[title_similarities >= TITLE_THRESHOLD]\n",
    "        \n",
    "        if len(title_candidates) == 0:\n",
    "            continue\n",
    "        \n",
    "        if len(title_candidates) > MAX_CANDIDATES_PER_VDEH:\n",
    "            title_candidates = title_candidates.nlargest(MAX_CANDIDATES_PER_VDEH)\n",
    "        \n",
    "        for ub_idx in title_candidates.index:\n",
    "            ub_row = ub_for_fuzzy.loc[ub_idx]\n",
    "            ub_author = ub_row['authors_normalized']\n",
    "            \n",
    "            author_sim = fuzz.ratio(vdeh_author, ub_author)\n",
    "            \n",
    "            if author_sim >= AUTHOR_THRESHOLD:\n",
    "                fuzzy_matches.append({\n",
    "                    'vdeh_index': vdeh_idx,\n",
    "                    'vdeh_title': vdeh_row['title'],\n",
    "                    'vdeh_authors': vdeh_row['authors'],\n",
    "                    'vdeh_year': vdeh_row['year'],\n",
    "                    'ub_id': ub_row['id'],\n",
    "                    'ub_title': ub_row['title'],\n",
    "                    'ub_authors': ub_row['authors_str'],\n",
    "                    'ub_year': ub_row['year'],\n",
    "                    'match_method': 'Title+Author Fuzzy',\n",
    "                    'title_similarity': title_similarities.loc[ub_idx],\n",
    "                    'author_similarity': author_sim,\n",
    "                    'combined_similarity': (title_similarities.loc[ub_idx] + author_sim) / 2\n",
    "                })\n",
    "                break\n",
    "    \n",
    "    df_fuzzy_matches = pd.DataFrame(fuzzy_matches)\n",
    "    print(f\"\\n   ‚úÖ Fuzzy-Matches: {len(df_fuzzy_matches):,}\")\n",
    "    if len(vdeh_for_fuzzy) > 0:\n",
    "        print(f\"   Match-Rate: {len(df_fuzzy_matches)/len(vdeh_for_fuzzy)*100:.1f}%\")\n",
    "    print(f\"   Dauer: {(datetime.now() - start_time).total_seconds()/60:.1f} Minuten\")\n",
    "    \n",
    "    # === COMBINE RESULTS ===\n",
    "    all_matches = []\n",
    "    if len(df_isbn_matches) > 0:\n",
    "        all_matches.append(df_isbn_matches)\n",
    "    if len(df_fuzzy_matches) > 0:\n",
    "        all_matches.append(df_fuzzy_matches)\n",
    "    \n",
    "    if len(all_matches) > 0:\n",
    "        df_all_matches = pd.concat(all_matches, ignore_index=True)\n",
    "        df_all_matches = df_all_matches.drop_duplicates(subset=['vdeh_index'], keep='first')\n",
    "    else:\n",
    "        df_all_matches = pd.DataFrame()\n",
    "    \n",
    "    # Nicht gefundene B√ºcher\n",
    "    if len(df_all_matches) > 0:\n",
    "        matched_indices = set(df_all_matches['vdeh_index'].values)\n",
    "    else:\n",
    "        matched_indices = set()\n",
    "    df_not_found = df_vdeh_work[~df_vdeh_work.index.isin(matched_indices)].copy()\n",
    "    \n",
    "    # Statistiken\n",
    "    stats = {\n",
    "        'dataset_name': vdeh_name,\n",
    "        'total_vdeh_books': len(df_vdeh_work),\n",
    "        'total_ub_books': len(df_ub),\n",
    "        'vdeh_with_isbn': int(df_vdeh_work['isbn_normalized'].notna().sum()),\n",
    "        'isbn_matches': len(df_isbn_matches),\n",
    "        'fuzzy_matches': len(df_fuzzy_matches),\n",
    "        'total_matches': len(df_all_matches),\n",
    "        'not_found': len(df_not_found),\n",
    "        'match_rate': len(df_all_matches) / len(df_vdeh_work) * 100 if len(df_all_matches) > 0 else 0,\n",
    "        'isbn_match_rate': len(df_isbn_matches) / df_vdeh_work['isbn_normalized'].notna().sum() * 100 if df_vdeh_work['isbn_normalized'].notna().sum() > 0 else 0,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüìä ZUSAMMENFASSUNG:\")\n",
    "    print(f\"   Total Matches: {stats['total_matches']:,} ({stats['match_rate']:.1f}%)\")\n",
    "    print(f\"   - ISBN-Matches: {stats['isbn_matches']:,}\")\n",
    "    print(f\"   - Fuzzy-Matches: {stats['fuzzy_matches']:,}\")\n",
    "    print(f\"   Nicht gefunden: {stats['not_found']:,} ({100-stats['match_rate']:.1f}%)\")\n",
    "    \n",
    "    return df_all_matches, df_not_found, stats\n",
    "\n",
    "print(\"‚úÖ Hilfsfunktionen definiert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "prepare_ub_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Bereite UB-Daten f√ºr Matching vor...\n",
      "\n",
      "UB Normalisierung:\n",
      "   ISBNs normalisiert: 289,292 (55.7%)\n",
      "   Normalisierte Titel: 497,797\n",
      "   Normalisierte Autoren: 451,330\n",
      "\n",
      "üîÑ Erstelle erweiterten ISBN-Index mit ISBN-10/13 Konvertierung...\n",
      "‚úÖ UB ISBN-Index erstellt: 519,124 eindeutige ISBNs\n",
      "   (davon 229,832 durch ISBN-10/13 Konvertierung)\n",
      "‚úÖ UB Fuzzy-Kandidaten: 441,101 Eintr√§ge\n"
     ]
    }
   ],
   "source": [
    "# üîÑ PREPARE UB DATA (einmalig f√ºr beide Matchings)\n",
    "print(\"üîÑ Bereite UB-Daten f√ºr Matching vor...\\n\")\n",
    "\n",
    "# UB: Normalisierte Felder hinzuf√ºgen\n",
    "df_ub['isbn_normalized'] = df_ub['isbn'].apply(normalize_isbn)\n",
    "df_ub['title_normalized'] = df_ub['title'].apply(normalize_text)\n",
    "df_ub['authors_normalized'] = df_ub['authors_str'].apply(normalize_text)\n",
    "\n",
    "print(\"UB Normalisierung:\")\n",
    "print(f\"   ISBNs normalisiert: {df_ub['isbn_normalized'].notna().sum():,} ({df_ub['isbn_normalized'].notna().sum()/len(df_ub)*100:.1f}%)\")\n",
    "print(f\"   Normalisierte Titel: {df_ub['title_normalized'].str.len().gt(0).sum():,}\")\n",
    "print(f\"   Normalisierte Autoren: {df_ub['authors_normalized'].str.len().gt(0).sum():,}\")\n",
    "\n",
    "# Erstelle erweiterten ISBN-Index f√ºr UB (mit ISBN-10/13 Konvertierung)\n",
    "print(\"\\nüîÑ Erstelle erweiterten ISBN-Index mit ISBN-10/13 Konvertierung...\")\n",
    "ub_isbn_map = {}\n",
    "\n",
    "for idx, row in df_ub[df_ub['isbn_normalized'].notna()].iterrows():\n",
    "    isbn = row['isbn_normalized']\n",
    "    \n",
    "    # Original ISBN\n",
    "    ub_isbn_map[isbn] = row\n",
    "    \n",
    "    # Konvertierung ISBN-10 <-> ISBN-13\n",
    "    if len(isbn) == 10:\n",
    "        isbn13 = isbn_10_to_13(isbn)\n",
    "        if isbn13:\n",
    "            ub_isbn_map[isbn13] = row\n",
    "    elif len(isbn) == 13 and isbn.startswith('978'):\n",
    "        isbn10 = isbn_13_to_10(isbn)\n",
    "        if isbn10:\n",
    "            ub_isbn_map[isbn10] = row\n",
    "\n",
    "# Erstelle DataFrame aus Map und setze Index\n",
    "ub_isbn_data = list(ub_isbn_map.values())\n",
    "ub_isbn_keys = list(ub_isbn_map.keys())\n",
    "\n",
    "if ub_isbn_data:\n",
    "    ub_isbn_index = pd.DataFrame(ub_isbn_data)\n",
    "    ub_isbn_index.index = pd.Index(ub_isbn_keys)\n",
    "    \n",
    "    print(f\"‚úÖ UB ISBN-Index erstellt: {len(set(ub_isbn_keys)):,} eindeutige ISBNs\")\n",
    "    print(f\"   (davon {len(set(ub_isbn_keys)) - df_ub['isbn_normalized'].notna().sum():,} durch ISBN-10/13 Konvertierung)\")\n",
    "else:\n",
    "    ub_isbn_index = pd.DataFrame()\n",
    "    print(f\"‚ö†Ô∏è  Keine ISBNs gefunden\")\n",
    "\n",
    "# UB-Kandidaten f√ºr Fuzzy Matching\n",
    "ub_for_fuzzy = df_ub[\n",
    "    (df_ub['title_normalized'].str.len() > 0) &\n",
    "    (df_ub['authors_normalized'].str.len() > 0)\n",
    "].copy()\n",
    "print(f\"‚úÖ UB Fuzzy-Kandidaten: {len(ub_for_fuzzy):,} Eintr√§ge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "matching_original",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üîç MATCHING: VDEH ORIGINAL (vor DNB/LoC)\n",
      "================================================================================\n",
      "\n",
      "Normalisierung:\n",
      "   ISBNs gefunden: 10,507 (18.0%)\n",
      "   Normalisierte Titel: 58,238\n",
      "   Normalisierte Autoren: 17,536\n",
      "\n",
      "üîç STRATEGIE 1: ISBN-MATCHING (mit ISBN-10/13 Konvertierung)\n",
      "   VDEH Datens√§tze mit ISBN: 10,507\n",
      "   ‚úÖ ISBN-Matches: 3,545\n",
      "   Match-Rate (mit ISBN): 33.7%\n",
      "   Match-Rate (gesamt): 6.1%\n",
      "\n",
      "üîç STRATEGIE 2: TITEL+AUTOR FUZZY MATCHING\n",
      "   VDEH ohne ISBN-Match: 54,760\n",
      "   Mit Titel+Autor: 14,581\n",
      "   [14,000/14,581] 96.0% | 3.9 rec/s | ETA: 2.5 min | Matches: 8556\n",
      "   ‚úÖ Fuzzy-Matches: 883\n",
      "   Match-Rate: 6.1%\n",
      "   Dauer: 62.0 Minuten\n",
      "\n",
      "üìä ZUSAMMENFASSUNG:\n",
      "   Total Matches: 4,428 (7.6%)\n",
      "   - ISBN-Matches: 3,545\n",
      "   - Fuzzy-Matches: 883\n",
      "   Nicht gefunden: 53,877 (92.4%)\n"
     ]
    }
   ],
   "source": [
    "# üîµ MATCHING 1: ORIGINAL VDEH (vor Fusion)\n",
    "df_matches_original, df_not_found_original, stats_original = perform_matching(\n",
    "    df_vdeh_original, \n",
    "    \"VDEH ORIGINAL (vor DNB/LoC)\",\n",
    "    df_ub,\n",
    "    ub_isbn_index,\n",
    "    ub_for_fuzzy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "matching_fused",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üîç MATCHING: VDEH FUSIONIERT (nach DNB/LoC)\n",
      "================================================================================\n",
      "\n",
      "Normalisierung:\n",
      "   ISBNs gefunden: 14,845 (25.5%)\n",
      "   Normalisierte Titel: 58,246\n",
      "   Normalisierte Autoren: 22,464\n",
      "\n",
      "üîç STRATEGIE 1: ISBN-MATCHING (mit ISBN-10/13 Konvertierung)\n",
      "   VDEH Datens√§tze mit ISBN: 14,845\n",
      "   ‚úÖ ISBN-Matches: 5,547\n",
      "   Match-Rate (mit ISBN): 37.4%\n",
      "   Match-Rate (gesamt): 9.5%\n",
      "\n",
      "üîç STRATEGIE 2: TITEL+AUTOR FUZZY MATCHING\n",
      "   VDEH ohne ISBN-Match: 52,758\n",
      "   Mit Titel+Autor: 17,709\n",
      "   [17,000/17,709] 96.0% | 4.2 rec/s | ETA: 2.8 min | Matches: 1,1771\n",
      "   ‚úÖ Fuzzy-Matches: 1,226\n",
      "   Match-Rate: 6.9%\n",
      "   Dauer: 70.3 Minuten\n",
      "\n",
      "üìä ZUSAMMENFASSUNG:\n",
      "   Total Matches: 6,773 (11.6%)\n",
      "   - ISBN-Matches: 5,547\n",
      "   - Fuzzy-Matches: 1,226\n",
      "   Nicht gefunden: 51,532 (88.4%)\n"
     ]
    }
   ],
   "source": [
    "# üü¢ MATCHING 2: FUSED VDEH (nach DNB+LoC Fusion)\n",
    "df_matches_fused, df_not_found_fused, stats_fused = perform_matching(\n",
    "    df_vdeh_fused,\n",
    "    \"VDEH FUSIONIERT (nach DNB/LoC)\",\n",
    "    df_ub,\n",
    "    ub_isbn_index,\n",
    "    ub_for_fuzzy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "comparison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä VORHER-NACHHER VERGLEICH\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Total B√ºcher</th>\n",
       "      <th>Mit ISBN</th>\n",
       "      <th>ISBN %</th>\n",
       "      <th>ISBN-Matches</th>\n",
       "      <th>Fuzzy-Matches</th>\n",
       "      <th>Total Matches</th>\n",
       "      <th>Match-Rate</th>\n",
       "      <th>Nicht gefunden</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>üîµ ORIGINAL (vor Fusion)</td>\n",
       "      <td>58305</td>\n",
       "      <td>10507</td>\n",
       "      <td>18.0%</td>\n",
       "      <td>3545</td>\n",
       "      <td>883</td>\n",
       "      <td>4428</td>\n",
       "      <td>7.6%</td>\n",
       "      <td>53877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>üü¢ FUSIONIERT (nach DNB+LoC)</td>\n",
       "      <td>58305</td>\n",
       "      <td>14845</td>\n",
       "      <td>25.5%</td>\n",
       "      <td>5547</td>\n",
       "      <td>1226</td>\n",
       "      <td>6773</td>\n",
       "      <td>11.6%</td>\n",
       "      <td>51532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Dataset  Total B√ºcher  Mit ISBN ISBN %  ISBN-Matches  \\\n",
       "0      üîµ ORIGINAL (vor Fusion)         58305     10507  18.0%          3545   \n",
       "1  üü¢ FUSIONIERT (nach DNB+LoC)         58305     14845  25.5%          5547   \n",
       "\n",
       "   Fuzzy-Matches  Total Matches Match-Rate  Nicht gefunden  \n",
       "0            883           4428       7.6%           53877  \n",
       "1           1226           6773      11.6%           51532  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà VERBESSERUNGEN DURCH FUSION:\n",
      "================================================================================\n",
      "ISBNs:          +4,338 (+41.3%)\n",
      "ISBN-Matches:   +2,002 (+56.5%)\n",
      "Total Matches:  +2,345 (+53.0%)\n",
      "Match-Rate:     7.6% ‚Üí 11.6% (+4.0 Prozentpunkte)\n",
      "\n",
      "‚úÖ Vergleich abgeschlossen\n"
     ]
    }
   ],
   "source": [
    "# üìä VORHER-NACHHER VERGLEICH\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä VORHER-NACHHER VERGLEICH\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison = pd.DataFrame([\n",
    "    {\n",
    "        'Dataset': 'üîµ ORIGINAL (vor Fusion)',\n",
    "        'Total B√ºcher': stats_original['total_vdeh_books'],\n",
    "        'Mit ISBN': stats_original['vdeh_with_isbn'],\n",
    "        'ISBN %': f\"{stats_original['vdeh_with_isbn']/stats_original['total_vdeh_books']*100:.1f}%\",\n",
    "        'ISBN-Matches': stats_original['isbn_matches'],\n",
    "        'Fuzzy-Matches': stats_original['fuzzy_matches'],\n",
    "        'Total Matches': stats_original['total_matches'],\n",
    "        'Match-Rate': f\"{stats_original['match_rate']:.1f}%\",\n",
    "        'Nicht gefunden': stats_original['not_found']\n",
    "    },\n",
    "    {\n",
    "        'Dataset': 'üü¢ FUSIONIERT (nach DNB+LoC)',\n",
    "        'Total B√ºcher': stats_fused['total_vdeh_books'],\n",
    "        'Mit ISBN': stats_fused['vdeh_with_isbn'],\n",
    "        'ISBN %': f\"{stats_fused['vdeh_with_isbn']/stats_fused['total_vdeh_books']*100:.1f}%\",\n",
    "        'ISBN-Matches': stats_fused['isbn_matches'],\n",
    "        'Fuzzy-Matches': stats_fused['fuzzy_matches'],\n",
    "        'Total Matches': stats_fused['total_matches'],\n",
    "        'Match-Rate': f\"{stats_fused['match_rate']:.1f}%\",\n",
    "        'Nicht gefunden': stats_fused['not_found']\n",
    "    }\n",
    "])\n",
    "\n",
    "display(comparison)\n",
    "\n",
    "# Berechne Verbesserungen\n",
    "print(\"\\nüìà VERBESSERUNGEN DURCH FUSION:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "isbn_improvement = stats_fused['vdeh_with_isbn'] - stats_original['vdeh_with_isbn']\n",
    "isbn_improvement_pct = (stats_fused['vdeh_with_isbn'] / stats_original['vdeh_with_isbn'] - 1) * 100 if stats_original['vdeh_with_isbn'] > 0 else 0\n",
    "print(f\"ISBNs:          +{isbn_improvement:,} ({isbn_improvement_pct:+.1f}%)\")\n",
    "\n",
    "isbn_match_improvement = stats_fused['isbn_matches'] - stats_original['isbn_matches']\n",
    "isbn_match_improvement_pct = (stats_fused['isbn_matches'] / stats_original['isbn_matches'] - 1) * 100 if stats_original['isbn_matches'] > 0 else 0\n",
    "print(f\"ISBN-Matches:   +{isbn_match_improvement:,} ({isbn_match_improvement_pct:+.1f}%)\")\n",
    "\n",
    "total_match_improvement = stats_fused['total_matches'] - stats_original['total_matches']\n",
    "total_match_improvement_pct = (stats_fused['total_matches'] / stats_original['total_matches'] - 1) * 100 if stats_original['total_matches'] > 0 else 0\n",
    "print(f\"Total Matches:  +{total_match_improvement:,} ({total_match_improvement_pct:+.1f}%)\")\n",
    "\n",
    "match_rate_improvement = stats_fused['match_rate'] - stats_original['match_rate']\n",
    "print(f\"Match-Rate:     {stats_original['match_rate']:.1f}% ‚Üí {stats_fused['match_rate']:.1f}% ({match_rate_improvement:+.1f} Prozentpunkte)\")\n",
    "\n",
    "print(\"\\n‚úÖ Vergleich abgeschlossen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "additional_matches_analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üîç ANALYSE: Welche zus√§tzlichen B√ºcher wurden durch Fusion gefunden?\n",
      "================================================================================\n",
      "\n",
      "Neue Matches durch Fusion: 2,862\n",
      "\n",
      "Match-Methoden der neuen Matches:\n",
      "   ISBN                     : 2,405 (84.0%)\n",
      "   Title+Author Fuzzy       : 457 (16.0%)\n",
      "\n",
      "üìã Beispiele neuer Matches durch Fusion:\n",
      "================================================================================\n",
      "\n",
      "Beispiel 1:\n",
      "  VDEH: Powder metallurgy science /\n",
      "        German, Randall M.,\n",
      "  UB:   Powder metallurgy science : Randall M. German\n",
      "  Methode: ISBN\n",
      "  ISBN: 1878954423\n",
      "  Similarity: Titel=nan%, Autor=nan%\n",
      "\n",
      "Beispiel 2:\n",
      "  VDEH: HaÃàrtereitechnisches Fachwissen\n",
      "        Mainka, Joachim\n",
      "  UB:   H√§rtereitechnisches Fachwissen : Joachim Mainka\n",
      "  Methode: ISBN\n",
      "  ISBN: 9783342004011\n",
      "  Similarity: Titel=nan%, Autor=nan%\n",
      "\n",
      "Beispiel 3:\n",
      "  VDEH: Grundlagen metallischer Werkstoffe, Korrosion und Korrosionsschutz\n",
      "        \n",
      "  UB:   Grundlagen metallischer Werkstoffe, Korrosion und Korrosionsschutz : v\n",
      "  Methode: ISBN\n",
      "  ISBN: 9783342002741\n",
      "  Similarity: Titel=nan%, Autor=nan%\n",
      "\n",
      "Beispiel 4:\n",
      "  VDEH: Stahlfibel\n",
      "        Eube, Joachim\n",
      "  UB:   Stahlfibel : von einem Autorenkollektiv. Hrsg. von Stahlberatungsstell\n",
      "  Methode: ISBN\n",
      "  ISBN: 9783342003991\n",
      "  Similarity: Titel=nan%, Autor=nan%\n",
      "\n",
      "Beispiel 5:\n",
      "  VDEH: Angewandte Betriebsfestigkeit\n",
      "        Cottin, Dieter, Puls, Erdmann\n",
      "  UB:   Angewandte Betriebsfestigkeit : Cottin ; Puls\n",
      "  Methode: ISBN\n",
      "  ISBN: 9783446161924\n",
      "  Similarity: Titel=nan%, Autor=nan%\n"
     ]
    }
   ],
   "source": [
    "# üîç ANALYSE DER ZUS√ÑTZLICHEN MATCHES\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üîç ANALYSE: Welche zus√§tzlichen B√ºcher wurden durch Fusion gefunden?\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Identifiziere neue Matches\n",
    "if len(df_matches_original) > 0 and len(df_matches_fused) > 0:\n",
    "    original_matched_indices = set(df_matches_original['vdeh_index'].values)\n",
    "    fused_matched_indices = set(df_matches_fused['vdeh_index'].values)\n",
    "    \n",
    "    # Neue Matches = in fused gefunden, aber nicht in original\n",
    "    new_match_indices = fused_matched_indices - original_matched_indices\n",
    "    \n",
    "    print(f\"Neue Matches durch Fusion: {len(new_match_indices):,}\")\n",
    "    \n",
    "    if len(new_match_indices) > 0:\n",
    "        df_new_matches = df_matches_fused[df_matches_fused['vdeh_index'].isin(new_match_indices)].copy()\n",
    "        \n",
    "        # Analysiere Match-Methoden der neuen Matches\n",
    "        print(\"\\nMatch-Methoden der neuen Matches:\")\n",
    "        method_dist = df_new_matches['match_method'].value_counts()\n",
    "        for method, count in method_dist.items():\n",
    "            print(f\"   {method:25s}: {count:,} ({count/len(df_new_matches)*100:.1f}%)\")\n",
    "        \n",
    "        # Zeige Beispiele\n",
    "        print(\"\\nüìã Beispiele neuer Matches durch Fusion:\")\n",
    "        print(\"=\"*80)\n",
    "        for i, (idx, row) in enumerate(df_new_matches.head(5).iterrows()):\n",
    "            print(f\"\\nBeispiel {i+1}:\")\n",
    "            print(f\"  VDEH: {row['vdeh_title'][:70]}\")\n",
    "            print(f\"        {row['vdeh_authors'][:70] if pd.notna(row['vdeh_authors']) else 'N/A'}\")\n",
    "            print(f\"  UB:   {row['ub_title'][:70]}\")\n",
    "            print(f\"  Methode: {row['match_method']}\")\n",
    "            if 'isbn' in row and pd.notna(row['isbn']):\n",
    "                print(f\"  ISBN: {row['isbn']}\")\n",
    "            if 'title_similarity' in row:\n",
    "                print(f\"  Similarity: Titel={row['title_similarity']:.0f}%, Autor={row['author_similarity']:.0f}%\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Keine neuen Matches zu analysieren\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "save_results",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üíæ SPEICHERE ERGEBNISSE\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Original Matches: /media/sz/Data/Bibo/analysis/notebooks/03_comparison/results/vdeh_ub_matches_original.parquet\n",
      "   Records: 4,428\n",
      "   CSV: /media/sz/Data/Bibo/analysis/notebooks/03_comparison/results/vdeh_ub_matches_original.csv\n",
      "\n",
      "‚úÖ Fused Matches: /media/sz/Data/Bibo/analysis/notebooks/03_comparison/results/vdeh_ub_matches_fused.parquet\n",
      "   Records: 6,773\n",
      "   CSV: /media/sz/Data/Bibo/analysis/notebooks/03_comparison/results/vdeh_ub_matches_fused.csv\n",
      "\n",
      "‚úÖ Nicht gefunden (Original): /media/sz/Data/Bibo/analysis/notebooks/03_comparison/results/vdeh_not_in_ub_original.parquet\n",
      "   Records: 53,877\n",
      "‚úÖ Nicht gefunden (Fused): /media/sz/Data/Bibo/analysis/notebooks/03_comparison/results/vdeh_not_in_ub_fused.parquet\n",
      "   Records: 51,532\n",
      "\n",
      "‚úÖ Statistiken: /media/sz/Data/Bibo/analysis/notebooks/03_comparison/results/comparison_statistics.json\n",
      "‚úÖ Vergleich CSV: /media/sz/Data/Bibo/analysis/notebooks/03_comparison/results/vorher_nachher_vergleich.csv\n",
      "\n",
      "================================================================================\n",
      "‚úÖ VORHER-NACHHER EVALUATION ABGESCHLOSSEN!\n",
      "================================================================================\n",
      "\n",
      "üìä Ergebnis-Zusammenfassung:\n",
      "   VORHER:  4,428 Matches (7.6%)\n",
      "   NACHHER: 6,773 Matches (11.6%)\n",
      "   VERBESSERUNG: +2,345 Matches (+4.0 Prozentpunkte)\n",
      "\n",
      "   Ergebnisse gespeichert in: /media/sz/Data/Bibo/analysis/notebooks/03_comparison/results\n"
     ]
    }
   ],
   "source": [
    "# üíæ SAVE RESULTS\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üíæ SPEICHERE ERGEBNISSE\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "comparison_dir = config.project_root / 'notebooks/03_comparison/results'\n",
    "comparison_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1. Original Matches\n",
    "if len(df_matches_original) > 0:\n",
    "    matches_orig_path = comparison_dir / 'vdeh_ub_matches_original.parquet'\n",
    "    df_matches_original.to_parquet(matches_orig_path, index=False)\n",
    "    print(f\"‚úÖ Original Matches: {matches_orig_path}\")\n",
    "    print(f\"   Records: {len(df_matches_original):,}\")\n",
    "    \n",
    "    matches_orig_csv = comparison_dir / 'vdeh_ub_matches_original.csv'\n",
    "    df_matches_original.to_csv(matches_orig_csv, index=False, encoding='utf-8-sig')\n",
    "    print(f\"   CSV: {matches_orig_csv}\")\n",
    "\n",
    "# 2. Fused Matches\n",
    "if len(df_matches_fused) > 0:\n",
    "    matches_fused_path = comparison_dir / 'vdeh_ub_matches_fused.parquet'\n",
    "    df_matches_fused.to_parquet(matches_fused_path, index=False)\n",
    "    print(f\"\\n‚úÖ Fused Matches: {matches_fused_path}\")\n",
    "    print(f\"   Records: {len(df_matches_fused):,}\")\n",
    "    \n",
    "    matches_fused_csv = comparison_dir / 'vdeh_ub_matches_fused.csv'\n",
    "    df_matches_fused.to_csv(matches_fused_csv, index=False, encoding='utf-8-sig')\n",
    "    print(f\"   CSV: {matches_fused_csv}\")\n",
    "\n",
    "# 3. Not found - Original\n",
    "if len(df_not_found_original) > 0:\n",
    "    not_found_orig_path = comparison_dir / 'vdeh_not_in_ub_original.parquet'\n",
    "    df_not_found_original.to_parquet(not_found_orig_path, index=True)\n",
    "    print(f\"\\n‚úÖ Nicht gefunden (Original): {not_found_orig_path}\")\n",
    "    print(f\"   Records: {len(df_not_found_original):,}\")\n",
    "\n",
    "# 4. Not found - Fused\n",
    "if len(df_not_found_fused) > 0:\n",
    "    not_found_fused_path = comparison_dir / 'vdeh_not_in_ub_fused.parquet'\n",
    "    df_not_found_fused.to_parquet(not_found_fused_path, index=True)\n",
    "    print(f\"‚úÖ Nicht gefunden (Fused): {not_found_fused_path}\")\n",
    "    print(f\"   Records: {len(df_not_found_fused):,}\")\n",
    "\n",
    "# 5. Comparison Statistics\n",
    "combined_stats = {\n",
    "    'original': stats_original,\n",
    "    'fused': stats_fused,\n",
    "    'improvements': {\n",
    "        'isbn_count': int(stats_fused['vdeh_with_isbn'] - stats_original['vdeh_with_isbn']),\n",
    "        'isbn_matches': int(stats_fused['isbn_matches'] - stats_original['isbn_matches']),\n",
    "        'total_matches': int(stats_fused['total_matches'] - stats_original['total_matches']),\n",
    "        'match_rate_improvement': float(stats_fused['match_rate'] - stats_original['match_rate'])\n",
    "    }\n",
    "}\n",
    "\n",
    "stats_path = comparison_dir / 'comparison_statistics.json'\n",
    "with open(stats_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(combined_stats, f, indent=2, ensure_ascii=False)\n",
    "print(f\"\\n‚úÖ Statistiken: {stats_path}\")\n",
    "\n",
    "# 6. Comparison Summary (CSV)\n",
    "comparison_csv = comparison_dir / 'vorher_nachher_vergleich.csv'\n",
    "comparison.to_csv(comparison_csv, index=False, encoding='utf-8-sig')\n",
    "print(f\"‚úÖ Vergleich CSV: {comparison_csv}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úÖ VORHER-NACHHER EVALUATION ABGESCHLOSSEN!\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nüìä Ergebnis-Zusammenfassung:\")\n",
    "print(f\"   VORHER:  {stats_original['total_matches']:,} Matches ({stats_original['match_rate']:.1f}%)\")\n",
    "print(f\"   NACHHER: {stats_fused['total_matches']:,} Matches ({stats_fused['match_rate']:.1f}%)\")\n",
    "print(f\"   VERBESSERUNG: +{total_match_improvement:,} Matches ({match_rate_improvement:+.1f} Prozentpunkte)\")\n",
    "print(f\"\\n   Ergebnisse gespeichert in: {comparison_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bibo-analysis-DoEGeq_l-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
